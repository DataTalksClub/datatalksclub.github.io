---
title: "Tuning Up"
description: "Book of the Week. Tuning Up by David Sweet"
cover: "images/books/20210816-tuning-up/cover.jpg"
image: "images/books/20210816-tuning-up/preview.jpg"
start: 2021-08-16 00:00:00
end: 2021-08-20 22:59:58
authors: [davidsweet]
links: 
  - text: Book's page
    link: https://www.manning.com/books/tuning-up-from-a-b-testing-to-bayesian-optimization
  - text: Book's GitHub repository
    link: https://github.com/tuningup/tuningup
archive:
- name: WingCode
  text: 'Hi David Sweet, nice to meet you!

    Should A/B test users selected randomly or should we select them based on specific
    attributes?'
  replies:
  - name: David Sweet
    text: "Short answer: randomly.\nLong answer: Selecting based on specific attributes\
      \ can improve the precision of a measurement. This is a technique called *blocking*.\n\
      Let\u2019s say you\u2019re running an A/B test. You have two versions of an\
      \ application, A and B, that you want to compare. You\u2019ll measure \u201C\
      time spent on the app\u201D for each.\nThe attribute you know about your users\
      \ is their age: They\u2019re either \u201Cover 25\" or \u201C25 or younger\u201D\
      .  The \u201Cover 25\" crowd generally spends less time on the app.\nIf you\
      \ assign users completely randomly to version A or B you might, by chance, have\
      \ more \u201Cover 25\u201D users seeing version A.  These extra \u201Cover 25\"\
      \ users would bias downward the measurement of time spent \u2014 but you\u2019\
      d attribute that bias to version A. If you were to rerun the experiment you\
      \ might, this time, assign more \u201Cover 25\u201D users to version B. In that\
      \ case you\u2019d attribute to the bias to version B.\nSo sometimes the age\
      \ attribute makes your measurement biased towards A and sometimes towards B.\
      \ On average, it\u2019s unbiased, thanks to randomization.\nUnfortunately, there\u2019\
      s variability from run to run. You can mitigate that by assigning the same number\
      \ of over-25 users to A as to B and\nthe same number of 25-or-younger users\
      \ to A as to B. Then age-related variability will disappear.\nThat being said,\
      \ when you select users from the over-25 group, you should select them randomly,\
      \ and the same goes for the under-25 group.  That way your experiment is still\
      \ unbiased to all of the other factors that might affect time spent."
  - name: WingCode
    text: "Thank you for the super detailed awesome answer David \U0001F642"
- name: Denis L.
  text: 'Hi David Sweet! Thanks for doing this.

    Bayesian techniques require specific likelihood function assumptions. From your
    experience, how often do you think these assumptions are violated by end-users
    and how robust are Bayesian methods under misspecification of likelihood (or prior)?

    To give a concrete example - if we run Bayesian A/B test analysis on a metric
    following what we think Poisson distribution, however upon closer inspection it
    violates some assumptions of the Poisson (but we model it as Poisson regardless).'
  replies:
  - name: David Sweet
    text: "The Bayesian method I discuss in the book uses a non-parametric method\
      \ call Gaussian process regression (GPR) to model what\u2019s called the *surrogate\
      \ function*, the function that maps system parameters to a business objective.\
      \ For example, you might serve ads based on a prediction of click-through-rate\
      \ (CTR) with a rule that says, \u201CIf the predicted CTR &lt; _threshold_ don\u2019\
      t show the ad.\u201D  The parameter _threshold_ affected how much ad revenue\
      \ (the business metric) you earn per day.\nYou could use GPR to model the function\
      \ mapping _threshold_ to daily ad revenue making (essentially) no assumptions\
      \ about the shape of the function.\nOnce you have that surrogate model, you\
      \ can ask it, \u201CWhich value of _threshold_ would give me the largest daily\
      \ ad revenue?\u201D"
  - name: Denis L.
    text: Thanks for the answer!
- name: Bayram Kapti
  text: "Thank you David Sweet! \nIt\u2019s easier to calculate the statistical significance\
    \ of an uplift in the test group\u2019s performance against the success metric(s)\
    \ during an AB test when the success metric is a rate -&gt; such as CTR. \nHowever,\
    \ I find it challenging to measure increased performance after a change in the\
    \ product. Example Hypothesis: If I add this new feature to my app, It will increase\
    \ the duration on the app."
  replies: []
- name: Bayram Kapti
  text: "A follow up question to above, what are the best practices to come up with\
    \ \u201CX % increase\u201D in the hypothesis statement. This varies for industry\
    \ or the metric itself, but is there a methodology to come up with The X percent\
    \ in the hypothesis statements?"
  replies:
  - name: WingCode
    text: "I would probably like to add onto the question \U0001F642\nIs there some\
      \ technique to predict the \u201CX % increase\u201D even before running the\
      \ experiment?"
  - name: David Sweet
    text: "Generally speaking, we\u2019ll hypothesize \u201Cno change in the metric\u201D\
      . This is called the null hypothesis. The A/B test will measure the change in\
      \ the metric and, potentially, reject the null hypothesis.\nThe good news is\
      \ that you don\u2019t have to guess, beforehand, by how much the metric will\
      \ change to run an A/B test.\nYou do need to know the level of variability (the\
      \ standard deviation) of the metric, however.  Usually you can estimate that\
      \ number from existing data. For example, if your metric were time spent by\
      \ a user during a session, you could look at existing logs of users sessions\
      \ and compute the standard deviation of the time spent.\nThat being said, it\
      \ would be nice to have a prediction of the A/B test result beforehand. If the\
      \ prediction said, \u201CThis new feature probably won\u2019t change time spent\
      \ by enough for anyone to care\u201D, then you could avoid running the A/B test\
      \ altogether.\nOne can sometimes  use a domain-specific simulation of the system\
      \ to make such a prediction. In quantitative trading, for example, it is common\
      \ to run a trading simulation (aka, a backtest) to estimate the profitability\
      \ of a trading strategy before deploying it. Simulations are sometimes used\
      \ by ad-serving systems, recommender systems, and others, too."
  - name: Bayram Kapti
    text: "Appreciate the answer David Sweet! \nWhen you have seasonality in your\
      \ users behavior, I guess than you could intentionally only look at the variance\
      \ for relavant season. \nImagine an e-commerce store having higher durations\
      \ during Christmas &amp; Holidays season when compared to Summer. In this case,\
      \ would it makes sense to include Summer data to calculate variance for an AB\
      \ test during Christmas time?"
  - name: David Sweet
    text: 'Yes.

      The goal is really to predict the variance that will be realized during the
      experiment. Using (in your variance calculation) only data expected to be similar
      to the data you will collect during the experiment would achieve that goal.'
- name: Wendy Mak
  text: hi David, would you use an 'off the shelf' system to run your A/B tests, and
    at what point would you decide that it's better to build your own?
  replies:
  - name: David Sweet
    text: "Sure. I would look at it like any other build-vs-buy decision. Does the\
      \ product do what you need? Is the price right? (Does the do *more* than you\
      \ need? If so maybe you\u2019re paying for features you won\u2019t use.)\nI\
      \ think it would be useful to initially run A/B tests in the lowest-effort way\
      \ possible \u2014 whether that\u2019s with a web-based tool or some simple manual\
      \ calculations in Jupyter. You\u2019ll learn about the details that are specific\
      \ to your system (ex., deploying \u201CB\u201D versions, logging measurements)\
      \ and get a sense for the value a piece of commercial software could provide."
- name: Wendy Mak
  text: also, how do you decide whether something is worth running A/B tests on?
  replies:
  - name: David Sweet
    text: One way to do this is to run a simulation before hand.
- name: Alexey Grigorev
  text: 'Hi David Sweet!

    What are your favorite blog posts and videos from companies that talk about their
    experimentation platforms and the way they run A/B tests?'
  replies:
  - name: David Sweet
    text: "This is a good one: [https://research.fb.com/videos/the-facebook-field-guide-to-machine-learning-episode-6-experimentation/](https://research.fb.com/videos/the-facebook-field-guide-to-machine-learning-episode-6-experimentation/)\n\
      I especially like the comments about how improvements in offline/modeling metrics\
      \ (ex., MSE of a regression, or cross-entropy of a classifier) don\u2019t exactly\
      \ translate to improvements in online/business metrics (like revenue, clicks/day,\
      \ etc.)"
  - name: David Sweet
    text: 'For a nice overview of Bayesian optimization in practice: [https://research.fb.com/blog/2018/09/efficient-tuning-of-online-systems-using-bayesian-optimization/](https://research.fb.com/blog/2018/09/efficient-tuning-of-online-systems-using-bayesian-optimization/)'
  - name: David Sweet
    text: 'Also, most large companies have internal, custom experimentation platforms:

      Uber: [https://eng.uber.com/xp](https://eng.uber.com/xp)/

      Netflix: Netflix [https://lnkd.in/dmKdFJ8](https://lnkd.in/dmKdFJ8)

      Twitter: [https://lnkd.in/dFHZxSM](https://lnkd.in/dFHZxSM)

      Facebook: [https://lnkd.in/dG7_8GV](https://lnkd.in/dG7_8GV)

      LinkedIn: [https://lnkd.in/dCc8aQN](https://lnkd.in/dCc8aQN)

      Spotify: [https://lnkd.in/dKcyyuM](https://lnkd.in/dKcyyuM)

      Spotify: [https://lnkd.in/d9E-5BC](https://lnkd.in/d9E-5BC)'
  - name: Alexey Grigorev
    text: Oh thank you!
- name: Oleg Polivin
  text: "Hi David Sweet\nThanks a lot for the opportunity to ask you a question!\n\
    Shall a data scientist know A/B testing well? \nHere is my reflection/speculation:\n\
    I'm working as a data scientist for some time now, and I didn't have to use A/B\
    \ testing so far. I know some basics, but surely there is much more to applying\
    \ A/B tests in production. It looks like it is a must for \"data analyst\" positions.\
    \ And I feel uncomfortable about this because:\n- I would speculate that A/B testing\
    \ itself brings more value to a company than a bunch of data scientists.\n- I\
    \ do not have that knowledge =&gt; am I/will be still relevant to the industry?"
  replies:
  - name: Eimhear Rainey
    text: Great question Oleg. I'm a data analyst venturing out into the word of unit
      testing and wondering if A/B testing is something I should be learning.
  - name: David Sweet
    text: "A/B testing \u2014 and related methods \u2014 help you translate your data\
      \ science work into concrete, business terms.\nFor example: You might design\
      \ a new feature and find that it reduces a model\u2019s RMSE by .1%.  Is that\
      \ good? How good? Would your boss care if s/he\u2019s not a data scientist?\
      \ Would a shareholder care?\nThe question you need to answer is: How much impact\
      \ does your new feature have on the business?  How much extra revenue can the\
      \ business generate by using your new feature? How much more do users enjoy\
      \ the product with your new feature?\nYou answer questions like that by running\
      \ an A/B test comparing the original model (without your feature, version A)\
      \ to the new model (with your feature, version B). You run that test on the\
      \ production system \u2014 the web site or mobile app or whatever \u2014 and\
      \ measure the business impact directly.\nYou could think of it this way: When\
      \ you write a self-assessment at the end of a quarter or year, would you rather\
      \ write, \u201CI improved RMSE by .1%\u201D or \u201CI added $XX million/year\
      \ to the bottom line\u201D?"
  - name: Oleg Polivin
    text: David thank you for this detailed answer, it is great!
- name: Oleg Polivin
  text: 'And a somewhat related question.

    Would it be correct to say that A/B testing is mostly used in e-commerce, advertising
    industries, and, more generally, where some kind of recommendations are involved?

    That would explain my lack of knowledge of A/B tests since I have not worked in
    those industries.'
  replies:
  - name: David Sweet
    text: "Yes, A/B testing and related experimental methods are used in advertising\
      \ and on recommender systems. They are used to improve web sites, web and mobile\
      \ applications (think Google, Facebook, Instagram, Twitter, Spotify, Amazon,\
      \ Uber, Apple products, and so on) and on trading systems.\nIn medicine, A/B\
      \ tests are called random controlled trials (RCT), and are used to test the\
      \ efficacy of new medications and other types of treatments. Anywhere you see\
      \ \u201CSix Sigma\u201D, \u201Cprocess improvement\u201D, etc. you\u2019ll find\
      \ A/B tests and related experimental methods. And, of course, you\u2019ll find\
      \ experiments in the sciences.\nA/B tests may be applied anywhere you need to\
      \ make a comparison in the face of complexity and uncertainty."
  - name: Oleg Polivin
    text: "I have some experience in running experiments in Economics, but we never\
      \ had to use multi-armed bandits and we didn\u2019t call it A/B testing, rather\
      \ RCTs as well.\nIt also seems to me that organizing A/B tests in industrial\
      \ setting is more complicated.\nThanks a lot!"
- name: Alexey Grigorev
  text: 'How much statistics do you think data scientists and analysts need for running
    A/B tests?

    It feels that data scientists are not always good with stats and focus more on
    ML.'
  replies:
  - name: Alexey Grigorev
    text: And often experimentation platforms take care of things like calculating
      the sample size. But do we need to understand how these things work to be able
      to use them properly?
- name: Tim Becker
  text: Hi David Sweet, really interesting book! When I went through the introduction,
    I was wondering what the biggest pitfalls are when evaluating models? What kind
    of mistakes are frequently made when doing the final test in production. Also,
    when evaluating financial models, e.g. for stock trading, isn't there a lot of
    randomness involved? How can we be sure about selecting the better model for the
    future? Extending the period over which we compare both models?
  replies:
  - name: David Sweet
    text: "A mistake that is common, has a big impact, and is easy to avoid is early\
      \ stopping. If your A/B test design says \u201Crun for 10 days\u201D, and you\
      \ stop before 10 days because the t statistic looks good, you\u2019ve made the\
      \ mistake of early stopping.  The t statistic itself is noisy and takes time\
      \ to settle down, so you really need to wait it out.\nWhen you design an A/B\
      \ test you\u2019ll, in part, try to limit your false positive* rate to 5%. Early\
      \ stopping can easily make that rate much higher (like 50% or 75% or more).\n\
      *A \u201Cfalse positive\u201D is when you think B looks better than A, but,\
      \ in fact, it\u2019s not.\n---\nThere is, indeed, a lot of randomness involve\
      \ in financial models. It\u2019s common to run an experiment on high-frequency\
      \ strategies or execution strategies (which also run on high-frequency data)\
      \ in 1 week - 1 month, depending on the specifics of the system."
  - name: Tim Becker
    text: David Sweet thank you for answering my questions. It is a quite challenging
      and interesting topic.
- name: Mansi Parikh
  text: 'Hi, David! It''s nice of you to do this - thank you!

    When easy-to-get large sample sizes generally cause most tests to be significant
    when means or proportions are actually only slightly different, what do you do?
    Do you have to look into other parts of the output, such Cohen''s D, to put the
    test in perspective?'
  replies:
  - name: David Sweet
    text: "If you have very large sample sizes, then you are fortunate. You\u2019\
      ll have very small standard errors of your measurements of your business metric\
      \ and can, thus, precisely measure the difference in business metric between\
      \ versions A and B of your system.\nThe question that remains is: How big of\
      \ a difference you care about?  The answer to this question is specific to your\
      \ system. For example, if \u201Cversion B\u201D of an ad serving system produced\
      \ $1,000/day more than version A, would you care? It depends. If your company\
      \ is a small startup that just started serving ads and has little revenue then,\
      \ yes! You need that $1k/day.\nBut if you work at Google, where ads produce\
      \ $150B/year (I think), maybe you\u2019d have to consider whether $1k/day extra\
      \ is worth the effort it takes to modify the code and the risk (however small)\
      \ of making a change to the system.\nI like to think of this as the question\
      \ of \u201Cpractical significance\u201D to differentiate it from statistical\
      \ significance. Statistical significance tells you how much to believe the measurement.\
      \ Practical significance tells you how much to care (from a business perspective)\
      \ about the the value you measured."
- name: Mansi Parikh
  text: Also, whenever you are cautious about your new variant possibly performing
    poorly compared to your control, wouldn't you always go with a split that allocates
    less traffic/money/etc towards the variant being tested? [https://geoffruddock.com/run-ab-test-with-unequal-sample-size/](https://geoffruddock.com/run-ab-test-with-unequal-sample-size/)
  replies:
  - name: David Sweet
    text: "Yes.\nGenerally, it\u2019s a good idea to start very small. With a small\
      \ size running you can detect bugs in the new code, bugs in the measurement\
      \ tooling, and very large, adverse changes in your metrics. Then you can scale\
      \ up to the full testing size.  Even that doesn\u2019t necessarily need to be\
      \ very large. Like you said, you might want to keep it small for safety\u2019\
      s sake.\nThe tradeoff to keep in mind might be that small sizes will take more\
      \ hours (or days) to run to completion."
- name: Mansi Parikh
  text: In most cases, we are running experiments with multifactorial designs. Is
    it appropriate to still compare all treatments to a common single control (T1
    vs C, T2 vs C, T3 vs C...) or perhaps create many different single treatment +
    control cuts (T1 vs not T1, T2 vs not T2, T3 vs not T3...) and use basic statistical
    hypothesis testing or should we graduate to something more sophisticated like
    an ANOVA (though to derive meaning out of those, we'll typically run Tukey paired
    tests anyway)?
  replies:
  - name: David Sweet
    text: "You can compare all of the treatments to a common control, but you\u2019\
      ll need to use a Bonferroni correction (or some other family-wise approach,\
      \ if I understand correctly, is what you\u2019re accounting for with the Tukey\
      \ paired tests) to get the right p values."
- name: Kyle Shannon
  text: "\u2753 Hey David Sweet, how do you handle evaluating multiple tests at once\
    \ that could have interference with each other? Perfect world is to isolate, but\
    \ that rarely happens. I was wondering if you have learned any tips or used any\
    \ frameworks you liked"
  replies:
  - name: David Sweet
    text: "This is a tough one. My knee-jerk reaction is to suggest finding a way\
      \ to decouple them. \U0001F642 For example, if you have a version A and version\
      \ B of a web app, each day (or each hour, or whatever) you could flip coin and\
      \ say, \u201Cheads we run A, tails we run B\u201D. It\u2019ll be lower precision\
      \ that running simultaneously, but at least they won\u2019t interfere.\nBut\
      \ if interference is unavoidable, I don\u2019t have a good answer. Perhaps you\
      \ could find a way to model the interference.  I\u2019m picturing something\
      \ akin to \u201Cy ~ chi_A + chi_B + chi_AB\u201D where y is your business metric\
      \ and the chi\u2019s are indicator variables. If you could fit a model of (roughly)\
      \ that sort to your measurements, then maybe you could separate the effects\
      \ of A and B from the effect of the interaction."
  - name: Kyle Shannon
    text: "Thank you for the response, haven't done much with modeling for interference\
      \ that's interesting. Gotta love the ol coin flip \U0001F642"
- name: Hironori Sakai
  text: Hello David Sweet, I have a question about the early stopping. In an ordinary
    (frequentist) A/B test, we compute the (minimal) number of observations of the
    groups for the experiment for the stopping condition in advance. How can we determine
    such a number in a Bayesian A/B test?
  replies:
  - name: David Sweet
    text: "In the Bayesian approach \u2014 aka., multi-armed bandits \u2014 you don\u2019\
      t.\nWhen you design an A/B test you place two constraints on your measurement:\
      \ (i) the false positive rate is limited (usually to 5%), and (ii) the false\
      \ negative rate is limited (usually to 20%). You calculate the minimum number\
      \ of observations needed to satisfy those constraints, given that there is variation\
      \ (error) in your measurement.\nBandit methods optimize for business-metric\
      \ impact. Bandit methods will monitor the business metrics and their standard\
      \ errors for A and B and allocate more observations to A or B as needed to capitalize\
      \ on business metric and/or decrease the standard error. \nBandit methods will\
      \ likely lead to higher false positive &amp; false negative rates \u2014 especially\
      \ when the business metric performance of A &amp; B are similar.  But the more\
      \ similar they are, the less you care about telling them apart (if business\
      \ metric maximization is your goal)."
- name: Kyle Shannon
  text: "\u2753 Do you have any suggestions of frameworks, tools or resources for\
    \  working with stakeholders to help them better plan out their A/B tests?"
  replies:
  - name: David Sweet
    text: "Names that come to mind are Optimizely and VWO, but I have not used them\
      \ personally. A/B testing is a big space, so you\u2019ll find many other commercial\
      \ and open source tools."
  - name: Kyle Shannon
    text: Cheers, thanks!
  - name: Kyle Shannon
    text: We're using a homegrown and looking to get something more managed currently
      checking out optimizely heard some good things
  - name: Alexey Grigorev
    text: 'I''ve just come across this:

      [https://docs.google.com/document/d/1uyQwvvFAa6I5aexiEmKWdHLoBzNPPskDKwEbAaOKe6M/edit?usp=sharing](https://docs.google.com/document/d/1uyQwvvFAa6I5aexiEmKWdHLoBzNPPskDKwEbAaOKe6M/edit?usp=sharing)'
- name: Eric Sims
  text: "Ahhh! I need this book in my life! \U0001F605 My co-worker and I have been\
    \ breaking our brains over A/B testing this week."
  replies:
  - name: David Sweet
    text: "Re: peeking\nIt\u2019s terrible! Resist it! It can send your false positive\
      \ rate through the roof.\nNow, simply *looking* at the t stats and measurements,\
      \ of course, won\u2019t cause any problems. In fact, you should watch to make\
      \ sure that something isn\u2019t going very wrong. After all, you\u2019re testing\
      \ something new. It could have an adverse effect on your metrics.\nThe problem\
      \ is really if you say: \u201CThe t statistic is high, and the B version looks\
      \ better, so I\u2019ll stop now and switch over to B.\u201D  Had you waited,\
      \ the t stat might have come back down, and you might not have been so excited\
      \ about the B version and just stuck with A.  That\u2019s how you drive your\
      \ false positive rate up.  That\u2019s why you need to wait."
  - name: Eric Sims
    text: Thank you!
- name: Eric Sims
  text: What is your opinion on 'peeking'? Is it terrible? Or is it something that
    people are going to do no matter what so you try not to lose sleep over it?
  replies: []
- name: Eric Sims
  text: "Do you have any good analogies/examples to help people understand why ending\
    \ a test when they see what they want is not a good idea?  ...asking for a friend\
    \ \U0001F642"
  replies: []
- name: Lavanya M K
  text: "David Sweet what does \"tuning up\" in the book title mean\U0001F642? Is\
    \ it tuning abtest system or optimising the business metrics?"
  replies:
  - name: David Sweet
    text: "I\u2019m using \u201Ctuning\u201D to refer to the optimizing the business\
      \ metrics. (Like setting the tuning knob on a radio.)"
---

Tuning Up: From A/B testing to Bayesian optimization is a toolbox for optimizing machine learning systems,
quantitative trading strategies, and more. Youâ€™ll start with a deep dive into tests like A/B testing,
and then graduate to advanced techniques used to measure performance in highly competitive industries
like finance and social media. The tests in this unique, practical guide will quickly reveal which
approaches and features deliver real results for your business.
