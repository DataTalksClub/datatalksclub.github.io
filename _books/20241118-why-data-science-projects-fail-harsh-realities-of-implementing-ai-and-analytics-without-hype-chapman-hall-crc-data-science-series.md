---
authors:
- evanshellshear
- douglasgray
cover: images/books/20241118-why-data-science-projects-fail-harsh-realities-of-implementing-ai-and-analytics-without-hype-chapman-hall-crc-data-science-series/cover.jpg
description: 'Book of the Week. Why Data Science Projects Fail: The Harsh Realities
  of Implementing AI and Analytics, without the Hype (Chapman & Hall/CRC Data Science
  Series) by Evan Shellshear and Douglas Gray'
end: 2024-11-22 23:59:59
image: images/books/20241118-why-data-science-projects-fail-harsh-realities-of-implementing-ai-and-analytics-without-hype-chapman-hall-crc-data-science-series/preview.jpg
links:
- link: https://www.routledge.com/Why-Data-Science-Projects-Fail-The-Harsh-Realities-of-Implementing-AI-and-Analytics-without-the-Hype/Gray-Shellshear/p/book/9781032660301
  text: Book's page
- link: https://www.amazon.com/Why-Data-Science-Projects-Fail/dp/1032660309/
  text: Buy on Amazon
start: 2024-11-18 00:00:00
title: 'Why Data Science Projects Fail: The Harsh Realities of Implementing AI and
  Analytics, without the Hype (Chapman & Hall/CRC Data Science Series)'

archive:
- name: Aizzaac
  text: Doug Gray How might emerging technologies like artificial intelligence and
    machine learning impact the field of optimization? Can you envision any specific
    applications or techniques that could revolutionize the way we solve complex problems?
  replies:
  - name: Doug Gray
    text: There are a couple of ways to look at this question---- ML is predominantly
      a predictive analytics methodology--- generally speaking, you can "optimize"
      a process or a system's performance with better predictive accuracy using ML---
      we do this at WMT by predicting, based on Supply Chain activity history, more
      accurately truck loads, miles, cases per trailer, containers, etc. we better
      position transportation capacity to reduce costs--- that optimizes the transportation
      system. Specifically speaking, the "field of optimization" most typically implies
      "mathematical optimization"--- i.e., optimizing an objective function subject
      to a set of constraints, e.g., linear programming and its siblings (nonlinear,
      quadratic, integer, mixed-integer programming). There are a plethora of well
      known, tried and true algorithms and modeling approaches for solving these kinds
      of problems. IMO, where AI and optimization intersect is when AI addresses "complex
      human decision-making" then optimization models and algorithms can be embedded
      in broader, larger scope and scale AI solutions--- I address this in the book
      with regard to AI _augmentation_ (vs _automation_) where a human and a model
      work _together_ _iteratively_, _interactively_ to solve a complex problem---
      _interactive optimization_ is a field, pioneered at Georgia Tech in the 1980s)
      that uses optimization-based models and heuristic algorithms that run very fast
      and achieve "good solutions quickly" that a human can evaluate, perform exception
      handling on, adjust some input data parameters and rerun the model. AI then
      creates new opportunities to apply optimization-based models and solutions.
- name: Ahmed Yassin
  text: "As I transition from a model-first to a product-first mindset, I\u2019m realizing\
    \ how essential it is to work closely with stakeholders to define clear goals\
    \ and success criteria. However, stakeholders often have conflicting priorities\
    \ or incomplete understanding of what AI can realistically achieve. How do you\
    \ recommend navigating these challenges while keeping everyone aligned and the\
    \ project moving forward? Doug Gray"
  replies: []
- name: Anj
  text: "Hi everyone!\nThank you all for your insightful questions! We are now closing\
    \ this week's Book of the Week.\nA big thanks to Doug Gray and Evan Shellshear\
    \ for sharing their time and answering our queries. \U0001F64F\nWe appreciate\
    \ your participation and look forward to seeing you again at the next event!"
  replies: []
- name: Doug Gray
  text: Hello, this is co-author Doug Gray--- I am online and available to take questions!
  replies: []
- name: Wali Mohamed
  text: "Do we have to read the book to ask the questions? I guess? If I don\u2019\
    t have to read the book, my question would be? What are the factors that contribute\
    \ to project failure?"
  replies:
  - name: Till
    text: "Since you can win the book at the end of the week, I would not buy it in\
      \ advance. This is more like an appetizer\u2026"
  - name: Evan Shellshear
    text: 'Good question Wali, there are many, many factors, well over 100. What we
      tried to do was distill it down to the main ones (following like an 80/20 rule)
      and the main reasons are the ones that you''d expect:'
  - name: Evan Shellshear
    text: '1. Not solving a problem aligned with the strategy

      2. Not having the needed data

      3. Not having the right people

      4. Not having the right tools'
  - name: Evan Shellshear
    text: So what is surprising is how often this still occurs in spite of all the
      frameworks, knowledge and research
  - name: Evan Shellshear
    text: Also see Doug's comment below
- name: Ahmed Yassin
  text: "Hi Doug Gray and Evan Shellshear,\nI have one big question (for now \U0001F604\
    ) that I\u2019d love to get your insights on:\nSince many early data science projects\
    \ tend to fail, especially for those of us at the beginning of our careers, what\
    \ strategies or mindsets would you recommend to help junior engineers turn these\
    \ failures into valuable learning experiences? How can we use them as stepping\
    \ stones for future success instead of getting discouraged?"
  replies:
  - name: Evan Shellshear
    text: 'Great question Ahmed, to add to what Doug wrote, I would say develop skills
      beyond just the technical. This is a big message in our book. You''ll need to
      learn how to do project management, stakeholder management, excellent communication,
      learn how to work with ambiguous problem statements.

      The most important mindset is knowing that problems will be handed to you in
      a half thought out, vague and ambiguous fashion, learning to deal with things
      like that is a great start.

      You can use a different mindset for future success too by simply setting expectations
      in line with what I wrote above. If what happens is what you are expecting,
      then it won''t be as painful.'
  - name: Ahmed Yassin
    text: "Agree! Your emphasis on dealing with ambiguity and honing communication\
      \ skills is spot on. At my work, I\u2019ve encountered situations where project\
      \ goals weren\u2019t fully fleshed out, and it was a challenge to align everyone!"
- name: Doug Gray
  text: 'We highly recommend you read the book as it is replete with great stories
    and examples of why projects fail (and succeed)--- but the The TOP 10 List that
    I put together originally and served as my contribution to the book''s foundation
    is as follows:'
  replies: []
- name: Doug Gray
  text: 1. Not understanding the business problem
  replies: []
- name: Doug Gray
  text: 2. Data issues
  replies: []
- name: Doug Gray
  text: 3. Misapplying the model
  replies: []
- name: Doug Gray
  text: 4. Solving a problem that is not a business priority (focus on high value
    business problems, questions, decisions)
  replies: []
- name: Doug Gray
  text: 5. Communication (or lack thereof)
  replies: []
- name: Doug Gray
  text: 6. Change management issues
  replies: []
- name: Doug Gray
  text: 7. Unrealistic expectations (on business value realization, timeline, scope,
    etc)
  replies: []
- name: Doug Gray
  text: 8. Project management issues (Only 20% of IT projects achieved scope, timeline,
    and budget (Standish Group CHAOS report, and most DS projects evolve into IT projects
    to put them in to Production mode)
  replies: []
- name: Doug Gray
  text: 9. Excessive focus on models and technologies
  replies: []
- name: Doug Gray
  text: 10. Getting the model from Sandbox to Production System (Models make the enterprise
    smarter, models embedded in production systems make the enterprise more economically
    efficient--- Tom Davenport, and THAT is the end game, goal.
  replies: []
- name: Doug Gray
  text: Building models is easy. Building models into enterprise grade production
    systems embedded in business processes is 10-100X more difficult, complex, costly---
    Doug Gray (this was confirmed in a recent MIT research report)
  replies: []
- name: Doug Gray
  text: BONUS--- 11. Leadership and 12. Culture
  replies: []
- name: Till
  text: '&gt; then a stunning 80% (or more) of analytics projects will continue to
    fail

    Is this your rate? Doug Gray and Evan Shellshear'
  replies:
  - name: Evan Shellshear
    text: As Doug wrote, for us personally, luckily not. We've both been fortunate
      to work for analytically mature organisations
- name: Doug Gray
  text: Wali, I hope this addresses your question---
  replies: []
- name: Doug Gray
  text: 80% is a rate confirmed in numerous published articles, studies, and research
    reports, yes---
  replies: []
- name: Doug Gray
  text: But the rate is higher for analytically immature orgs like 90%, and lower
    for analytically mature orgs, like 40%
  replies:
  - name: Till
    text: So you worked in both to archive the 80%?
  - name: Doug Gray
    text: I have worked in analytically mature companies like American Airlines and
      Walmart, and companies that were less mature, so yes.
  - name: Doug Gray
    text: But many examples in the book come from colleagues stories that run the
      gamut from large Fortune 500 companies to start ups that also range from analytically
      immature to mature.
- name: Doug Gray
  text: A failure rate of 10-20% is to be expected given how difficult this is, and
    0% would imply that your org is not stretching or taking enough risk and experimenting
  replies: []
- name: Doug Gray
  text: We focus on avoidable failures--- failures we can avert using lessons learned
    as we describe in detail in the book
  replies: []
- name: Doug Gray
  text: 'Ahmed, we need to remove the stigma of failure and realize Failure is Feedback---
    fail fast and learn is the mantra of Silicon Valley that has been adopted by other
    large companies that innovate with technology and AI, even my company Fortune
    #1 Walmart. This is as much a cultural issue for companies where failure is viewed
    as a negative.'
  replies:
  - name: Ahmed Yassin
    text: "I love the idea that adopting a \u201Cfail fast and learn\u201D mindset\
      \ can foster innovation!"
- name: Doug Gray
  text: The TOP 10 Reasons WDSPF are examined through real world cases studies and
    stories from Evan and myself and our colleagues--- we expect that there is something
    in there for literally everyone that you can relate to, learn from, and apply
    to your environment, regardless of industry.
  replies: []
- name: Aizzaac
  text: "Hi Doug Gray Evan Shellshear \nHow do you effectively balance the need for\
    \ rapid delivery with the requirement for robust, high-quality data science solutions?"
  replies:
  - name: Evan Shellshear
    text: Hi Aizzaac, for this one it is a question of setting expectations. With
      a given set of resources, it takes only so long to build something so people
      need to be aware of what is possible with the given constraints. The solution
      is likely to stage gate it in an agile fashion and release functional code along
      the journey that does a small, high quality solution for a piece of the problem.
      I really feel this is typically a stakeholder engagement/expectation type of
      problem when stated this way. Do you have an example of this for yourself?
  - name: Aizzaac
    text: "No, I always run out of time. \U0001F605"
  - name: Doug Gray
    text: There is an old saw in software engineering that also applies to DS projects...
      "Good. Fast. Cheap. Pick two." Agile MVP (Minimum Viable Product) is the best
      approach for building and delivering models, solutions rapidly with minimal
      time to value. We talk about Minimum Time to Value (identification, validation,
      then capture). Once value is identified that can buy time and garner support
      for investment needed to productionalize the solution. That said, there is no
      substitute for rigorous testing (unit, integration, UAT), MLOPS, pipeline development
      and stabilization. My quote from the book is relevant here. "Building models
      is easy. Building models into enterprise production systems is difficult by
      10-100X" in cost, resources, complexity. MIT recently produced a research report
      that validated that 1-2 order of magnitude multiplier to get to production.
- name: Aizzaac
  text: "Evan Shellshear Doug Gray \nHow do you incorporate ethical considerations\
    \ into data science projects, and how do these considerations impact timelines\
    \ and deliverables?"
  replies:
  - name: Evan Shellshear
    text: 'This will come from the company, you should be implementing the ethical
      guidelines set by the organisation. It is not hard to take a code of conduct
      or similar ethical policy and ensure that, when designing your system, that
      it meets these requirements. This should occur first and be part of the scoping
      process that should then set the timelines and deliverables before they are
      set in stone.

      Do you have any specific examples you are thinking of in your own work?'
  - name: Aizzaac
    text: "We were tasked with developing a predictive model to identify potential\
      \ fraud in financial transactions.\nSo, we scheduled specific phases for bias\
      \ detection and mitigation techniques, such as fairness testing and algorithmic\
      \ audits. \nBut as usual,we ran out of time \U0001F605"
- name: Aizzaac
  text: What emerging technologies or methodologies do you believe will significantly
    impact the future of data science project timelines and delivery?
  replies:
  - name: Evan Shellshear
    text: 'The ones that are doing it already are things that automate processes like
      AutoML, (manage, what used to be, manual processes) like database versioning
      e.g. DBT and these are having an impact now and will have more in the future
      as companies adopt them. But especially generative AI tools that are able to
      generate quality code will help speed up development

      Any specific use cases that you are thinking of?'
  - name: Aizzaac
    text: 'Automated Explanation Generation: AI can generate human-readable explanations
      for complex models, improving transparency and trust.'
  - name: Evan Shellshear
    text: Interesting it sounds like a combination of generative AI and something
      like LIME or SHAP, I haven't heard of it but it reads like a ChatGPT created
      kind of concept
  - name: Aizzaac
    text: Well, some already stablished companies (Recorded Future, Spark Beyond)
      just connect a ChatGPT on the output of their models and showcase some "explainability".
  - name: Doug Gray
    text: '[H2O.ai](http://H2O.ai) for example is an advanced AutoML platform that
      generates detailed reports in prose/English that explain predictive model results.'
- name: Aizzaac
  text: How do you identify and mitigate your own biases, as well as those of your
    stakeholders, to ensure objective analysis?
  replies:
  - name: Evan Shellshear
    text: I wouldn't say this is a data science specific question, this is a generic
      problem across all disciplines. One example that I have seen is that making
      people aware of biases can assist in reducing their effect.
- name: Aizzaac
  text: How can we effectively map a complex business problem to a well-understood
    algorithmic problem?
  replies:
  - name: Doug Gray
    text: There are field guides that walk you through how to diagnose a problem and
      figure out which model to use--- here is one from Booz Allen [https://www.boozallen.com/s/insight/publication/field-guide-to-data-science.html](https://www.boozallen.com/s/insight/publication/field-guide-to-data-science.html)
  - name: Doug Gray
    text: I have built some of my own tools that look like a Pachinko Machine--- you
      drop the coin (problem) in the top and it filters down into a bucket at the
      bottom that suggests appropriate model forms, tools, algorithms--- using criteria
      such as Predictive vs Prescriptive, Stochastic vs, Deterministic vs. Hybrid
  - name: Doug Gray
    text: We built a tool at WMT that uses ChatGPT to recommend LLM model forms in
      response to a problem description prompt.
  - name: Doug Gray
    text: 'In the optimization world (prescriptive), Garey &amp; Johnson''s book Computers
      and Intractability: A Guide to the Theory of NP-Completeness is the industry
      standard for diagnosing super-complex problems and mapping them onto well known
      problems, e.g., airline crew scheduling maps to set covering/partitioning and
      is usually solved with an integer programming formulation using column generation.'
  - name: Doug Gray
    text: This is a skill, and an art form, as much as it is science, that must be
      honed and developed over time by working with people more experienced, more
      skilled than you are--- either professionals, or academics (that understand
      and have worked on real world problems). The good news is that today for predictive
      problems, if you have clean data, AutoML tools will generate the best model
      (or ensemble of models) for you with the best independent predictor variables
      to achieve the greatest predictive efficacy--- you still need to understand
      how to interpret and apply the results but it does cut down on time and errors
      in coding to get to a useful model form.
- name: Aizzaac
  text: How can we adapt to the evolving landscape of data science and AI, where new
    technologies and techniques are constantly emerging?
  replies:
  - name: Evan Shellshear
    text: Good question, the answer comes back to the business questions. Just because
      there are new technologies and techniques does not mean you need to learn them
      or use them if they are not relevant (especially after doing a ROI calculation
      on how much effort they are to learn, their cost to use, etc that can quickly
      filter out uncommercial options). Narrow down to those that are relevant and
      then just understand them. The next step is what you would expect - trial them,
      test them in PoCs, etc.
- name: Tim Becker
  text: "Hi Doug Gray and Evan Shellshear , thank you very much for this book! It\
    \ sounds like an essential read for anyone working in or with a data science team.\
    \ Here are my questions:\n- Do you have any advice on how to develop reasonable\
    \ value cases for data science projects, particularly in the early stages? In\
    \ my experience, predicting benefits and concrete value cases can be quite challenging\
    \ for digital projects in general, and especially for data science initiatives,\
    \ where the model's performance can be uncertain.\n- In many organizations, multiple\
    \ projects often aim to address the same issues simultaneously, which complicates\
    \ post-investment reviews. For instance, how would you justify the value of a\
    \ data science model designed to predict customer churn when other parameters\u2014\
    like pricing strategies\u2014are also being adjusted at the same time?\n- Often,\
    \ the people responsible for creating data aren\u2019t the ones who directly benefit\
    \ from or use it. This disconnect can make it difficult to motivate them to maintain\
    \ the quality standards needed for data science projects to succeed. Do you have\
    \ any recommendations on who in an organization should \"own\" the data to maximize\
    \ project success?\n- Do you recommend any specific project management frameworks\
    \ for data science projects? Estimating timelines for tasks like data analysis\
    \ and model development is notoriously difficult. How do you approach this challenge?"
  replies:
  - name: Doug Gray
    text: I outline a methodology in the book on how to estimate value _before_ the
      project begins. Having a BIG multiplier value target helps a lot to start with,
      like a big expense category--- like jet fuel spend for a $20B airline is $6B---
      you establish a maximum potential benefit of ~ 1-10% and then take a % of that
      as your goal to reduce cost...
  - name: Doug Gray
    text: The churn - pricing change example is a great one. It is very difficult
      to get to as the economists say _ceteris paribus_ (all other things equal) experiment
      conditions in the real world. I think you estimate churn for a given price structure,
      then rerun the churn experiment with the new price structure. I think an experiment
      to understand price elasticity of demand can be factored into churn. Experimental
      design is critically important and I address that concept in the book with two
      real world examples. Addressing confounding factors and properly "blocking"
      experiments, and using concepts like double-blind studies, control groups, etc.
      are critical to ensuring the integrity of outcomes.
  - name: Doug Gray
    text: The data owner(s) include and range from the data steward (usually a business
      person/SME/data domain expert), to the data governance person responsible for
      metadata, data lineage, data transformations, etc., the VP of Data Engineering
      responsible for data infrastructure integrity, the CISO responsible for data
      security/access controls, and last but not least the CDO---Chief Data Officer.
      The Data Scientist plays a role in maintaining data integrity, certainly on
      what happens to the model outputs, but they are in effect a customer of the
      data inputs.
  - name: Doug Gray
    text: "I recommend Agile Kanban for DS modeling projects, and Agile Scrum for\
      \ system development projects. I have found \"planning poker\" (using Fibonnacci\
      \ numbers 1,2,3,5,8,13,21) to be a useful approach for assessing complexity\
      \ at a task level and estimating level of effort for scrum planning purposes.\
      \ The downfall of Waterfall largely for software projects in favor of Agile/SAFe\
      \ is the answer to your question--- breaking down large, complex components\
      \ and tasks into smaller, more manageable chunks is a key--- lots of data on\
      \ team progress and performance (burn down charts, etc) against tasks and milestones\
      \ on the timeline, lots of unvarnished, open, honest communication and feedback\
      \ about how things are \"really going\" and where adjustments/course corrections\
      \ need to be made--- short, iterative dev/test cycles, like 2 week sprints,\
      \ where you get feedback and can make course corrections before things get too\
      \ far out of control are all critical factors to project success. Estimates\
      \ about the unknown are always wrong--- it is really a question of how far off\
      \ is the estimate from reality, and the sooner you can figure out how far off\
      \ you are and then course correct 1) take a delay and move the deadline out\
      \ 2) assign more resources and break up the big rocks 3) refactor the code...\
      \ an old rule of thumb from Fred Brooks in The Mythical Man-Month was that you\
      \ should spend 1/3 of your time doing the design, 1/6 of your time coding, and\
      \ 1/2 of your time testing (unit, integration, UAT, verification, validation,\
      \ post-deployment tests) STILL holds true today IMHO. Also knowing your people\
      \ and their skill levels--- in absolute and relative terms across levels (Senior,\
      \ Staff, Principal) and even within levels--- not all engineers are created\
      \ equal--- hence the term Mythical Man-Month... there is no such thing as a\
      \ \"man-month\" rather how long will _John_ take to get this done vs. _Judy_\
      \ or  _Jamaal_.\U0001F600\U0001F607"
  - name: Tim Becker
    text: Doug Gray thank you very much
- name: Daniel Kleine
  text: "Hi Doug Gray! In your book, you categorize failures into four main areas\
    \ (strategy, process, people, and technology). Given that many organizations struggle\
    \ with aligning DS projects to business goals, what specific steps can a company\
    \ take to ensure that its data strategy is not only well-defined but also adaptable\
    \ to evolving business needs?\nI\u2019ve been following the discussion in this\
    \ channel and noticed that sometimes the answers are posted separately from the\
    \ question threads. I find it a bit challenging to track which responses go with\
    \ which questions, and I imagine others might feel the same. Would it be possible\
    \ to reply within the same thread as the question? I think it could help keep\
    \ the conversation more organized for everyone."
  replies:
  - name: Evan Shellshear
    text: Hi Daniel, sorry for the non-threading, I've sent Doug an email letting
      him know how.
  - name: Daniel Kleine
    text: Alright, thanks!
- name: Doug Gray
  text: Daniel Kleine Often times companies undertake data strategy projects (one
    in the book) that entail "capturing 100% of enterprise data"--- the example in
    the book is a monumental failure where the project was cancelled after 3 years
    and $300M spent as the data was doubling every year or so and the company never
    caught up--- in my experience not all data is equal in value potential--- I faced
    this situation in a Director Data &amp; Analytics role--- we did the opposite--
    we first identified a project that high potential business value--- Customer Journey
    &amp; Customer Lifetime Value--- we focused data capture efforts on this project
    only--- the result was an incremental $100M in annual revenue--- net-net, data
    strategy, data capture, data governance, and data science programs and initiatives
    should all be linked, aligned, and focused on the highest value potential efforts---
  replies:
  - name: Daniel Kleine
    text: Interesting point and insightful use case, thank you Doug Gray and Evan
      Shellshear!
- name: Aizzaac
  text: 'Hi Doug Gray Evan Shellshear

    What are some specific examples of unrealistic technologies or simplifications
    of complex projects that you are referring to?'
  replies:
  - name: Evan Shellshear
    text: There are so many, I heard a hilarious one the other day when one of Australia's
      AI leaders got up and told everyone how Shell was using Generative AI to optimise
      wind farm design. This showed a clear lack of understanding of the different
      technologies (completely unrealistic use of the technology)
- name: Aizzaac
  text: Doug Gray Evan Shellshear Why is it important for practitioners and leaders
    to have a deep understanding of data science projects, even if they are not technical
    experts?
  replies:
  - name: Evan Shellshear
    text: I wouldn't say that practitioners need a deep understanding, I don't think
      we claim that anywhere. The most important part is realistic expectations which
      might require some high level knowledge
- name: Aizzaac
  text: 'Doug Gray Evan Shellshear

    How can the high failure rate of analytics projects be addressed?'
  replies:
  - name: Evan Shellshear
    text: That's a complex one and one that we address in the book
- name: Aizzaac
  text: 'Doug Gray Evan Shellshear

    How can companies effectively identify the highest value potential data sources?'
  replies:
  - name: Evan Shellshear
    text: Via a business case, also see Doug's good responses to these questions below.
- name: Ahmed Yassin
  text: 'Hi Doug Gray &amp; Evan Shellshear

    I got another question in my mind!

    I think defining the right success metrics is always a tricky part of any AI project.
    Could you share any frameworks or practical tips for identifying the most meaningful
    metrics, especially in projects where the business impact might not be immediately
    measurable?'
  replies: []
- name: Aizzaac
  text: 'Doug Gray Evan Shellshear

    How can companies avoid the "shiny object syndrome" and stay focused on high-value
    data initiatives?'
  replies:
  - name: Evan Shellshear
    text: That's excellent and a core issue we raise in the book and Doug's response
      below nails this one.
- name: Doug Gray
  text: Aizzaac Technologies in and of themselves are not unrealistic, but rather
    how they are applied and or an unrealistic expectation of their application complexity
    or the benefits that will be gained are unrealistic. Technologies should not drive
    the project--- the business problem, question, or decision is the primary consideration---
    then you figure out what is the most appropriate technology/modeling methodology
    to address the situation at hand. Sometimes complexity (or cost) inhibits a solution---
    so we use a heuristic algorithm instead of say an optimal solution to reduce compute
    time and get a "good answer quickly instead of the "optimal answer that is super
    costly." Some teams may say "let's do a a computer vision project"--- that is
    not the best place to start--- rather "let's automate inspection of our tractor
    trailers to verify broken seals or locks, damage, or obscured ID tags... and let's
    experiment with computer vision to see if we can get a good result to solve that
    problem"--- that is a better approach. IMO.
  replies: []
- name: Doug Gray
  text: Aizzaac Practioner DS folks should have a deep understanding of DS projects
    to ensure their validity and correct application. Practitioner business people
    or leaders do not need to have a deep technical understanding but they should
    understand generally how the model is working to achieve the results to avoid
    "black box" syndrome--- "before and after the model" experiments comparing results,
    gamification "can you beat the model's performance?", story boarding walkthroughs,
    and lots of testing and validation can help non-technical folks be confident in
    the model results being produced.
  replies: []
- name: Doug Gray
  text: Aizzaac We addressed the failure rates using these top ten factors in the
    book, backed up by research and real-world case study stories---
  replies: []
- name: Doug Gray
  text: '1. Not understanding the business problem

    [11:45 AM] Doug Gray

    2. Data issues

    [11:45 AM] Doug Gray

    3. Misapplying the model

    [11:45 AM] Doug Gray

    4. Solving a problem that is not a business priority (focus on high value business
    problems, questions, decisions)

    [11:45 AM] Doug Gray

    5. Communication (or lack thereof)

    [11:45 AM] Doug Gray

    6. Change management issues

    [11:46 AM] Doug Gray

    7. Unrealistic expectations (on business value realization, timeline, scope, etc)

    [11:47 AM] Doug Gray

    8. Project management issues (Only 20% of IT projects achieved scope, timeline,
    and budget (Standish Group CHAOS report, and most DS projects evolve into IT projects
    to put them in to Production mode)'
  replies: []
- name: Doug Gray
  text: '1. Aizzaac Not understanding the business problem

    [11:45 AM] Doug Gray

    2. Data issues

    [11:45 AM] Doug Gray

    3. Misapplying the model

    [11:45 AM] Doug Gray

    4. Solving a problem that is not a business priority (focus on high value business
    problems, questions, decisions)

    [11:45 AM] Doug Gray

    5. Communication (or lack thereof)

    [11:45 AM] Doug Gray

    6. Change management issues

    [11:46 AM] Doug Gray

    7. Unrealistic expectations (on business value realization, timeline, scope, etc)

    [11:47 AM] Doug Gray

    8. Project management issues (Only 20% of IT projects achieved scope, timeline,
    and budget (Standish Group CHAOS report, and most DS projects evolve into IT projects
    to put them in to Production mode)

    [11:47 AM] Doug Gray

    9. Excessive focus on models and technologies

    [11:48 AM] Doug Gray

    10. Getting the model from Sandbox to Production System (Models make the enterprise
    smarter, models embedded in production systems make the enterprise more economically
    efficient--- Tom Davenport, and THAT is the end game, goal'
  replies: []
- name: Doug Gray
  text: Aizzaac Data in and of itself has no value--- data used to solve a problem,
    answer a question, make a decision can generate business value and economic impact.
    Figure out what are the "big rocks" and "big opportunities" your firm is facing---
    frame those as business problems, questions, or decisions--- use your financial
    statements to apply revenue and cost figures--- for example the cost of excess
    inventory, input costs like labor and fuel--- prioritize these based on magnitude,
    and then find the data needed to model these scenarios--- the data takes on value
    when used to solve a problem--- or answer a question or make a decision.
  replies: []
- name: Doug Gray
  text: Aizzaac Avoid shiny object syndrome by not leading with technical solutions---
    do not become enamored with the latest new new thing, like ChatGPT in and of itself---
    focus on the most valuable, high impact business problems, questions, decisions---
    the model or technology you use is secondary--- focus on how are we creating something
    new to enhance the customer experience or change customer behavior or transform
    operations--- the technical solution is secondary, never primary, it is a means
    to an end, not an end in itself.
  replies: []
- name: Doug Gray
  text: Ahmed Yassin Metrics should reflect quantities or statistics the business
    (leadership, managers, shareholders, board members) cares about deeply and that
    impact financial performance and or economic efficiency of the business. Most
    businesses have KPIs--- Key Performance Indicators that measure the health of
    the business--- in retail COGs, inventory costs, and labor costs are big drivers,
    in airlines flight crew labor and jet fuel costs are the biggest two operating
    expenses on the income statement. The data should be in ERP systems, financial
    statements/systems, HR systems, and may need to be consolidated into a data warehouse,
    lake, lakehouse, or mesh so it can be analyzed. Sometimes you need to conduct
    experiments and collect the data to measure the status quo approach to a new and
    improved DS/AI solution approach to see how KPIs are affected and to what magnitude.
    Our advice is to work closely with business leaders, managers, subject matter/domain
    experts that understand the underlying business processes and the metrics used
    to measure how well or not the system is performing. DS/AI folks should not have
    to come up with metrics on their own--- work alongside the people that do the
    work.
  replies: []
- name: Aizzaac
  text: Doug Gray How can organizations balance the need for innovation with the need
    for practical, business-oriented solutions?
  replies:
  - name: Evan Shellshear
    text: 'Hi Aizzaac, I think Doug partially answered this question above when he
      wrote:

      [https://datatalks-club.slack.com/archives/C01H403LKG8/p1732035785245949](https://datatalks-club.slack.com/archives/C01H403LKG8/p1732035785245949)

      But I think the way to do it is the allocation of some budget for innovation.
      This is not just a data science problem but one that is across any new risky
      project in the business that is more than just business as usual'
- name: Aizzaac
  text: Doug Gray How can organizations effectively communicate the complexities of
    data science models to non-technical stakeholders, fostering trust and ensuring
    buy-in for data-driven decisions?
  replies:
  - name: Evan Shellshear
    text: 'Good question. I don''t think this is something that organisations just
      communicate and unfortunately when we talk about organisations communicating
      stuff like this usually we mean the leadership communicating which is normally
      the group to whom the complexities need to be communicated (or better yet understood).

      It is an education piece and the data scientists themselves won''t be able to
      do this on their own (i.e. communicating upwards) and hopefully things like
      our book will be read by leaders and provide this education.'
- name: Aizzaac
  text: 'Evan Shellshear Doug Gray

    How can we ensure that the metrics we use are not only accurate but also actionable?'
  replies:
  - name: Evan Shellshear
    text: By ensuring we align them with the business, we talk about the greatest
      reason of failure in our book being around lack of alignment with a business
      problem, and metrics that don't align with business priorities have a much lower
      chance of being actioned as they are deemed unimportant
- name: Aizzaac
  text: 'Evan Shellshear Doug Gray

    In situations where data quality is a challenge, how can we develop robust metrics
    that are still reliable?'
  replies:
  - name: Evan Shellshear
    text: "My suggest is that you need to fix the data quality issue and apply metrics\
      \ on that. Garbage in, garbage out as they say \U0001F604"
- name: zora
  text: "Hi Doug Gray,\nAs someone working closely at the intersection of business\
    \ and technical, I often find it incredibly challenging to communicate effectively\
    \ with technical teams, particularly engineers, to align on our objectives. Many\
    \ times, the data team seems to approach their role from a purely technical perspective\
    \ rather than viewing themselves as partners supporting business outcomes.\nIn\
    \ your experience, what\u2019s the best way to navigate and address this kind\
    \ of situation to foster better collaboration and alignment?"
  replies:
  - name: Doug Gray
    text: DS professionals to be most effective need to be intellectually curious
      about their industry, company, department, and problem domain. Understanding
      the business problem (or not) is the first reason DS Projects Fail. DS is a
      means to an end of delivering business value, not an end in itself building
      models and doing the math. Economic impact is the end game and in order to achieve
      that DS folks need to engage in understanding the business. Multiple good examples
      in the book on this front. DS is a support function to the business and needs
      to be embraced as such.
- name: Rafael Barbosa
  text: 'Hi Doug Gray!

    One common reason I''ve seen data science projects fail is the imposition of very
    short deadlines that fail to account for the complexity of the problem at hand.
    This complexity is often difficult to measure during the conceptual phase due
    to inherent uncertainties, even for experienced data scientists. Unrealistic timelines
    can lead to suboptimal initial results, causing stakeholder dissatisfaction and,
    in some cases, abandonment of the project altogether. On the other hand, excessively
    long development cycles can also lead to failure. By the time such projects are
    deployed, the company/market may have already moved on from the problem they were
    meant to address, rendering the solution irrelevant.

    Striking the right balance is essential. What would you consider to be the ideal
    timespan for a data science delivery cycle, one that allows for a satisfactory
    first delivery and incorporates continuous improvements based on feedback, while
    minimizing the risk of obsolescence?'
  replies:
  - name: Doug Gray
    text: Gauging complexity is challenging--- it goes back to deeply understanding
      the underlying business problem--- then mapping that problem onto a well known,
      well understood problem--- e.g., like Traveling Salesman Problem (TSP)/Vehicle
      Routing Problem (VRP), or set covering/partitioning for airline crew scheduling,
      or job scheduling on machines--- complexity also stems from the number of integration
      points for data and other systems integration, dynamism, mission criticality---
      we cover this in the book in the section on moving from Sandbox to Production---
      rigor and thoroughness of analysis is the only answer I can give to properly
      ID complexity--- timespan varies a lot depending on data availability and the
      above factors--- my DS pros regularly build models in the cloud in hours, days,
      or a week, once the data is in place--- a typical DS project end to end may
      take 3-6-9-12 months depending on data, modeling, testing, deployment, environmental
      constraints, resource availability--- XXXL projects like UPS ORION (TSP/VRP)
      took 10 years and cost $250M to build and deploy, but saves $300-400M per year
      in costs. The best approach is to be conservative, realistic in your estimating
      process. If the value is there, the business should be prepared to stick with
      the plan, program. That requires persistence, commitment, and a champion.
  - name: Rafael Barbosa
    text: Thank you for the reply, Doug! All valid points and I completely agree.
      The book sounds very interesting, will be sure to give it a read.
- name: Rafael Barbosa
  text: "Hi Doug Gray,\nI\u2019ve often observed data scientists fall into the trap\
    \ of data snooping, or \u201Cp-hacking,\u201D in an effort to prevent a project\
    \ from being deemed a failure. This typically occurs when stakeholders approach\
    \ a project with a strong bias toward a specific hypothesis, expecting the data\
    \ scientist to validate it. When the analysis produces results that contradict\
    \ their expectations, stakeholders can become confrontational, scrutinizing every\
    \ aspect of the methodology and citing anecdotal or empirical examples to challenge\
    \ the findings. This often pressures the data scientist to repeatedly reanalyze\
    \ the data until the results align with the stakeholder\u2019s preconceived notions,\
    \ epitomizing the phrase: \u201Ctorture the data long enough, and it will confess.\u201D\
    \nHow can data scientists effectively deliver results that contradict a stakeholder's\
    \ assumptions while helping them understand why their hypothesis is not supported\
    \ by the data? Given that explanations involving statistical tests and p-values\
    \ often fail to resonate with non-technical teams, how can we effectively communicate\
    \ negative findings in a way that fosters understanding and collaboration?"
  replies:
  - name: Doug Gray
    text: This is the challenge (and opportunity) with analytics, data science in
      general--- when we "let the data speak", inconvenient truths are revealed, long
      held beliefs and assumptions are invalidated, tried and true heuristics and
      rules of thumb no longer apply--- the question is a cultural one--- do you work
      for a company that is run by *HiPPO*s? *Hi*ghest *P*aid *P*erson's *O*pinion---
      or is the company authentically fact-based, data-driven, model-oriented in the
      way it solves problems, answers questions, makes decisions? It is a HUGE distinction---
      but as DS we must be able to clearly, succinctly, diplomatically, rationally
      "speak truth to power" in English, in business terms they will understand, utilizing
      a variety of tools, like before and after (the model) experiments, visualization
      with charts, graphs, heat maps, etc. (this is HUGE, as a picture is worth 1,000
      words), storytelling in the business problem/question/decision context--- if
      stakeholders want to ignore the facts/data/model results, and hold onto their
      biases, you should probably look for another job somewhere else.
  - name: Rafael Barbosa
    text: 'I agree! Establishing a culture of data-driven decision-making has been
      a key focus for us, though it can be challenging, especially when teams make
      fast critical decisions based on empirical evidence that later proves incorrect.

      As you pointed out, visualizations and storytelling are invaluable in overcoming
      these challenges. I feel like data scientists play a vital role as champions
      of this culture, helping to promote and embed it across the organization, especially
      in companies where this mindset isn''t yet deeply ingrained and has gained focus
      recently mostly because of the current AI hype. Perhaps another reason why soft
      skills should never be ignored as well haha thank you for sharing your insights!'

---

The field of artificial intelligence, data science, and analytics is crippling itself. Exaggerated promises of unrealistic technologies, simplifications of complex projects, and marketing hype are leading to an erosion of trust in one of our most critical approaches to making decisions: data driven.

This book aims to fix this by countering the AI hype with a dose of realism. Written by two experts in the field, the authors firmly believe in the power of mathematics, computing, and analytics, but if false expectations are set and practitioners and leaders don’t fully understand everything that really goes into data science projects, then a stunning 80% (or more) of analytics projects will continue to fail, costing enterprises and society hundreds of billions of dollars, and leading to non-experts abandoning one of the most important data-driven decision-making capabilities altogether.

For the first time, business leaders, practitioners, students, and interested laypeople will learn what really makes a data science project successful. By illustrating with many personal stories, the authors reveal the harsh realities of implementing AI and analytics.