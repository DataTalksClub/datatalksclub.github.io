---
title: "Optimize Decisions with ML: Prescriptive & Robust Optimization for Supply Chain and Pricing"
short: "Decision Optimization"
season: 2
episode: 6
guests:
- danbecker
image: images/podcast/machine-learning-decision-optimization.jpg
ids:
  youtube: SJuzQ4bcU2c
  anchor: Translating-ML-Predictions-Into-Better-Real-World-Results-with-Decision-Optimization---Dan-Becker-eqk0b1/a-a4maq87
links:
  youtube: https://www.youtube.com/watch?v=SJuzQ4bcU2c
  anchor: https://anchor.fm/datatalksclub/episodes/Translating-ML-Predictions-Into-Better-Real-World-Results-with-Decision-Optimization---Dan-Becker-eqk0b1/a-a4maq87
  spotify: https://open.spotify.com/episode/42eAhI6F31DZ96Mnq2I4bJ
  apple: https://podcasts.apple.com/us/podcast/translating-ml-predictions-into-better-real-world-results/id1541710331?i=1000509855317

description: "Learn prescriptive analytics & robust optimization for supply chain pricing: align ML predictions to decisions, scale models, pick solvers, and boost revenue."
intro: "How do you turn machine learning predictions into better real-world decisions—especially under uncertainty in supply chains and pricing? In this episode, Dan Becker, Founder & CEO of Decision AI and former Google data scientist and Product Director at DataRobot, walks through prescriptive analytics and decision optimization for practical business impact. With a background that includes top Kaggle performance and contributions to TensorFlow and Keras, Dan explains how to formulate optimization problems, choose objectives and constraints, and integrate ML forecasts into prescriptive and robust optimization models. <br><br> We cover robust vs. stochastic optimization, aligning loss functions with business objectives, and the solvers and tools that make this work—OR-Tools, Gurobi, Pyomo and open-source options. Dan also digs into scalability, approximation techniques, and deployment: pipelines, monitoring, and feedback loops. Use cases include supply chain optimization, resource allocation, and pricing/bidding strategies, plus operational, legal, and ethical constraints. Listeners will get practical guidance on evaluation metrics, common pitfalls like mis-specified objectives and overfitting decisions, and the cross-functional skills needed—data science, operations research, and software engineering—to get started with prescriptive optimization projects."
topics:
- machine learning
- decision optimization
dateadded: 2021-02-23


quotableClips:
- name: Podcast Introduction
  startOffset: 0
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=0
  endOffset: 70
- name: 'Introduction: Dan Becker and Decision Optimization Overview'
  startOffset: 70
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=70
  endOffset: 180
- name: 'Gap: Machine Learning Predictions vs. Real-World Decisions'
  startOffset: 180
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=180
  endOffset: 360
- name: 'Prescriptive Analytics: Role in ML Pipelines'
  startOffset: 360
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=360
  endOffset: 540
- name: 'Formulating Optimization Problems: Objectives and Constraints'
  startOffset: 540
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=540
  endOffset: 720
- name: 'Modeling Uncertainty: Robust and Stochastic Optimization'
  startOffset: 720
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=720
  endOffset: 930
- name: Integrating Predictions into Optimization Models
  startOffset: 930
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=930
  endOffset: 1125
- name: Aligning Loss Functions with Business Objectives
  startOffset: 1125
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=1125
  endOffset: 1320
- name: 'Solvers & Tools: OR-Tools, Gurobi, Pyomo, Open-Source Options'
  startOffset: 1320
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=1320
  endOffset: 1530
- name: 'Scalability: Large-Scale Optimization and Approximation Techniques'
  startOffset: 1530
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=1530
  endOffset: 1710
- name: 'Use Case: Supply Chain Optimization and Resource Allocation'
  startOffset: 1710
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=1710
  endOffset: 1920
- name: 'Use Case: Pricing, Bidding, and Revenue Optimization'
  startOffset: 1920
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=1920
  endOffset: 2100
- name: 'Decision Constraints: Operational, Legal, and Ethical Considerations'
  startOffset: 2100
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=2100
  endOffset: 2280
- name: 'Evaluation Metrics: Measuring Real-World Impact of Decisions'
  startOffset: 2280
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=2280
  endOffset: 2460
- name: 'Deployment: Pipelines, Monitoring, and Feedback Loops'
  startOffset: 2460
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=2460
  endOffset: 2640
- name: 'Organizational Adoption: Cross-Functional Collaboration and Change Management'
  startOffset: 2640
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=2640
  endOffset: 2790
- name: 'Skillset Blend: Data Science, Operations Research, and Software Engineering'
  startOffset: 2790
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=2790
  endOffset: 2940
- name: 'Common Pitfalls: Mis-specified Objectives and Overfitting Decisions'
  startOffset: 2940
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=2940
  endOffset: 3090
- name: 'Robustness vs. Optimality: Trade-offs in Decision Optimization'
  startOffset: 3090
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=3090
  endOffset: 3240
- name: 'Future Trends: Automated Decisioning and Prescriptive Systems'
  startOffset: 3240
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=3240
  endOffset: 3390
- name: 'Resources: Learning Paths, Libraries, and Community Recommendations'
  startOffset: 3390
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=3390
  endOffset: 3570
- name: 'Practical Advice: Getting Started with Decision Optimization Projects'
  startOffset: 3570
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=3570
  endOffset: 3720
- name: Episode Wrap-Up and Links
  startOffset: 3720
  url: https://www.youtube.com/watch?v=SJuzQ4bcU2c&t=3720
  endOffset: 3720

transcript:
- line: "I\u2019ll just do a short introduction. On the slide, it says \"Decision\
    \ Optimization\" because the full title was too long to fit. The actual title\
    \ is \"Translating Machine Learning Predictions Into Better Real-World Results\
    \ with Decision Optimization.\" This talk will be different from the three previous\
    \ ones we had."
  sec: 0
  time: 0:00
  who: Alexey
- line: The other talks were presentations with a Q&A session afterward. This one
    will be a live discussion and will also be released as a podcast on DataTalks.Club.
    Unlike the previous videos, this will not have a video version on YouTube; it
    will be just a conversation.
  sec: 27
  time: 0:27
  who: Alexey
- line: For those who just joined, we use Slido for questions. On YouTube, in the
    chat section, there is a pinned message that links to Slido. At any time during
    the chat, you can submit a question there, and we will cover it eventually.
  sec: 54
  time: 0:54
  who: Alexey
- line: Are you ready to start?
  sec: 79
  time: '1:19'
  who: Alexey
- line: Dan
  sec: 79
  time: '1:19'
  who: Alexey
- line: "Yes, let\u2019s start. Today we\u2019ll talk about decision optimization,\
    \ which is about making better decisions with machine learning. We have a special\
    \ guest today, Dan, the founder of Decision AI, a company that specializes in\
    \ improving decisions using machine learning."
  sec: 79
  time: '1:19'
  who: Alexey
- line: Welcome, Dan. Glad to have you here.
  sec: 107
  time: '1:47'
  who: Alexey
- line: Dan
  sec: 107
  time: '1:47'
  who: Alexey
- line: Thanks for having me.
  sec: 107
  time: '1:47'
  who: Alexey
- line: Before we dive into decision optimization, can you tell us a bit about your
    career journey so far?
  sec: 113
  time: '1:53'
  who: Alexey
- line: I got started with machine learning in 2000 at a startup that used it to help
    companies optimize their listings on eBay, which was very popular at the time.
    We worked on machine learning for about six months, but it was a complete failure.
    The predictions were not good, and no one really understood what we were doing.
    After six months, we switched to using simple descriptive statistics.
  sec: 124
  time: '2:04'
  who: Dan
- line: "After about nine months, I thought machine learning would never catch on.\
    \ It sounded cool, but I didn\u2019t think it would work. I then pursued a PhD\
    \ in econometrics. In 2009, I started competing in Kaggle competitions, initially\
    \ using simple statistical techniques because I thought they were best. I quickly\
    \ realized I was in almost last place, and machine learning had improved dramatically."
  sec: 153
  time: '2:33'
  who: Dan
- line: I eventually got second place in a competition with a three million dollar
    grand prize, but no prize for second place. That experience led me to consulting
    with several large companies. I was an early employee at DataRobot, worked at
    Google as a data scientist, and in January 2020, I started Decision AI.
  sec: 187
  time: '3:07'
  who: Dan
- line: You started Kaggle in 2009?
  sec: 218
  time: '3:38'
  who: Alexey
- line: Dan
  sec: 218
  time: '3:38'
  who: Alexey
- line: Yes, it was probably one of the first competitions. At the time, there were
    maybe three million users, and my user ID was 9000 or 9028. I did well in the
    early competitions, but now the competitions are much more complex and competitive.
  sec: 218
  time: '3:38'
  who: Alexey
- line: I started Kaggle in 2015, six years after it launched. Looking at competitions
    now, they are very different from what existed five years ago. The level of complexity
    has increased significantly.
  sec: 254
  time: '4:14'
  who: Alexey
- line: What is decision optimization, and what kind of problems does it solve?
  sec: 274
  time: '4:34'
  who: Alexey
- line: Dan
  sec: 274
  time: '4:34'
  who: Alexey
- line: Decision optimization is essentially what it sounds like. One way to explain
    it is through an example. A few years ago, I spoke with a friend at a large U.S.
    airline. Their most important problem was setting prices for flights.
  sec: 274
  time: '4:34'
  who: Alexey
- line: "They built machine learning models to predict how many tickets would sell\
    \ at different prices. For instance, Flight 1000 leaves on March 20th. If the\
    \ price is $400, how many tickets will sell before the next price reset? If it\u2019\
    s $500, how many will sell?"
  sec: 311
  time: '5:11'
  who: Dan
- line: Decision optimization takes this a step further. You have a machine learning
    model that predicts sales, something you control like price, and an objective
    like total revenue. The challenge is choosing the price to maximize that objective,
    considering real-world dynamics over time.
  sec: 351
  time: '5:51'
  who: Dan
- line: For example, if you have 90 days before the flight, tickets not sold today
    can be sold tomorrow. If you lower your price too much today, competitors may
    lower theirs, reducing future revenue. Decision optimization answers which action
    maximizes overall performance.
  sec: 387
  time: '6:27'
  who: Dan
- line: So, you are saying we have a model, like a demand forecasting model, that
    predicts seat sales for a given price. The problem is how to use this model most
    effectively, avoiding losses from selling tickets too cheaply or too quickly.
  sec: 421
  time: '7:01'
  who: Alexey
- line: Does this also work for other types of problems? As a data scientist, I mostly
    deal with binary classification problems, which I think are common in industry.
  sec: 469
  time: '7:49'
  who: Alexey
- line: Yes, let me give two examples. For a binary classification problem like financial
    fraud, a company provides an API that predicts the probability a transaction is
    fraudulent.
  sec: 504
  time: '8:24'
  who: Dan
- line: 'The question becomes: what do I do with a transaction that is five percent
    likely to be fraud? Accept, reject, or investigate further? In the simplest case,
    decision optimization helps set the threshold for rejecting or accepting transactions.'
  sec: 538
  time: '8:58'
  who: Dan
- line: This approach can significantly impact business outcomes. For example, an
    insurance company uses a threshold to decide which claims to investigate. Even
    a simple threshold-based system can be very effective.
  sec: 586
  time: '9:46'
  who: Dan
- line: However, in practice, the situation is more complex. If a claim comes in for
    50,000 euros and is nine percent likely to be fraud, they might not investigate
    because it is under the 10 percent threshold. Meanwhile, smaller claims may be
    treated differently.
  sec: 633
  time: '10:33'
  who: Dan
- line: "It is 10 percent likely to be fraud, so the expected value of investigating\
    \ the nine percent is about 98 to 90 times as large. Yet they don\u2019t do it\
    \ because we focus too narrowly on classification output as a probability. I see\
    \ people make mistakes because they look at that probability in isolation."
  sec: 661
  time: '11:01'
  who: Dan
- line: "Shouldn\u2019t the model be able to account for the potential value, like\
    \ considering the sum or overall impact?"
  sec: 699
  time: '11:39'
  who: Alexey
- line: The probability reflects the likelihood of fraud, but the value of investigating
    is different. For example, if a claim is 10 percent likely to be fraud and the
    amount is 500 euros, the expected value of investigating is 50 euros. So, even
    a small probability can have a high expected value depending on the amount at
    risk.
  sec: 705
  time: '11:45'
  who: Dan
- line: We need to do more than just look at probability. At the company where I work,
    we have a moderation team for an online classifieds platform. When someone publishes
    a listing, we want to determine if it is fraudulent. If it is, a moderator or
    automated system investigates.
  sec: 744
  time: '12:24'
  who: Dan
- line: Typically, we set thresholds using precision and recall. We choose a level
    of precision and recall that is acceptable and then roll out the system. However,
    this is not always ideal.
  sec: 780
  time: '13:00'
  who: Dan
- line: How should we approach this differently with decision optimization?
  sec: 812
  time: '13:32'
  who: Alexey
- line: Decision optimization is very use case specific. Not every type of fraud has
    the same impact on a platform. You need to assign a value or cost to each type
    of fraud.
  sec: 817
  time: '13:37'
  who: Dan
- line: For example, one post might be a 10,000 euro racing bike, and another a low-cost
    cruiser. Even if the probability of fraud is the same, the potential loss is different.
    You might prioritize protecting someone from losing the high-value item.
  sec: 843
  time: '14:03'
  who: Dan
- line: Similarly, if the risk is physical harm, you need stricter standards to protect
    users. The point is to assign values to different outcomes. Once you do that,
    you can incorporate those values into your decision-making.
  sec: 903
  time: '15:03'
  who: Dan
- line: 'So we attach a value to each item or post. Next, we combine that with predictions
    from our classifier. We now have two inputs: predicted probability and value.
    The goal is to create a single decision function that takes these factors into
    account.'
  sec: 927
  time: '15:27'
  who: Dan
- line: Think of it like a Python function. It takes inputs like the probability of
    fraud, type of fraud, and item value. You may also factor in customer importance
    or repeated behavior. The output is accept, reject, or manually investigate.
  sec: 960
  time: '16:00'
  who: Dan
- line: This process varies by company. At tech companies, banks, and insurance companies,
    the people making decisions differ, but the idea is the same. You simulate what
    happens under different decision rules.
  sec: 1011
  time: '16:51'
  who: Dan
- line: We propose several decision rules and see which outcome is most desirable.
    You may also consider metrics like lifetime value or the fraction of users who
    leave if upset. Data scientists often make the mistake of treating each model
    in isolation.
  sec: 1050
  time: '17:30'
  who: Dan
- line: For example, one model predicts likelihood of fraud, another predicts user
    reaction to a mistake, and a third predicts lifetime value. You combine these
    models to evaluate overall outcomes. Each piece interacts to reflect the real-world
    system.
  sec: 1095
  time: '18:15'
  who: Dan
- line: At the end of the day, you want a single decision function, like a Python
    function, to simulate outcomes and see which one is best. You can use it for actual
    decisions or experimentation.
  sec: 1132
  time: '18:52'
  who: Dan
- line: In some businesses, A/B testing is possible and reliable. If not, you can
    simulate outcomes in a sandbox environment. The simulator allows you to try different
    decision functions and see their results.
  sec: 1159
  time: '19:19'
  who: Dan
- line: The decision function is what will eventually be deployed. You can play with
    its parameters and observe outcomes. This is the core of what Decision AI specializes
    in.
  sec: 1209
  time: '20:09'
  who: Dan
- line: "Who should create this decision function? Should a data scientist define\
    \ it? Isn\u2019t the point of machine learning to avoid hand-crafting rules?"
  sec: 1241
  time: '20:41'
  who: Alexey
- line: 'If you were to build a machine learning model, what is the API for it? Whether
    you use XGBoost, TensorFlow, or scikit-learn, one thing is consistent: it always
    has .predict. In plain English, that answers the question: what will happen?'
  sec: 1266
  time: '21:06'
  who: Alexey
- line: 'Compared to models from ten years ago, like linear regression or GLMs, we
    used to do a lot of manual feature engineering to improve predictions. Machine
    learning automates the prediction of what will happen. Now, the question becomes:
    what should I do about it? This is very different from predicting outcomes.'
  sec: 1285
  time: '21:25'
  who: Dan
- line: "Reinforcement learning is a branch of machine learning that optimizes an\
    \ objective in a complex environment. Someday, reinforcement learning might solve\
    \ this for us, but we are not there yet. Most breakthroughs in reinforcement learning,\
    \ like AlphaGo or OpenAI\u2019s Dota agent, are in games because games provide\
    \ a simulator."
  sec: 1318
  time: '21:58'
  who: Dan
- line: Simulators let you try different rules in a dynamic environment and see how
    things progress over time. We need similar simulators for dynamic real-world environments.
    You cannot take a conventional supervised model and expect it to optimize for
    broader objectives. Supervised models are too narrow for this purpose.
  sec: 1383
  time: '23:03'
  who: Dan
- line: "So ideally, we would combine our supervised models into one decision function\
    \ and train it with reinforcement learning, but we don\u2019t yet have an environment\
    \ to experiment properly."
  sec: 1441
  time: '24:01'
  who: Alexey
- line: Exactly. Right now, we usually do this manually, encoding rules in a decision
    function. Some rules are straightforward to humans but hard for machine learning
    to learn. The best approach often combines known rules with machine learning predictions.
  sec: 1467
  time: '24:27'
  who: Dan
- line: For example, in the airline case, total flight revenue is the sum of revenue
    today, tomorrow, and the next day until departure. This is simple to encode manually,
    but a machine learning model would struggle without the right data. In many cases,
    domain knowledge is more efficient than learning everything from scratch.
  sec: 1500
  time: '25:00'
  who: Dan
- line: The person providing the knowledge is often a domain expert, not a data scientist.
    The process varies by company. In tech companies, a data scientist may collaborate
    with a product manager. In other businesses, it could involve multiple stakeholders.
  sec: 1573
  time: '26:13'
  who: Dan
- line: What tools are available for this? Does your company develop a tool for decision
    optimization?
  sec: 1638
  time: '27:18'
  who: Alexey
- line: Yes. The primitives are your machine learning models. Then we have a simulator
    that incorporates these models to predict business-level outcomes over time. The
    last part is the decision function. You take your models and drag them into our
    web-based app.
  sec: 1671
  time: '27:51'
  who: Dan
- line: We have a domain-specific language to write formulas. For example, the number
    of tickets to sell today is whatever we had yesterday minus what we sold. You
    use .predict from the demand model as input to these formulas.
  sec: 1714
  time: '28:34'
  who: Dan
- line: The simulator propagates information over time. It calculates outcomes for
    day one, then simulates day two based on competitor prices and previous sales.
    It continues sequentially, propagating predictions through all days.
  sec: 1766
  time: '29:26'
  who: Dan
- line: The exact behavior depends on the formulas you wrote. You could implement
    this in raw Python, but iterating is difficult. You need to calculate in the correct
    order, piping information from one calculation to the next like a DAG. Maintaining
    it manually is very complex and messy.
  sec: 1802
  time: '30:02'
  who: Dan
- line: Solving this iteration and maintenance problem is the main challenge we addressed
    with our tool.
  sec: 1913
  time: '31:53'
  who: Dan
- line: With your domain-specific language, how are companies using this now? Are
    there open-source tools, or do people just write scripts in Python to create these
    environments?
  sec: 1918
  time: '31:58'
  who: Alexey
- line: "There aren\u2019t great tools for this, which is why I started Decision AI.\
    \ At Google and a few other companies, people either use raw Python with NumPy\
    \ for calculations and connect it to TensorFlow or PyTorch models at a low level.\
    \ I\u2019ve also seen proof-of-concept approaches using probabilistic programming\
    \ languages like PyMC3 or Pyro."
  sec: 1936
  time: '32:16'
  who: Dan
- line: So basically, the alternatives are to encode everything manually, or in large
    companies, maybe they have internal tools. This is not widely available for general
    use. Our software, by the way, is available for free, and it allows iteration
    10 to 30 times faster than raw Python.
  sec: 1996
  time: '33:16'
  who: Dan
- line: 'Coming back to your career journey: you competed on Kaggle, worked at DataRobot,
    then Google, and now started Decision AI. What did you see in the industry that
    led you to start your company?'
  sec: 2040
  time: '34:00'
  who: Alexey
- line: I noticed that in 80 percent of applications, there is an important decision
    step after the machine learning model that is often overlooked. Many data scientists
    deploy models, but the business may not use the results effectively. For example,
    at the airline my friend worked for, his group was cut because they could not
    turn accurate predictions into better business outcomes.
  sec: 2077
  time: '34:37'
  who: Dan
- line: Twenty percent of the time, turning predictions into decisions is almost obvious.
    But 80 percent of the time, there is a huge opportunity. Machine learning has
    improved predictions significantly, but how you use those predictions is equally
    important. Five years ago, when I suggested simulation approaches we used at Google,
    people found it too manual.
  sec: 2130
  time: '35:30'
  who: Dan
- line: "Custom tooling didn\u2019t exist, so I realized there was a strong need to\
    \ help people make better decisions with models. Researchers focus on small improvements\
    \ in accuracy, like 0.91 AUC instead of 0.9, but in practice, the value is often\
    \ wasted if the model is not used effectively."
  sec: 2179
  time: '36:19'
  who: Dan
- line: That resonates with what I see. How can a data scientist start integrating
    this into their workflow?
  sec: 2233
  time: '37:13'
  who: Alexey
- line: The first step is to start small. For a binary classification problem, talk
    to someone in the business to assign a monetary value to every false positive
    and true positive. You can transform your confusion matrix to see the impact of
    different thresholds.
  sec: 2277
  time: '37:57'
  who: Dan
- line: "For each threshold, calculate the expected monetary value. You don\u2019\
    t need our software for this\u2014it\u2019s the first step. Even thinking rigorously\
    \ about optimizing the decision threshold, the same way you are analytical in\
    \ modeling, is very valuable."
  sec: 2340
  time: '39:00'
  who: Dan
- line: "If there are dynamics over time, like customers reacting differently tomorrow\
    \ based on today\u2019s decisions, a simple threshold isn\u2019t optimal. Being\
    \ rigorous about a single threshold is better than nothing, but letting it vary\
    \ with the environment is the next step."
  sec: 2381
  time: '39:41'
  who: Dan
- line: I recommend checking out our software. You can use it for free. Even the mindset
    shift of optimizing how you use your model for decisions is valuable. It lets
    you model impacts on profit and loss or customer metrics a year from now.
  sec: 2412
  time: '40:12'
  who: Dan
- line: Even imperfect modeling is a big step forward. Our website is www.decision.ai
  sec: 2452
  time: '40:52'
  who: Dan
- line: ', and our app is app.decision.ai. I recommend exploring it to approximate
    the ideal way to make decisions based on your models.'
  sec: 2452
  time: '40:52'
  who: Dan
- line: I actually meant slightly different thing
  sec: 2483
  time: '41:23'
  who: Alexey
- line: Let's say we've played with Decision AI and found a reasonably good decision
    function. Do we export it and use it in production, or how does it look?
  sec: 2489
  time: '41:29'
  who: Alexey
- line: Dan
  sec: 2489
  time: '41:29'
  who: Alexey
- line: "Yes, you deploy it as a RESTful endpoint. It takes the inputs for your decision\
    \ function and returns the output, like \u201Caccept this transaction\u201D or\
    \ \u201Creject it.\u201D From the rest of the organization\u2019s perspective,\
    \ it looks like a standard web service or REST API. Features go in, decisions\
    \ come out."
  sec: 2489
  time: '41:29'
  who: Alexey
- line: Who usually drives this initiative? Should it come from a data scientist or
    a product manager? How can a data scientist explain the value of this?
  sec: 2544
  time: '42:24'
  who: Alexey
- line: Dan
  sec: 2544
  time: '42:24'
  who: Alexey
- line: "I think the initiative should come from the data scientist. They spend a\
    \ lot of time building models, and if those models don\u2019t pay off in business\
    \ terms, both the data scientists and product managers are in trouble. Increasingly,\
    \ companies expect data scientists to prove that their models drive profitability,\
    \ not just improve RMSE or accuracy."
  sec: 2544
  time: '42:24'
  who: Alexey
- line: "It\u2019s an analytical process. The data scientist should lead and bring\
    \ in the product manager. Start by asking what the product manager cares about\u2014\
    daily active users, revenue, or some other metric. Then map predictions from your\
    \ model to that metric. For example, certain types of fraud may hurt reputation,\
    \ which affects daily active users. Once you show that you can optimize for the\
    \ metric the business cares about, it\u2019s usually an easy sell."
  sec: 2634
  time: '43:54'
  who: Dan
- line: "So it\u2019s about metrics the business cares about, not just accuracy?"
  sec: 2705
  time: '45:05'
  who: Alexey
- line: Dan
  sec: 2705
  time: '45:05'
  who: Alexey
- line: Exactly. Accuracy is only a tool. Start with what the business cares about,
    and ensure the model helps achieve those goals.
  sec: 2705
  time: '45:05'
  who: Alexey
- line: "Are there cases where we don\u2019t need decision optimization?"
  sec: 2761
  time: '46:01'
  who: Alexey
- line: Dan
  sec: 2761
  time: '46:01'
  who: Alexey
- line: "Yes. Sometimes predictions are \u201Cgood enough.\u201D For example, recommendation\
    \ systems like Netflix or YouTube: predicting user engagement over the next six\
    \ months can be accurate enough that a separate decision function isn\u2019t needed."
  sec: 2761
  time: '46:01'
  who: Alexey
- line: However, these are often very complex models, like YouTube-scale recommendation
    engines. In such cases, the decision logic is already largely encoded in the model.
  sec: 2882
  time: '48:02'
  who: Dan
- line: What trends do you see in data science as a whole?
  sec: 2940
  time: '49:00'
  who: Alexey
- line: Dan
  sec: 2940
  time: '49:00'
  who: Alexey
- line: I work a lot with tabular data, especially supply chain problems. A major
    concern is train-test drift or concept drift. Historical data may no longer represent
    current conditions, especially after events like 2020. Models trained on older
    data can fail to predict accurately, so businesses must adjust quickly.
  sec: 2940
  time: '49:00'
  who: Alexey
- line: Common approaches include model monitoring, where you track performance or
    covariate shift daily. If the current data differs from the training data, the
    model must be refreshed quickly. Another approach is programmatic adjustments
    based on hypotheses about how the world is changing. Explainability techniques,
    like SHAP values, can help make these adjustments.
  sec: 3037
  time: '50:37'
  who: Dan
- line: "The world changes quickly, so it\u2019s crucial to be thoughtful about relying\
    \ on historical data. This is a complex problem for both data scientists and businesses."
  sec: 3239
  time: '53:59'
  who: Dan
- line: How can people find you?
  sec: 3253
  time: '54:13'
  who: Alexey
- line: Dan
  sec: 3253
  time: '54:13'
  who: Alexey
- line: "On Twitter, I\u2019m @dansbecker, and I\u2019m on LinkedIn as the founder\
    \ of Decision AI. If you have a data science challenge, feel free to reach out\u2014\
    I enjoy discussing how to use machine learning effectively. Email me at dan@decision.ai"
  sec: 3253
  time: '54:13'
  who: Alexey
- line: .
  sec: 3253
  time: '54:13'
  who: Alexey
- line: "Thanks for joining us. This was a long session, but very informative. For\
    \ those who stayed until the end, thanks a lot. For those who didn\u2019t, the\
    \ video will be uploaded separately. See you next week."
  sec: 3303
  time: '55:03'
  who: Alexey
---
