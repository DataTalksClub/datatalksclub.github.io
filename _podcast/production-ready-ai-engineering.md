---
title: 'Production AI Engineering: Data Pipelines, Prompt Optimization and Caching'
short: Data Intensive AI
season: 20
episode: 5
guests:
- bartoszmikulski
image: images/podcast/s20e05-data-intensive-ai.jpg
ids:
  anchor: atalksclub/episodes/Data-Intensive-AI---Bartosz-Mikulski-e30fhoi
  youtube: BP6w_vKySN0
links:
  anchor: https://creators.spotify.com/pod/show/datatalksclub/episodes/Data-Intensive-AI---Bartosz-Mikulski-e30fhoi
  apple: https://podcasts.apple.com/us/podcast/data-intensive-ai-bartosz-mikulski/id1541710331?i=1000700288876
  spotify: https://open.spotify.com/episode/0nFSU92IQDbM4C9FLvdn4z
  youtube: https://www.youtube.com/watch?v=BP6w_vKySN0
description: 'Master production AI engineering: build scalable data pipelines, optimize
  prompts, and implement caching to cut latency and costs for production-ready models'
intro: How do you move AI projects from proof-of-concept to reliable production systems
  while keeping prompts, pipelines, and response times under control? In this episode
  Bartosz Mikulski, an AI and data engineer who specializes in productionizing AI,
  breaks down the engineering work required to make models dependable beyond demos.
  Bartosz explains how to design robust data pipelines, apply prompt optimization
  practices, and introduce caching strategies that reduce load and improve responsiveness.
  He also covers building testing infrastructure and using tests to surface issues
  that block production readiness—then how to fix those issues. Listeners will get
  concrete, engineering-focused insights into production AI, including practical approaches
  to pipeline orchestration, prompt tuning for stability, and where caching fits in
  an operational stack. Whether you’re responsible for deploying models, improving
  inference reliability, or creating reproducible pipelines, this conversation offers
  actionable techniques and perspectives for turning experiments into maintainable
  production systems.
dateadded: 2025-03-26
duration: PT01H01M37S
quotableClips:
- name: Episode Opening & Guest Overview (Data Intensive AI)
  startOffset: 0
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=0
  endOffset: 122
- name: Book Contribution Clarified & Testing Focus
  startOffset: 122
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=122
  endOffset: 240
- name: 'Career Path: Java → Data Engineering → AI Engineering'
  startOffset: 240
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=240
  endOffset: 364
- name: 'Publishing Routine: Blogging Frequency & Content Practice'
  startOffset: 364
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=364
  endOffset: 545
- name: 'Data Trust: Why Testing Prevents "This Number Doesn’t Look Correct"'
  startOffset: 545
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=545
  endOffset: 707
- name: 'Test Strategy for Data Pipelines: Snapshot & Integration Testing'
  startOffset: 707
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=707
  endOffset: 794
- name: 'Testing Tools: Great Expectations, Soda, SQL Tests vs Spark Tests'
  startOffset: 794
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=794
  endOffset: 1030
- name: 'Technology Choice: When to Use Apache Spark'
  startOffset: 1030
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=1030
  endOffset: 1118
- name: 'Data Engineering’s Role in AI: Preprocessing & Fine-Tuning Data'
  startOffset: 1118
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=1118
  endOffset: 1306
- name: 'Invisible AI Use Cases: Augmented Generation & Review Analysis'
  startOffset: 1306
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=1306
  endOffset: 1513
- name: 'Prompt Engineering Basics: In-Context Learning & Examples'
  startOffset: 1513
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=1513
  endOffset: 1696
- name: 'Prompt Evaluation: Formatting, Examples, and Cost Tradeoffs'
  startOffset: 1696
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=1696
  endOffset: 1800
- name: 'Prompt Compression: Token Optimization Techniques'
  startOffset: 1800
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=1800
  endOffset: 1905
- name: Prompt Caching & Model Efficiency (attention caching, Claude)
  startOffset: 1905
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=1905
  endOffset: 2022
- name: Open-Source Models & Tools Experience (DeepSeek, Perplexity)
  startOffset: 2022
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=2022
  endOffset: 2154
- name: 'AI for Lead Scoring: LinkedIn Automation & Qualification'
  startOffset: 2154
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=2154
  endOffset: 2464
- name: 'Chrome Extension Architecture: Backend AI Integration Pattern'
  startOffset: 2464
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=2464
  endOffset: 2525
- name: 'Coding Assistants: Cursor Workflow & Productivity Boosts'
  startOffset: 2525
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=2525
  endOffset: 2678
- name: 'Code AI Comparison: Cursor vs GitHub Copilot & Alternatives'
  startOffset: 2678
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=2678
  endOffset: 2839
- name: 'Search-Focused Assistants: Using Perplexity & Tool Selection'
  startOffset: 2839
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=2839
  endOffset: 3129
- name: 'Website Hosting: Static Site Generators & GitHub Pages'
  startOffset: 3129
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=3129
  endOffset: 3190
- name: 'Blogging as Business: Attracting Clients & Teaching Workshops'
  startOffset: 3190
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=3190
  endOffset: 3377
- name: 'AI-Assisted Writing: Drafting, Rewriting, and Maintaining Voice'
  startOffset: 3377
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=3377
  endOffset: 3621
- name: Episode Wrap-Up & Guest Resources (blog link invitation)
  startOffset: 3621
  url: https://www.youtube.com/watch?v=BP6w_vKySN0&t=3621
  endOffset: 3697
transcript:
- header: Episode Opening & Guest Overview (Data Intensive AI)
- line: This week, we’ll talk about Data Intensive AI. Our special guest today is
    Bartosz, an AI and data engineer. He specializes in moving AI projects from the
    “good enough for a demo” phase to production by building testing infrastructure
    and fixing issues detected by tests.
  sec: 0
  time: 0:00
  who: Alexey
- line: He also teaches programmers and non-programmers how to use AI. He is a public
    speaker and contributed to a book called 97 Things Every Data Engineer Should
    Know.
  sec: 0
  time: 0:00
  who: Alexey
- header: Book Contribution Clarified & Testing Focus
- line: There’s a mistake there. It should be 97 Things Every Data Engineer Should
    Know.
  sec: 122
  time: '2:02'
  who: Bartosz
- line: Okay, 97 Things Every Data Engineer Should Know.
  sec: 127
  time: '2:07'
  who: Alexey
- line: Yeah, I noticed.
  sec: 132
  time: '2:12'
  who: Bartosz
- line: Actually, my colleague—now working at Meta—also wrote a chapter there.
  sec: 138
  time: '2:18'
  who: Alexey
- line: It was a very community-based book.
  sec: 145
  time: '2:25'
  who: Bartosz
- line: Was it 97 authors?
  sec: 153
  time: '2:33'
  who: Alexey
- line: Bartosz
  sec: 153
  time: '2:33'
  who: Alexey
- line: No, some people contributed two or three chapters. I contributed one chapter
    about testing.
  sec: 153
  time: '2:33'
  who: Alexey
- line: So you like testing?
  sec: 167
  time: '2:47'
  who: Alexey
- line: Bartosz
  sec: 167
  time: '2:47'
  who: Alexey
- line: Well, “like” might not be the right word, but I do it.
  sec: 167
  time: '2:47'
  who: Alexey
- line: You don’t dislike it?
  sec: 175
  time: '2:55'
  who: Alexey
- line: Bartosz
  sec: 175
  time: '2:55'
  who: Alexey
- line: I accept that you have to do it.
  sec: 175
  time: '2:55'
  who: Alexey
- line: Many engineers and developers aren’t fans of writing tests. I sometimes start
    a project, and when it works, I realize it might fall apart later. That’s when
    I start covering it with tests. I don’t follow test-driven development because
    it’s difficult for me to start with a test and make sure it fails.
  sec: 183
  time: '3:03'
  who: Alexey
- line: We’ll talk about that later. Anyway, welcome to the interview, Bartosz!
  sec: 183
  time: '3:03'
  who: Alexey
- header: 'Career Path: Java → Data Engineering → AI Engineering'
- line: I started as a Java developer many years ago, working in the banking industry.
    Then, I moved to data engineering. Initially, I wanted to be a data scientist
    and worked at a startup for a few months. I realized it wasn’t for me, and I preferred
    putting things into production.
  sec: 240
  time: '4:00'
  who: Bartosz
- line: So, I switched to data engineering and stayed in that area until I discovered
    MLOps. That allowed me to go back to machine learning but focus on deployment
    and testing. Now, I’ve shifted more toward AI engineering, still related to data.
    I avoid building backend applications or data pipelines unless they involve AI.
  sec: 240
  time: '4:00'
  who: Bartosz
- line: Interesting. We had a similar career path. I also started with Java development
    and worked at a bank in Poland. Then, I thought, “Enough of banking,” and moved
    into data science.
  sec: 326
  time: '5:26'
  who: Alexey
- header: 'Publishing Routine: Blogging Frequency & Content Practice'
- line: Yes, I try to publish every week. I can’t always promise that, but the goal
    is to publish something every Monday. I’ve tried different schedules, like publishing
    three times a week or even daily for 100 days. That was a terrible experience,
    but also interesting because I ran out of ideas after 20 days.
  sec: 364
  time: '6:04'
  who: Bartosz
- line: Did you write a blog post every day for 100 days?
  sec: 415
  time: '6:55'
  who: Alexey
- line: I published every day for 100 days. I tried to write two posts daily to have
    backups for later.
  sec: 422
  time: '7:02'
  who: Bartosz
- line: During COVID, I tried posting on LinkedIn every day for 100 days. After 20
    days, I struggled to find ideas. It was exhausting, and I burned out after a month.
  sec: 434
  time: '7:14'
  who: Alexey
- line: Now, I publish every weekday on LinkedIn. It’s easier because there’s a lot
    happening in AI to write about.
  sec: 469
  time: '7:49'
  who: Bartosz
- line: How long have you been writing?
  sec: 482
  time: '8:02'
  who: Alexey
- line: I’ve been blogging since 2017. I have around 480 articles.
  sec: 487
  time: '8:07'
  who: Bartosz
- line: Now you write about AI. Before, you wrote about data engineering and data
    science?
  sec: 522
  time: '8:42'
  who: Alexey
- line: Yes, I wrote about whatever I was doing at work.
  sec: 528
  time: '8:48'
  who: Bartosz
- header: 'Data Trust: Why Testing Prevents "This Number Doesn’t Look Correct"'
- line: How did you end up writing a chapter for 97 Things Every Data Engineer Should
    Know?
  sec: 545
  time: '9:05'
  who: Alexey
- line: I noticed they were looking for contributions, so I volunteered. I used one
    of my blog articles as the submission.
  sec: 552
  time: '9:12'
  who: Bartosz
- line: So you already had the article and just sent it?
  sec: 578
  time: '9:38'
  who: Alexey
- line: Yes, it was accepted.
  sec: 581
  time: '9:41'
  who: Bartosz
- line: What are the 97 things every data engineer should know?
  sec: 586
  time: '9:46'
  who: Alexey
- line: 'In my case, it was about the few words you don’t want to hear as a data engineer:
    “This number doesn’t look correct on the dashboard.” Because you immediately lose
    trust. If you make a mistake like this, it’s hard to regain trust. So, it was
    an observation that led me to write about the importance of testing and verification
    of data.'
  sec: 591
  time: '9:51'
  who: Bartosz
- line: Why is testing important? Why should we run tests, and how is it related to
    the phrase, “This number doesn’t look correct”?
  sec: 626
  time: '10:26'
  who: Alexey
- line: If you have a data pipeline and cannot prove it works correctly, how can you
    prove the number is correct? You need tests to prove it. Of course, we can argue
    whether software tests prove something works correctly or just show that you can’t
    detect a bug with what you have. But at least you have tests. Then, we can wonder
    if they are good enough.
  sec: 644
  time: '10:44'
  who: Bartosz
- line: So, at least you have some certainty that your pipeline is functional. If
    someone says, “This number doesn’t look correct,” you can say, “It should be right,
    but let’s figure out why you think so.”
  sec: 679
  time: '11:19'
  who: Alexey
- line: Yes, and you have something to rely on while debugging. If you see a behavior
    and there’s a test for it, you know it was intentional. Otherwise, you might not
    remember what it’s supposed to do.
  sec: 691
  time: '11:31'
  who: Bartosz
- header: 'Test Strategy for Data Pipelines: Snapshot & Integration Testing'
- line: How do we create tests for data pipelines?
  sec: 707
  time: '11:47'
  who: Alexey
- line: As you said earlier, it’s often easier not to start with tests, and I agree.
    Make the pipeline run first, then observe the outputs and decide what’s acceptable.
    Use those as test cases. It might not catch all errors immediately, especially
    edge cases, but it’s a starting point.
  sec: 713
  time: '11:53'
  who: Bartosz
- line: I used to think test-driven development was the way to go, but I don’t push
    it anymore. It’s more difficult in data engineering because you’re not testing
    a single function. You’re dealing with pipelines, data preparation, and more.
    It’s not as convenient.
  sec: 713
  time: '11:53'
  who: Bartosz
- header: 'Testing Tools: Great Expectations, Soda, SQL Tests vs Spark Tests'
- line: What kind of tools do you use? Correct me if I’m wrong, but are you suggesting
    we create a data pipeline and then use integration tests rather than unit tests?
  sec: 794
  time: '13:14'
  who: Alexey
- line: I’m not sure if integration tests are the right way, but unit tests are less
    useful in data pipelines. We have to do integration testing.
  sec: 812
  time: '13:32'
  who: Bartosz
- line: So, we run sample data through the pipeline and check if the output matches
    our expectations?
  sec: 830
  time: '13:50'
  who: Alexey
- line: Yes, that’s snapshot testing. If you prepare the input data and know what
    to expect, you can name the tests according to the business rules they’re testing.
    It’s like test-driven development but not in the same order.
  sec: 844
  time: '14:04'
  who: Bartosz
- line: If your data pipeline is a Python script using pandas, it’s relatively easy
    to test. You can create functions and test them with unit tests. For Spark jobs,
    it’s less straightforward, but you can isolate functions like UDFs. Are there
    tools that work for all data pipelines, or how do you choose the tools?
  sec: 877
  time: '14:37'
  who: Alexey
- line: I work with Spark, so I configure the pipeline externally and switch between
    production and test environments. I’m not aware of tools for every type of pipeline,
    but Spark works for me.
  sec: 917
  time: '15:17'
  who: Bartosz
- line: At my previous company, we used SQL queries, which were harder to test. We
    used tools like Great Expectations and Soda to define checks and run them after
    each pipeline step. These checks included things like the number of columns or
    ensuring no null values.
  sec: 949
  time: '15:49'
  who: Alexey
- line: Those are basic tests. If you’re doing a join and missing columns, you can
    catch errors. Great Expectations is one tool for this. For SQL, you can use templating
    to replace table names with test tables.
  sec: 993
  time: '16:33'
  who: Bartosz
- header: 'Technology Choice: When to Use Apache Spark'
- line: Would you recommend Spark for new projects, or would you choose something
    else?
  sec: 1030
  time: '17:10'
  who: Alexey
- line: I’d use Spark because I’m familiar with it. If your team knows Spark, it’s
    still a great tool. If not, there are many tutorials to learn from. Spark is here
    to stay, at least for some time.
  sec: 1038
  time: '17:18'
  who: Bartosz
- line: In our data engineering course, we still teach Spark. Companies still use
    it, but I wonder if it’s chosen for new projects. Sometimes, it’s easier to write
    SQL queries and run them on BigQuery or Snowflake.
  sec: 1061
  time: '17:41'
  who: Alexey
- line: At a previous company, we built an MLOps platform and used Spark for processing
    logs. It was a new project, and even though most developers had no Spark experience,
    it was still the easiest solution.
  sec: 1088
  time: '18:08'
  who: Bartosz
- header: 'Data Engineering’s Role in AI: Preprocessing & Fine-Tuning Data'
- line: We’ve talked a lot about data engineering. Now, let’s talk about AI. How is
    data engineering connected to AI tools and LLMs?
  sec: 1118
  time: '18:38'
  who: Alexey
- line: First, you can use AI in your pipeline, but it might not be the smartest idea
    because of hallucinations. You’ll have many bugs to fix. Another connection is
    fine-tuning models. You need a lot of data, and data engineers preprocess it.
    Even if you’re doing backend work, you’ll have logs to analyze. Data engineers
    are always needed, though the scale varies.
  sec: 1145
  time: '19:05'
  who: Bartosz
- line: I was also curious about your journey. How did you switch from data engineering
    to focusing more on AI? Was it something you liked, or was it something you needed
    to work on and learn? How did it happen for you?
  sec: 1218
  time: '20:18'
  who: Alexey
- line: I wanted to switch to data science from backend engineering but ended up in
    data engineering. Still, I wanted to do some machine learning, so I was kind of
    in between those two areas. Then, I moved into MLOps, which was what I wanted
    to do. It combined data engineering, machine learning, and some backend work I
    was familiar with.
  sec: 1236
  time: '20:36'
  who: Bartosz
- line: So, it was passion-driven? You wanted to do it, saw the opportunity, and thought,
    “This is something I want to try”?
  sec: 1261
  time: '21:01'
  who: Alexey
- line: Yes, you could say that. I was also not very satisfied with backend engineering
    anymore, so I wanted to try something new.
  sec: 1274
  time: '21:14'
  who: Bartosz
- line: Can you tell us about some use cases of AI that you’ve worked on?
  sec: 1299
  time: '21:39'
  who: Alexey
- header: 'Invisible AI Use Cases: Augmented Generation & Review Analysis'
- line: Of course, the typical use case is augmented generation. You have a database,
    and you use AI to generate content. But I think the most interesting use cases
    are when you don’t even see that AI is involved. For example, I worked on a project
    where we analyzed Google reviews for a company. The goal was to find out what
    worked and what didn’t at specific locations and generate a report.
  sec: 1306
  time: '21:46'
  who: Bartosz
- line: The AI was used in the background to analyze the data, but it wasn’t presented
    to the user as an AI feature. There was no “Summarize with AI” button. It just
    made the report more useful. I think this is the most useful way to use AI—don’t
    show everyone you’re using it, just make the product better.
  sec: 1306
  time: '21:46'
  who: Bartosz
- line: But marketing-wise, if you show people you use AI, it’s easier to attract
    investment and users.
  sec: 1405
  time: '23:25'
  who: Alexey
- line: 'It might be easier to attract investment, but is it easier to attract users?
    There are two camps: people who love everything with AI and people who absolutely
    hate it.'
  sec: 1416
  time: '23:36'
  who: Bartosz
- line: I live in Berlin, and now when I take the subway, I see banners like “Samsung
    with AI.” Everything needs AI now, apparently.
  sec: 1428
  time: '23:48'
  who: Alexey
- line: But do users need it? They might need it, but do they need to know they’re
    using it?
  sec: 1442
  time: '24:02'
  who: Bartosz
- line: Machine learning models have been around in mobile phones for quite some time,
    right?
  sec: 1452
  time: '24:12'
  who: Alexey
- line: Exactly. If you’re using an app where you can type something to search for
    pictures, that’s using machine learning to recognize what’s in the picture. But
    you don’t have to tell people it’s AI. It’s just a feature that works.
  sec: 1461
  time: '24:21'
  who: Bartosz
- line: Google does advertise their cameras with AI. I guess their marketing department
    knows what they’re doing.
  sec: 1480
  time: '24:40'
  who: Alexey
- line: Probably. But saying something is built with AI is like saying it was built
    with Spark. It’s just supposed to work.
  sec: 1492
  time: '24:52'
  who: Bartosz
- header: 'Prompt Engineering Basics: In-Context Learning & Examples'
- line: You’ve written a lot of posts—480, you said. One of them was about prompt
    engineering. Can you tell us more about the content of that post and what exactly
    makes a good prompt?
  sec: 1513
  time: '25:13'
  who: Alexey
- line: It depends on the model, but there are a few things that almost always work.
    One is called in-context learning, which is a fancy name for giving the model
    examples. This works because you’re showing the model what’s supposed to happen
    in a similar case, so it can imitate the response.
  sec: 1527
  time: '25:27'
  who: Bartosz
- line: This is especially useful if you need a specific format, like JSON with certain
    keys. Another technique is chain-of-thought prompting, but with reasoning models,
    it’s often built-in now.
  sec: 1527
  time: '25:27'
  who: Bartosz
- line: There are also prompting tricks, like saying “step by step” or offering a
    tip like “$100 for doing it right.” These are more like hacks that might work
    once for a specific model.
  sec: 1527
  time: '25:27'
  who: Bartosz
- line: Proper prompt engineering is more about providing examples and explaining
    how things are supposed to happen. This works for all models, though some might
    perform better than others.
  sec: 1527
  time: '25:27'
  who: Bartosz
- line: Is prompt engineering becoming less important for models like GPT-4 compared
    to GPT-3.5? GPT-3.5 was good but not as good as GPT-4, so we needed to spend more
    time crafting prompts. Now, it feels like GPT-4 just gets it.
  sec: 1643
  time: '27:23'
  who: Alexey
- line: It gets it, but if it doesn’t, you can either describe it in detail or provide
    an example. Examples work better. This is the in-context learning technique, which
    is essentially prompt engineering. We just gave it a fancy name.
  sec: 1665
  time: '27:45'
  who: Bartosz
- line: Can you give us an example of how to provide an example?
  sec: 1689
  time: '28:09'
  who: Alexey
- header: 'Prompt Evaluation: Formatting, Examples, and Cost Tradeoffs'
- line: Sure. Let’s say you’re analyzing the sentiment of a sentence. You have one
    review that’s positive and another that’s negative. You need the output in a specific
    format, like JSON.
  sec: 1696
  time: '28:16'
  who: Bartosz
- line: You can either explain, “Give me valid JSON with this key,” or you can provide
    an example of a review and the corresponding JSON output. The model will follow
    the format even if you don’t explicitly say, “I need JSON with this key.”
  sec: 1696
  time: '28:16'
  who: Bartosz
- line: But the bigger the prompt, the more money we pay. How many examples should
    we include? For sentiment analysis, is it enough to include one positive and one
    negative example? Or do we need more? If we do this at scale, including more examples
    in the prompt could become expensive.
  sec: 1773
  time: '29:33'
  who: Alexey
- header: 'Prompt Compression: Token Optimization Techniques'
- line: Yes, and that’s why I would recommend testing—or rather, gathering data from
    tests. You prepare an evaluation dataset with inputs and expected outputs, then
    measure how well the model performs. At some point, when you keep adding examples,
    the results will stop improving, so you can stop there.
  sec: 1800
  time: '30:00'
  who: Bartosz
- line: I recently discovered a part of AI engineering called prompt compression.
    Apparently, you can remove some tokens from the prompt, and the model will still
    perform the same. I can’t explain it yet because I’m still reading about it, but
    it’s interesting.
  sec: 1800
  time: '30:00'
  who: Bartosz
- line: Is it related to prompt caching? I don’t know how it works internally, but—
  sec: 1859
  time: '30:59'
  who: Alexey
- line: I think it’s not related. In prompt compression, you create a different prompt
    that’s shorter but supposed to do the same thing. It’s about creating an equivalent
    prompt with fewer tokens, possibly dropping parts of the words or something like
    that.
  sec: 1864
  time: '31:04'
  who: Bartosz
- line: So, it’s like asking ChatGPT, “Hey, I have this prompt. Can you rewrite it
    to make it shorter but not lose any meaningful parts?”
  sec: 1899
  time: '31:39'
  who: Alexey
- header: Prompt Caching & Model Efficiency (attention caching, Claude)
- line: Maybe. I don’t know how it works internally yet, but I imagine it might be
    something like that.
  sec: 1905
  time: '31:45'
  who: Bartosz
- line: I don’t know how prompt caching works either, but I know that Anthropic’s
    models, like Claude, have it. It makes coding tasks cheaper because you don’t
    send the entire codebase every time. For example, with Codex, they cache part
    of the prompt, so you pay less.
  sec: 1912
  time: '31:52'
  who: Alexey
- line: I think I read something about OpenAI’s documentation where they cache the
    attention matrix values. You don’t need to calculate it every time. But maybe
    I’m mixing things up.
  sec: 1963
  time: '32:43'
  who: Bartosz
- line: I imagine if two prompts have the same beginning but different endings, you
    can cache the shared part and only process the differences.
  sec: 1983
  time: '33:03'
  who: Alexey
- line: I think that’s the case, but if anyone wants to learn how it works, they should
    check the documentation, not rely on us trying to recall it.
  sec: 2009
  time: '33:29'
  who: Bartosz
- line: What do you think about DeepSeek? Have you played with it?
  sec: 2015
  time: '33:35'
  who: Alexey
- header: Open-Source Models & Tools Experience (DeepSeek, Perplexity)
- line: Only in the Perplexity tool as one of the models. I like that there are many
    new open-source models. It’s good for everyone. I also like that models are being
    created outside the United States. One location having a monopoly is never good.
  sec: 2022
  time: '33:42'
  who: Bartosz
- line: Overall, I’m enthusiastic about it. When I use it in Perplexity, it works
    really well.
  sec: 2022
  time: '33:42'
  who: Bartosz
- line: You haven’t used it in any projects yet?
  sec: 2063
  time: '34:23'
  who: Alexey
- line: Bartosz
  sec: 2063
  time: '34:23'
  who: Alexey
- line: No, I haven’t used it in any projects. I only use it as a user of the tool,
    and I’m satisfied with the results.
  sec: 2063
  time: '34:23'
  who: Alexey
- line: I haven’t tried Perplexity yet, but I’ve heard it’s good.
  sec: 2077
  time: '34:37'
  who: Alexey
- line: Sometimes it makes silly mistakes, like quoting something that doesn’t exist
    or using Reddit as an authoritative source. But overall, it’s fine. You just have
    to check if the information it gives you is true, especially for important things.
  sec: 2086
  time: '34:46'
  who: Bartosz
- line: If it’s something like code, you can test it quickly. But for security-related
    things, you should always double-check.
  sec: 2086
  time: '34:46'
  who: Bartosz
- line: Overall, it’s helpful, right?
  sec: 2147
  time: '35:47'
  who: Alexey
- line: Bartosz
  sec: 2147
  time: '35:47'
  who: Alexey
- line: Yes, overall, it’s helpful.
  sec: 2147
  time: '35:47'
  who: Alexey
- header: 'AI for Lead Scoring: LinkedIn Automation & Qualification'
- line: Another article you wrote is about using AI for lead classification on LinkedIn,
    where you created a Chrome extension. Can you tell us more about this project?
  sec: 2154
  time: '35:54'
  who: Alexey
- line: The goal was to send personalized invitations on LinkedIn. I was writing them
    manually, not generating them. The process involved opening someone’s profile,
    checking their posts, visiting their website if they had one, and reading their
    blog if they were active.
  sec: 2166
  time: '36:06'
  who: Bartosz
- line: This method worked but took a lot of time. The idea behind the extension was
    to automate some of these steps. It would analyze the profile and website data
    using AI and add the profile to a list if it was worth reaching out to.
  sec: 2166
  time: '36:06'
  who: Bartosz
- line: Was this for personal connections?
  sec: 2211
  time: '36:51'
  who: Alexey
- line: Bartosz
  sec: 2211
  time: '36:51'
  who: Alexey
- line: Yes, it was for personal connections. The extension helped filter out profiles
    that weren’t worth the effort, like those with outdated websites or no recent
    activity.
  sec: 2211
  time: '36:51'
  who: Alexey
- line: It depends. I receive many requests daily, so a personalized message might
    make a difference. But for people who don’t get many requests, it might not matter.
  sec: 2291
  time: '38:11'
  who: Alexey
- line: Yes, you could check if someone is posting about topics related to what you’re
    selling. For example, if you’re selling AI monitoring tools, you’d want to target
    people working with AI, not someone selling shoes.
  sec: 2331
  time: '38:51'
  who: Bartosz
- line: Every person on LinkedIn is a potential lead. You can assess how qualified
    they are, whether they have the problem your product solves, and if they’re in
    the right domain.
  sec: 2368
  time: '39:28'
  who: Alexey
- line: Exactly. If you look at LinkedIn’s Sales Navigator, it has similar features,
    but it’s disappointing.
  sec: 2400
  time: '40:00'
  who: Bartosz
- line: I remember it costs around 100 EUR per month. It’s not cheap.
  sec: 2412
  time: '40:12'
  who: Alexey
- line: Yes, it’s not cheap. Whether it’s useful is debatable.
  sec: 2419
  time: '40:19'
  who: Bartosz
- line: I tried using it for two months and found that sending personalized connection
    requests works as well as Sales Navigator, if not better.
  sec: 2426
  time: '40:26'
  who: Alexey
- line: The best feature is filtering out profiles you’ve already visited. That’s
    useful, but it’s not the main selling point.
  sec: 2441
  time: '40:41'
  who: Bartosz
- line: If someone wants to learn how to build a Chrome extension and include AI,
    how should they start?
  sec: 2454
  time: '40:54'
  who: Alexey
- header: 'Chrome Extension Architecture: Backend AI Integration Pattern'
- line: I had a backend service that the Chrome extension called. I didn’t put the
    AI inside the extension because I wasn’t familiar with browser extensions. I kept
    it simple and put most of the logic in the backend.
  sec: 2464
  time: '41:04'
  who: Bartosz
- line: I used the Cursor editor, and a lot of the code was generated. It didn’t always
    work, but it was faster than writing everything myself.
  sec: 2464
  time: '41:04'
  who: Bartosz
- header: 'Coding Assistants: Cursor Workflow & Productivity Boosts'
- line: Do you still use Cursor?
  sec: 2525
  time: '42:05'
  who: Alexey
- line: Bartosz
  sec: 2525
  time: '42:05'
  who: Alexey
- line: Yes, I still use it as a paid product.
  sec: 2525
  time: '42:05'
  who: Alexey
- line: Would you recommend it to others?
  sec: 2532
  time: '42:12'
  who: Alexey
- line: Bartosz
  sec: 2532
  time: '42:12'
  who: Alexey
- line: If someone is coding at work and can use it, definitely. If it’s for personal
    use, maybe start with the free version.
  sec: 2532
  time: '42:12'
  who: Alexey
- line: It’s not going to generate an entire application from one prompt, but it helps
    a lot with smaller functions. If you write the function signature and docstring,
    it can generate the rest.
  sec: 2532
  time: '42:12'
  who: Alexey
- line: Have you compared Cursor with GitHub Copilot?
  sec: 2631
  time: '43:51'
  who: Alexey
- line: I used GitHub Copilot when it first came out. It was great at the time, but
    Cursor is much better now. I haven’t compared the latest versions, but I’m sticking
    with Cursor because I already pay for it.
  sec: 2637
  time: '43:57'
  who: Bartosz
- header: 'Code AI Comparison: Cursor vs GitHub Copilot & Alternatives'
- line: I already pay for ChatGPT, Claude, and GitHub Copilot (free for open-source
    contributors). I’m wondering if I should switch to Cursor, but I’d have to cancel
    one of my existing subscriptions.
  sec: 2678
  time: '44:38'
  who: Alexey
- line: You don’t have to copy with Cursor. It can reference files directly, which
    speeds up development. There’s also a composer tool that can edit multiple files
    or even run command-line commands.
  sec: 2724
  time: '45:24'
  who: Bartosz
- line: There are so many alternatives, like P AI (an open-source alternative to Cursor).
    New tools appear every day. I’ve decided to stick with Cursor unless something
    significantly better comes along.
  sec: 2787
  time: '46:27'
  who: Alexey
- header: 'Search-Focused Assistants: Using Perplexity & Tool Selection'
- line: Sometimes, but I use Perplexity more often. You can switch off the search
    feature and use it like ChatGPT.
  sec: 2839
  time: '47:19'
  who: Bartosz
- line: Have you tried other tools like P AI, Bolt, or Lavable?
  sec: 2870
  time: '47:50'
  who: Alexey
- line: I started using Cursor early and stuck with it.
  sec: 2889
  time: '48:09'
  who: Bartosz
- line: So, Cursor is your go-to tool for programming?
  sec: 2897
  time: '48:17'
  who: Alexey
- line: Bartosz
  sec: 2897
  time: '48:17'
  who: Alexey
- line: Yes, right now. I don’t even miss the refactoring features from IntelliJ anymore.
  sec: 2897
  time: '48:17'
  who: Alexey
- line: JetBrains is trying to catch up with their AI plugin, but you’d have to pay
    extra for it.
  sec: 2910
  time: '48:30'
  who: Alexey
- line: Bartosz
  sec: 2910
  time: '48:30'
  who: Alexey
- line: Exactly. I already pay for Cursor, so I’m not switching unless something is
    way better.
  sec: 2910
  time: '48:30'
  who: Alexey
- line: I guess you’re used to Visual Studio Code by now.
  sec: 2938
  time: '48:58'
  who: Alexey
- line: Bartosz
  sec: 2938
  time: '48:58'
  who: Alexey
- line: Yes, but I miss some of the automatic refactoring features from IntelliJ.
  sec: 2938
  time: '48:58'
  who: Alexey
- line: Comparing IntelliJ for Java isn’t fair because Java is a static language,
    and Python is dynamic.
  sec: 2976
  time: '49:36'
  who: Alexey
- line: Bartosz
  sec: 2976
  time: '49:36'
  who: Alexey
- line: True, but Visual Studio Code doesn’t even try to match IntelliJ’s refactoring
    capabilities.
  sec: 2976
  time: '49:36'
  who: Alexey
- line: I’ve tried GitHub Copilot for refactoring, like extracting functions, and
    it works well.
  sec: 3003
  time: '50:03'
  who: Alexey
- line: Bartosz
  sec: 3003
  time: '50:03'
  who: Alexey
- line: Cursor does the same. You don’t have to remember everything in the standard
    library anymore, which is a big improvement.
  sec: 3003
  time: '50:03'
  who: Alexey
- line: I know a bit of frontend development, but the ecosystem evolves quickly. With
    these tools, I can give a prompt and get a working project, but I don’t always
    understand what’s happening.
  sec: 3056
  time: '50:56'
  who: Alexey
- line: Bartosz
  sec: 3056
  time: '50:56'
  who: Alexey
- line: That’s one of the problems, but it works. I can’t do frontend development,
    but with Cursor, I can create something. It’s ugly, but it’s better than nothing.
  sec: 3056
  time: '50:56'
  who: Alexey
- header: 'Website Hosting: Static Site Generators & GitHub Pages'
- line: What do you use for your website?
  sec: 3129
  time: '52:09'
  who: Alexey
- line: Bartosz
  sec: 3129
  time: '52:09'
  who: Alexey
- line: It’s a static site generator hosted on GitHub Pages.
  sec: 3129
  time: '52:09'
  who: Alexey
- line: In AWS, you can put your files in S3 and attach a domain to the bucket.
  sec: 3160
  time: '52:40'
  who: Alexey
- line: Bartosz
  sec: 3160
  time: '52:40'
  who: Alexey
- line: There’s a service that downloads the code from GitHub, builds a Docker image,
    and deploys it.
  sec: 3160
  time: '52:40'
  who: Alexey
- header: 'Blogging as Business: Attracting Clients & Teaching Workshops'
- line: I want to start writing and get motivated. Does your blog help you find clients?
  sec: 3190
  time: '53:10'
  who: Alexey
- line: Bartosz
  sec: 3190
  time: '53:10'
  who: Alexey
- line: Yes, a few people reached out after finding my email on the blog. It also
    helps when teaching workshops. Sending links to my articles makes a good impression.
  sec: 3190
  time: '53:10'
  who: Alexey
- line: I started blogging because I had a lot of ideas about programming and wanted
    to share them. Over time, it evolved into a tool to attract clients.
  sec: 3190
  time: '53:10'
  who: Alexey
- line: Now you work as an AI consultant, right?
  sec: 3267
  time: '54:27'
  who: Alexey
- line: Bartosz
  sec: 3267
  time: '54:27'
  who: Alexey
- line: Yes, that’s correct. The blog’s topics evolved to AI, and it became a tool
    to attract clients.
  sec: 3267
  time: '54:27'
  who: Alexey
- line: So, you didn’t start with the goal of attracting clients, but it evolved into
    that.
  sec: 3288
  time: '54:48'
  who: Alexey
- line: Yes, exactly. At the beginning, the goal was to write down some ideas and
    maybe send links to a few people, mostly for my own team. Over time, it evolved.
  sec: 3293
  time: '54:53'
  who: Bartosz
- line: For a while, it was a tool for learning. I was learning about data science,
    so I wrote a lot of articles about it. They were essentially notes for myself.
    Eventually, it became more than that.
  sec: 3293
  time: '54:53'
  who: Bartosz
- line: It wasn’t a marketing tool from the start, and it still isn’t entirely. A
    lot of the content is just notes I can copy-paste into projects.
  sec: 3293
  time: '54:53'
  who: Bartosz
- line: For that, I used to use WikiMedia. I had my own instance of a wiki, but the
    problem was it wasn’t presentable for others. It was just a bunch of thoughts,
    helpful for copy-pasting but not something I could show to clients to impress
    them.
  sec: 3339
  time: '55:39'
  who: Alexey
- line: It’s also not very discoverable on Google. A blog works better for that.
  sec: 3339
  time: '55:39'
  who: Alexey
- line: If you have your notes in the form of articles, it’s relatively easy to turn
    them into a blog.
  sec: 3369
  time: '56:09'
  who: Bartosz
- header: 'AI-Assisted Writing: Drafting, Rewriting, and Maintaining Voice'
- line: With AI tools, you can turn drafty notes into blog posts relatively quickly.
    It’s a nice weekend project.
  sec: 3377
  time: '56:17'
  who: Alexey
- line: The problem is, I’ve never succeeded at making AI write in a style I’d like
    to present as my own.
  sec: 3399
  time: '56:39'
  who: Bartosz
- line: I managed to do it with LinkedIn recommendations. When I left my previous
    company, I wanted to write recommendations for my colleagues. I wrote bullet points
    and asked AI to write in the same style as my other recommendations. It did a
    decent job, though I still had to edit it a bit.
  sec: 3414
  time: '56:54'
  who: Alexey
- line: For long-form content, though, it doesn’t work as well.
  sec: 3414
  time: '56:54'
  who: Alexey
- line: I tried publishing an AI-generated article on my blog. It was about generating
    articles with AI, so it wasn’t cheating—it was a tutorial. It was okay, but if
    someone reads a lot of my content, they’d notice it wasn’t written by me.
  sec: 3456
  time: '57:36'
  who: Bartosz
- line: Do you use ChatGPT to help you get started with articles?
  sec: 3495
  time: '58:15'
  who: Alexey
- line: Not to get started, but when I get stuck or need to rewrite something, yes.
    I tend to write long sentences, which aren’t good for readers. If I’m stuck on
    how to split a sentence, I ask ChatGPT for examples.
  sec: 3502
  time: '58:22'
  who: Bartosz
- line: For me, ChatGPT eliminates the blank page problem. I can dump my thoughts
    into it, and it structures them, giving me a starting point.
  sec: 3537
  time: '58:57'
  who: Alexey
- line: When I want to write freely, I use the simplest editor I have. I don’t want
    anything finishing my sentences. Most editors try to do that now, but Obsidian
    doesn’t.
  sec: 3565
  time: '59:25'
  who: Bartosz
- line: You could probably add a plugin to Obsidian for that, but I wouldn’t. Google
    Docs also has this feature, but you can disable it.
  sec: 3588
  time: '59:48'
  who: Alexey
- line: Yes, but then you have to remember to enable it again. I just switch to a
    different editor when I want to write something imperfect but exactly what I want.
  sec: 3600
  time: '1:00:00'
  who: Bartosz
- header: Episode Wrap-Up & Guest Resources (blog link invitation)
- line: It’s been over an hour. Time flies! I think we should wrap up. It was amazing
    talking to you, Bartosz. Thanks for joining us today and sharing your experiences.
  sec: 3621
  time: '1:00:21'
  who: Alexey
- line: I invite everyone to visit my blog. You might find something useful there.
    You don’t need to subscribe to the newsletter, but you’re welcome to if you like.
  sec: 3651
  time: '1:00:51'
  who: Bartosz
- line: I’ll put the link to your blog in the live chat. Thanks a lot, Bartosz. Enjoy
    your evening—it’s almost 9:00 PM for you, right?
  sec: 3677
  time: '1:01:17'
  who: Alexey
- line: Yes, almost 9:00 PM.
  sec: 3689
  time: '1:01:29'
  who: Bartosz
- line: Have a good evening, and see you around!
  sec: 3697
  time: '1:01:37'
  who: Alexey
- line: Bartosz
  sec: 3697
  time: '1:01:37'
  who: Alexey
- line: Bye, everyone!
  sec: 3697
  time: '1:01:37'
  who: Alexey
context: 'Context: a practitioner’s tour through the end-to-end work of turning data
  and models into reliable, efficient products—from Java and data engineering foundations
  to AI fine-tuning, prompt craft, tooling choices, and developer workflows.

  Core: the episode’s through-line is a data‑centric engineering mindset for trustworthy,
  production-ready AI: rigorous testing and pipeline design to ensure data trust,
  deliberate choices about models and tools for cost and performance, prompt and token-efficiency
  techniques to make inference practical, and pragmatic engineering patterns (architecture,
  caching, assistants) that let teams ship AI features and sustain them—while using
  content and teaching as a way to refine thinking and capture business value.'
---
Links:

* [LinkedIn](https://www.linkedin.com/in/mikulskibartosz/){:target="_blank"}
* [Github](https://github.com/mikulskibartosz){:target="_blank"}
* [Website](https://mikulskibartosz.name/blog/){:target="_blank"}