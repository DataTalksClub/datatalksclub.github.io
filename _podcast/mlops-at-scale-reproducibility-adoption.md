---
title: 'MLOps at Scale: CI/CD, Reproducibility, Model Monitoring & Adoption Strategies'
short: MLOps as a Team
season: 19
episode: 4
guests:
- raphaelhoogvliets
image: images/podcast/s19e04-mlops-as-team.jpg
ids:
  anchor: datatalksclub/episodes/MLOps-as-a-Team---Raphal-Hoogvliets-e2qnnu5/a-abkcdlr
  youtube: rMq63r3zi4c
links:
  anchor: https://creators.spotify.com/pod/show/datatalksclub/episodes/MLOps-as-a-Team---Raphal-Hoogvliets-e2qnnu5/a-abkcdlr
  apple: https://podcasts.apple.com/us/podcast/mlops-as-a-team-rapha%C3%ABl-hoogvliets/id1541710331?i=1000676238840
  spotify: https://open.spotify.com/episode/0Dl372MFGvN0zDa1YQx7oe?si=eCy-a4fkRtOaEe21-KDHXQ
  youtube: https://youtube.com/watch?v=rMq63r3zi4c
description: Learn MLOps CI/CD and model monitoring to scale reliable deployments,
  accelerate delivery, ensure reproducibility, and drive model adoption in production.
intro: 'How do you run MLOps at scale so models stay deployed, reproducible, and actually
  adopted? In this episode Raphaël Hoogvliets—who leads a 12‑engineer team at Eneco
  and brings a career arc from agriculture into data science and MLOps—walks through
  practical approaches for CI/CD for ML, reproducibility, model monitoring, and adoption
  strategy. <br><br> We cover the core trade‑offs between speed and robustness, design
  choices for long‑term maintainability, and the team coordination needed to scale
  ML: evangelists, tech translators, and technical leads. Raphaël explains why a centralized
  MLOps platform team often works as an enabling layer, how MLOps should support product
  teams, and how to drive adoption through iteration, feedback loops, and developer
  experience. You’ll hear concrete practices—CI, repo structure, parameterization,
  testing—plus reproducibility tactics like data versioning, traceability, and experiment
  capture. We also discuss KPIs (deployment frequency and impact tracking), skill
  mix, dependency management, container strategies, and real success and failure stories.
  <br><br> Listen to learn actionable priorities for getting started (start with CI/CD
  and solve tangible pain points), and how to measure and sustain model value through
  monitoring and operational processes.'
dateadded: 2024-11-16
duration: PT01H04M07S
quotableClips:
- name: Podcast Introduction
  startOffset: 0
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=0
  endOffset: 81
- name: 'Guest Overview: Raphaël Hoogvliets and Eneco role'
  startOffset: 81
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=81
  endOffset: 154
- name: 'Career Path: From agriculture to data science and MLOps'
  startOffset: 154
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=154
  endOffset: 521
- name: Agriculture technology, scale, and sustainability trade-offs
  startOffset: 521
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=521
  endOffset: 636
- name: Design Choices and Long‑Term Tradeoffs in ML projects
  startOffset: 636
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=636
  endOffset: 817
- name: 'Speed vs. Robustness: trade-offs in MLOps delivery'
  startOffset: 817
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=817
  endOffset: 845
- name: 'Team Coordination: why collaboration matters for ML at scale'
  startOffset: 845
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=845
  endOffset: 1018
- name: 'Key Team Roles: evangelists, tech translators, and technical leads'
  startOffset: 1018
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=1018
  endOffset: 1381
- name: Centralized MLOps as an enabling platform team
  startOffset: 1381
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=1381
  endOffset: 1520
- name: 'Support Model: how MLOps assists product teams and ML engineers'
  startOffset: 1520
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=1520
  endOffset: 1676
- name: 'Adoption Strategy: iteration, feedback loops, and developer experience'
  startOffset: 1676
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=1676
  endOffset: 1966
- name: 'Building Trust: collecting pain points and delivering quick wins'
  startOffset: 1966
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=1966
  endOffset: 2215
- name: 'Measuring Value: KPIs, deployment frequency, and impact tracking'
  startOffset: 2215
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=2215
  endOffset: 2346
- name: 'Core Practices: CI, repo structure, parameterization, and testing'
  startOffset: 2346
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=2346
  endOffset: 2551
- name: 'Reproducibility: data versioning, traceability, and experiment capture'
  startOffset: 2551
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=2551
  endOffset: 2662
- name: 'Maturity Signals: when to introduce data versioning and governance'
  startOffset: 2662
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=2662
  endOffset: 2710
- name: 'Skill Mix: combining data science, SRE/devops, and platform engineering'
  startOffset: 2710
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=2710
  endOffset: 2921
- name: 'Getting Started: prioritize CI/CD and solve tangible pain points'
  startOffset: 2921
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=2921
  endOffset: 3081
- name: 'MLOps Toolset: experiment tracking, model registry, serving, and monitoring'
  startOffset: 3081
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=3081
  endOffset: 3188
- name: 'Dependency Management: package registries for reproducible deployments'
  startOffset: 3188
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=3188
  endOffset: 3410
- name: 'Container Strategy: Docker, Kubernetes, Databricks trade-offs'
  startOffset: 3410
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=3410
  endOffset: 3476
- name: 'Success & Failure Stories: deployment wins and integration freezes'
  startOffset: 3476
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=3476
  endOffset: 3654
- name: 'MLOps Defined: operationalizing machine learning in business'
  startOffset: 3654
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=3654
  endOffset: 3718
- name: 'Core Challenge: keeping models deployed, monitored, and maintained'
  startOffset: 3718
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=3718
  endOffset: 3762
- name: Closing Remarks and next steps
  startOffset: 3762
  url: https://youtube.com/watch?v=rMq63r3zi4c&t=3762
  endOffset: 3847
transcript:
- header: Podcast Introduction
- line: Hi, everyone! Welcome to our event. This is brought to you by DataTalks.Club,
    a community for people who love data. We have weekly events — well, almost weekly
    lately, but we're getting back on track. Actually, we have two events this week!
    If you want to check out our upcoming events, there's a link in the description.
    I think I need to update it since we now have over 50K subscribers, so if you
    haven't yet, hit the subscribe button.
  sec: 0
  time: 0:00
  who: Alexey
- line: Also, we have a live chat where you can ask questions. There’s a pinned link
    for submitting your questions, and we’ll cover them during the interview. That’s
    it for the intro! I’ll stop sharing my screen now. We haven’t had a podcast interview
    in a while — maybe a month — but don’t worry, we’ll have more content coming soon.
    Today, we have Raphaël with us. If you're ready, we can start.
  sec: 0
  time: 0:00
  who: Alexey
- line: Yes, definitely.
  sec: 78
  time: '1:18'
  who: Raphaël
- header: 'Guest Overview: Raphaël Hoogvliets and Eneco role'
- line: Great! This week we’re talking about MLOps with a special guest, Raphaël Hoogvliets.
    Raphaël is a leader in MLOps with a background in data science and machine learning.
    You might know him from LinkedIn, where he frequently shares MLOps content. I
    see his posts daily — though I suspect his profile picture is AI-generated, right?
    Anyway, Raphaël currently leads a team of engineers at Eneco, a major sustainable
    energy provider.
  sec: 81
  time: '1:21'
  who: Alexey
- line: Hey, Alexey. Thanks so much for having me here. It's great to be here.
  sec: 126
  time: '2:06'
  who: Raphaël
- line: It’s a pleasure to finally make this happen! We’ve been planning this for
    a while. And a quick shoutout to Johanna Bayer for preparing today’s questions.
  sec: 130
  time: '2:10'
  who: Alexey
- line: Before we dive into MLOps, let’s start with your background. Can you tell
    us about your career journey so far?
  sec: 130
  time: '2:10'
  who: Alexey
- header: 'Career Path: From agriculture to data science and MLOps'
- line: Sure! I’ve been working in the data field for over ten years now. I started
    as a data scientist, which was challenging at first because there was so much
    to learn. But I kept pushing through.
  sec: 154
  time: '2:34'
  who: Raphaël
- line: What did you do before that?
  sec: 182
  time: '3:02'
  who: Alexey
- line: Something quite different. Before data science, I spent five years in sustainable
    agriculture. My role included project management and lobbying for a green organization.
  sec: 184
  time: '3:04'
  who: Raphaël
- line: So, it wasn’t IT-related, right?
  sec: 204
  time: '3:24'
  who: Alexey
- line: Not initially, no. But I noticed that large projects lacked proper data management.
    Farmers had a lot of valuable data on crop quality and sustainability, but it
    was mostly on paper or outdated systems. That’s when I started thinking there
    was potential for improvement. Eventually, I moved into agricultural innovation,
    focusing on technology and data, which ultimately led me to data science.
  sec: 206
  time: '3:26'
  who: Raphaël
- line: I transitioned from project manager to consultant, freelancing in agriculture
    and food tech. But agriculture isn’t always data-driven, so I chose some challenging
    projects. About ten years ago, I was working with a client on a proof of concept,
    and they asked if I could put it in production. I didn’t even know what “production”
    meant back then, so I did some research, learned a bit about IT, and realized
    I needed to dive deeper into these concepts.
  sec: 206
  time: '3:26'
  who: Raphaël
- line: This led me to an IT consultancy — a Microsoft Platinum partner — where I
    worked as a data scientist in a mostly BI-engineering team. It was challenging
    but a great learning experience. Later, I joined a more data-focused consultancy
    and became a lead data scientist. Here, I really started learning about production
    deployments.
  sec: 206
  time: '3:26'
  who: Raphaël
- line: In consultancies, projects are often short-term, but data science projects
    require more time. So, I joined an internal team where I got my first taste of
    MLOps. We had valuable models, but they were built in a very "spaghetti code"
    style on an old R server. I got the chance to upgrade this setup, working with
    a great mentor, and I found that I really enjoyed MLOps. It was exciting to add
    value beyond just building models.
  sec: 206
  time: '3:26'
  who: Raphaël
- line: That was about three years ago. Since then, I’ve worked in MLOps and even
    started freelancing. I’m currently contracting with Eneco, where I’ll soon join
    as a full-time employee to continue building the team and see the ongoing reorganization
    through. There's so much work to do, and it’s fulfilling to stay on and see things
    progress.
  sec: 206
  time: '3:26'
  who: Raphaël
- header: Agriculture technology, scale, and sustainability trade-offs
- line: It’s a fascinating journey! You mentioned starting in agriculture, and given
    that you're from the Netherlands, I know the country is advanced in agriculture.
    Even though it’s a small country, I see Dutch produce across Europe. It’s impressive
    how much you achieve with limited land — it’s almost like an agricultural marvel!
  sec: 521
  time: '8:41'
  who: Alexey
- line: Yes, it’s definitely a point of pride for us! In the Netherlands, we often
    say we're the second-largest agricultural exporter, but that statistic can be
    misleading. It includes a lot of re-exports. In reality, we rank around 22nd in
    actual production, which is still impressive for a small country.
  sec: 553
  time: '9:13'
  who: Raphaël
- line: Our agricultural success is due to high-tech innovations, from genetics to
    production processes. However, our approach also has a high environmental footprint.
    We import a lot of grains and soy from the Americas to feed livestock, which creates
    challenges like excess manure and air quality issues. But I believe we can continue
    being a sustainable player if we adapt the right practices.
  sec: 553
  time: '9:13'
  who: Raphaël
- header: Design Choices and Long‑Term Tradeoffs in ML projects
- line: 'Back to MLOps — your LinkedIn profile has an interesting tagline: “Creating
    the future’s technical debt today.” What does that mean?'
  sec: 636
  time: '10:36'
  who: Alexey
- line: It’s a bit of a joke but also has a serious side. In our field, technology
    moves so quickly that by the time you implement something, it might already feel
    outdated.
  sec: 661
  time: '11:01'
  who: Raphaël
- line: There's a gap between when you make design decisions and when you implement
    them. Sometimes, as new tools or models emerge, it feels like we’re creating "technical
    debt" just by sticking to what’s current. But in reality, true technical debt
    is the gap between an organization’s capabilities and its ambitions.
  sec: 661
  time: '11:01'
  who: Raphaël
- line: But it's unavoidable, right? Sometimes you want to create a prototype, move
    fast, and you're unsure how many out of 10 prototypes will survive — maybe just
    one. So, you don't necessarily want to invest a lot of effort into each prototype.
    But when one works, you know, okay, it proved its value, so now it's time to repay
    the debt.
  sec: 739
  time: '12:19'
  who: Alexey
- line: Yeah, I agree. These decisions and conversations happen daily, especially
    between tech leads and product managers, but also among tech leads. At Eco, some
    of the teams have a startup mentality — cut corners, move fast, and focus on delivering
    value for the customer. Others are more cautious, saying, "Let's do this the right
    way before we deploy."
  sec: 767
  time: '12:47'
  who: Raphaël
- line: We should stop cutting corners, right?
  sec: 801
  time: '13:21'
  who: Alexey
- line: Exactly, yeah. It's an ongoing discussion. And in engineering, the cliché
    is that everything is a trade-off, and that's true on the product level as well.
  sec: 804
  time: '13:24'
  who: Raphaël
- header: 'Speed vs. Robustness: trade-offs in MLOps delivery'
- line: You mentioned building teams in your career. You’ve built teams multiple times,
    and now you’re building another one. Why focus on teams specifically? And what
    do teams have to do with ML Ops?
  sec: 817
  time: '13:37'
  who: Alexey
- header: 'Team Coordination: why collaboration matters for ML at scale'
- line: Great question. It was a natural progression from my earlier life. I’ve always
    valued teamwork, from playing sports to working in a team at an art-house cinema
    and even in online games. I’ve always focused on how coordination and good culture
    are key. When I moved into data science, I saw the importance of having a well-coordinated
    team. In data science and ML Ops, a strong team is crucial. If you want to go
    fast, you go alone, but if you want to go far, you go together. In non-tech organizations,
    doing data science can be particularly challenging, and having a well-rounded
    team with diverse roles ensures success.
  sec: 845
  time: '14:05'
  who: Raphaël
- header: 'Key Team Roles: evangelists, tech translators, and technical leads'
- line: So, what makes a good team? What kind of people do you need?
  sec: 1018
  time: '16:58'
  who: Alexey
- line: It depends on the context and the organization’s maturity, but there are common
    roles. One often overlooked role is the evangelist. Someone has to advocate for
    the team, whether it's internally or on the executive level. This person isn’t
    just about stakeholder management, but about driving the vision and support. In
    product teams, you also need a tech translator — someone who can bridge the gap
    between technical and non-technical stakeholders. This could be a product manager,
    but often the roles are split, with one person focusing more on the technical
    side.
  sec: 1049
  time: '17:29'
  who: Raphaël
- line: We’re talking about non-IT companies, where ML Ops is secondary, right?
  sec: 1117
  time: '18:37'
  who: Alexey
- line: Exactly. In these companies, it’s essential to have someone at the executive
    level who understands ML Ops, or an evangelist who can rally support. Additionally,
    a tech translator helps communicate technical complexities in a way the business
    side can understand. These roles are vital for success in ML Ops.
  sec: 1134
  time: '18:54'
  who: Raphaël
- line: So, evangelist and tech translator are key roles in the team?
  sec: 1196
  time: '19:56'
  who: Alexey
- line: Yes, and it’s possible for one person to fill both roles. On the technical
    side, having an experienced lead is essential — someone who understands ML Ops
    principles and can guide the team. It’s great if you can split between ML Ops
    engineers focused on building infrastructure and automating the ML lifecycle,
    and ML engineers working with data scientists on the product side. But the team
    composition really depends on the type of ML you're working with.
  sec: 1233
  time: '20:33'
  who: Raphaël
- header: Centralized MLOps as an enabling platform team
- line: So what does the ML Ops team do? It sounds like a central team that helps
    other teams, is that right?
  sec: 1381
  time: '23:01'
  who: Alexey
- line: Yes, it depends on the organization, but in many cases, the ML Ops team is
    centralized. In our agile framework, we act as an enabling team. We work closely
    with ML engineers, helping define best practices, create design documentation,
    and build reusable tools. We focus on deployment, maintenance, and monitoring,
    which are key components of ML Ops. We aim to make things easier for the other
    teams to adopt, but we also have to make sure we’re flexible tnough not to alienate
    them.
  sec: 1412
  time: '23:32'
  who: Raphaël
- line: So the ML Ops team helps 34 product teams with different use cases, like demand
    forecasting or energy supplier maintenance. Is that correct?
  sec: 1519
  time: '25:19'
  who: Alexey
- header: 'Support Model: how MLOps assists product teams and ML engineers'
- line: Yes, that’s the setup. We have product teams working on different use cases,
    and the ML Ops team helps by providing infrastructure, tools, and best practices
    to make model deployment easier. It’s a luxury to have both a centralized ML Ops
    team and ML engineers embedded in the product teams, which is the situation we're
    in.
  sec: 1520
  time: '25:20'
  who: Raphaël
- header: 'Adoption Strategy: iteration, feedback loops, and developer experience'
- line: How do you standardize practices when only about 25–30% of data scientists
    are on board with your framework? How do you get the rest of the team to follow?
  sec: 1676
  time: '27:56'
  who: Alexey
- line: Iteration is key. You test, talk to people, and get feedback. It takes time
    to build relationships and trust. You need to listen to your end users — data
    scientists are crucial in this process — and adjust your framework accordingly.
    You can’t please everyone, but you can find common ground. Some aspects, like
    CI, repository structure, and packaging solutions, are easy to standardize. But
    the actual code structure is trickier. You need to find a balance between enforcing
    standards and allowing flexibility for data scientists to work the way they’re
    comfortable.
  sec: 1867
  time: '31:07'
  who: Raphaël
- header: 'Building Trust: collecting pain points and delivering quick wins'
- line: How do you go about talking and getting feedback? Do you select a few projects
    that are either the most important or not so important because you don’t want
    to touch the important projects? Walk us through the process of understanding
    what kind of standards we can have as a team and what kind of standards will get
    adoption.
  sec: 1966
  time: '32:46'
  who: Alexey
- line: From the data science side, in my current situation, we're in a position of
    luxury because we have many ML engineers. But if you don't have that, and you're
    just an isolated AIOps team, I would approach it like a product manager. You treat
    the process itself as you would when developing products for users. You try to
    develop your AIOps platform that way for developers. People often talk about user
    experience, but developer experience is a huge driver for success. While I'm not
    a product manager, I do apply some of those principles for AIOps.
  sec: 1993
  time: '33:13'
  who: Raphaël
- line: To create buy-in from the organization, the first thing you do is collect
    pain points — what are people struggling with? Then you can create a matrix to
    compare what I, as the ML Ops lead, think we should work on versus what the data
    scientists think we should focus on. In this matrix, you’ll have four quadrants.
    Start with the one where there's overlap to create quick wins, even if you don’t
    think it’s the most important thing to do. The key is to build trust and prove
    your value.
  sec: 1993
  time: '33:13'
  who: Raphaël
- line: Once you have the pain points, you also need to show a clear "before" and
    "after." Before starting, paint a picture of where things are now, acknowledge
    that it will take time and possibly some of their regular work, but highlight
    the clear gains once it’s done. These could include saved time in deployments,
    mitigated risks, or saved money. Once we save those resources, the data scientists
    will have more time for the actual work they love, rather than getting stuck in
    operations. Without proper machine learning operations, data scientists often
    end up debugging pipelines. As they build more models and solutions, they get
    criticized for broken pipelines. Eventually, they start manually running chunks
    of code with print statements to figure out what went wrong. This process takes
    a lot of time. You need to show the team that by implementing these tools and
    practices, they can return to improving models or building new AI solutions rather
    than fixing pipelines.
  sec: 1993
  time: '33:13'
  who: Raphaël
- header: 'Measuring Value: KPIs, deployment frequency, and impact tracking'
- line: So that’s why you start with pain points. As a data scientist, I definitely
    don't like debugging my pipelines. You identify what they don’t like, which avoids
    the risk of, as you mentioned, a platform engineer thinking the best way to deploy
    models is the only way, and then others don't find it useful. You could end up
    with fancy solutions that aren’t solving the real pain points of the product teams.
  sec: 2215
  time: '36:55'
  who: Alexey
- line: Exactly. You might start rolling out your amazing practices, and data scientists
    will find that the tests don’t accept what they just built, or even worse, the
    pre-commit hooks don’t work. They’ll ask, "What is this mypy thing? I can't even
    merge my code now." And if they’re using branches, it gets worse. That’s how you
    lose buy-in. You really need to show the value of what you're doing.
  sec: 2252
  time: '37:32'
  who: Raphaël
- line: From the leadership side, one of the challenges in ML Ops is dealing with
    KPIs and OKRs. Everyone wants measurable results, but it can be tough to show
    those in engineering, especially when it feels like, “If we weren’t here, nothing
    would work.” But you need to present those KPIs, even if it's a challenge.
  sec: 2252
  time: '37:32'
  who: Raphaël
- line: It could be as simple as tracking the number of models deployed through the
    platform.
  sec: 2321
  time: '38:41'
  who: Alexey
- line: Exactly.
  sec: 2324
  time: '38:44'
  who: Raphaël
- line: That’s a good way to measure. As we discussed earlier, you need to approach
    an ML Ops team as an internal product team. You need to talk to users rather than
    assume what they need.
  sec: 2326
  time: '38:46'
  who: Alexey
- header: 'Core Practices: CI, repo structure, parameterization, and testing'
- line: For sure.
  sec: 2346
  time: '39:06'
  who: Raphaël
- line: We briefly touched on best practices and tools. You mentioned having proper
    CI and a clear structure for ML repositories and packaging. Are there any other
    must-haves when it comes to standardization and best practices?
  sec: 2348
  time: '39:08'
  who: Alexey
- line: It's always a good idea to isolate your parameters, especially when building
    solutions. For software engineers, this might be a no-brainer, but for data scientists,
    it's crucial.
  sec: 2381
  time: '39:41'
  who: Raphaël
- line: And this should be done in a standardized way, right? Not one team using YAML
    files, another using JSON, and a third using XML. Everyone should stick to the
    same standard.
  sec: 2398
  time: '39:58'
  who: Alexey
- line: Exactly.
  sec: 2410
  time: '40:10'
  who: Raphaël
- line: For your testing suite, it’s difficult to get code coverage for everything,
    but I think testing data transformations — preprocessing and post-processing —
    should always be a priority. And your development team can handle this themselves.
  sec: 2410
  time: '40:10'
  who: Raphaël
- line: Another thing to consider is data exploration. A lot of knowledge is captured
    there, and it can help with monitoring setups. Even if you don’t have an advanced
    monitoring system yet, the insights gained during data exploration can be valuable.
    Before, we’d just comment out code for visualization, but if your code is in production,
    you want to keep that part in. This will aid with root cause analysis down the
    line.
  sec: 2410
  time: '40:10'
  who: Raphaël
- line: So, it’s more about organizing your code — keeping exploratory work separate
    from deployment code, and having clear test coverage, right?
  sec: 2518
  time: '41:58'
  who: Alexey
- line: Exactly. But there’s a risk that exploratory work gets lost. For example,
    code could be version-controlled, but the original exploration might be on someone’s
    desktop who’s left the company.
  sec: 2533
  time: '42:13'
  who: Raphaël
- header: 'Reproducibility: data versioning, traceability, and experiment capture'
- line: So, if we have a Git repo, should we keep exploratory work in a specific folder,
    like a "notebooks" folder? Even if it’s messy, just commit it, push it, and keep
    it around?
  sec: 2551
  time: '42:31'
  who: Alexey
- line: Yes, exactly. There’s usually a lot of value there. It’s worth keeping around.
  sec: 2574
  time: '42:54'
  who: Raphaël
- line: For best practices, once you reach a more advanced stage, reproducibility
    and traceability become important. Traceability depends on your sector and legal
    requirements, but reproducibility is key. While it's difficult to make everything
    100% reproducible, tying your code to data versioning is a good start. If you
    know which data version is connected to a particular deployment, it helps you
    reverse-engineer when needed.
  sec: 2574
  time: '42:54'
  who: Raphaël
- header: 'Maturity Signals: when to introduce data versioning and governance'
- line: At what point in an organization’s maturity should they start thinking about
    data versioning? Sometimes it feels like overkill, especially if we only have
    a few models and aren’t dealing with a large portfolio. Do we need to care about
    data versioning from the start, or can it come later?
  sec: 2662
  time: '44:22'
  who: Alexey
- line: I agree, it can feel overwhelming. It really depends on your sector and any
    obligations you have to customers or legal frameworks. But, yes, it’s not always
    necessary to focus on data versioning from the start, especially for smaller teams.
    It can come later as your needs grow.
  sec: 2686
  time: '44:46'
  who: Raphaël
- header: 'Skill Mix: combining data science, SRE/devops, and platform engineering'
- line: 'We have a few questions from the audience. The first one is: is it important
    to first work as a data scientist before moving into MLOps? And maybe here we
    can also discuss what MLOps actually means in this context. There’s this idea
    that MLOps is strictly about tools. As a data scientist, I start using these tools
    and then move into MLOps. But we''ve also talked about other aspects of MLOps,
    like processes and team structure. With this in mind, do you think it’s necessary
    to work as a data scientist first, or is it not required?'
  sec: 2710
  time: '45:10'
  who: Alexey
- line: You do need these skills in your team. Not everyone has to come from a data
    science background, but you do need those skills in the mix.
  sec: 2756
  time: '45:56'
  who: Raphaël
- line: In the MLOps team, right?
  sec: 2765
  time: '46:05'
  who: Alexey
- line: Yes, exactly. It’s good to have a mix of skills and backgrounds. MLOps has
    a lot of overlap with SRE (Site Reliability Engineering), which some people refer
    to as DevOps. It’s interesting because I learned not too long ago that DevOps
    is more of a movement, and no one really knows exactly what it is. On the other
    hand, SRE is a well-defined practice. MLOps overlaps a lot with SRE, so it’s useful
    to have someone with that expertise. It’s also helpful to have good software engineering
    experience. As data scientists, we’re generally not the best software engineers.
    Interestingly, roles like SRE and platform engineering are often called data engineering
    in many organizations. Data engineers are often building data pipelines or doing
    data warehousing, but they also do a lot more. In many cases, adding a data engineer
    to your MLOps team can be really beneficial.
  sec: 2766
  time: '46:06'
  who: Raphaël
- line: 'So, in general, we aim for a diverse set of skills: someone with engineering
    experience, someone with platform experience, and someone with data science experience.
    Is it fair to say that while it’s not essential to have worked as a data scientist
    before moving into MLOps, having that expertise in the team is important?'
  sec: 2853
  time: '47:33'
  who: Alexey
- line: Yes, ideally more than one data scientist, but you definitely need that skill
    mix.
  sec: 2882
  time: '48:02'
  who: Raphaël
- line: Probably someone with translation skills should at least have some experience
    in data science.
  sec: 2888
  time: '48:08'
  who: Alexey
- line: Yes, that could definitely help.
  sec: 2895
  time: '48:15'
  who: Raphaël
- line: 'A question from Sam: what would you say is the best place to start when implementing
    MLOps in a new team? The team has experimented with Vertex AI.'
  sec: 2898
  time: '48:18'
  who: Alexey
- line: What is the question again?
  sec: 2911
  time: '48:31'
  who: Raphaël
- line: What is the best way to start implementing MLOps in a new team?
  sec: 2913
  time: '48:33'
  who: Alexey
- header: 'Getting Started: prioritize CI/CD and solve tangible pain points'
- line: It’s important, from a product management perspective, to think about what’s
    most needed in the organization. What challenge are you trying to solve? If the
    problem is that you have models running in production and you don’t know what
    they’re doing, then you start with monitoring. Or, if the issue is that deploying
    new model versions takes too long and you want to deploy a new model every month,
    but it currently takes multiple months, then you start there. It’s a hard question
    to answer without more context, but there’s always low-hanging fruit in MLOps.
    For me, CI/CD is always the starting point. You can set that up quickly.
  sec: 2921
  time: '48:41'
  who: Raphaël
- line: So, essentially, you talk to data scientists and users, understand their pain
    points, and go from there.
  sec: 2992
  time: '49:52'
  who: Alexey
- line: Yes. And if you want to start with MLOps, you should first assess whether
    you can build it with the tools you already have. If you have the tools, use them.
    Sometimes people want new tools, but depending on the company type, getting new
    tools can take a lot of time. In a startup, you can just get the tools with a
    credit card, but in corporate environments, procurement processes can be very
    slow. So, if you can, start with what you have. But if you find you have nothing
    to work with, realize that early. For instance, version control is crucial. If
    you don’t have it, you should flag it with leadership right away and get a subscription
    to the right tools.
  sec: 3004
  time: '50:04'
  who: Raphaël
- header: 'MLOps Toolset: experiment tracking, model registry, serving, and monitoring'
- line: Another question came up as you were speaking. You mentioned building MLOps
    with the tools you have. What does it actually mean to build MLOps? How do we
    know when we have a complete MLOps setup? Is there a set of tools we need to have
    to say, “Okay, we have MLOps now”?
  sec: 3081
  time: '51:21'
  who: Alexey
- line: Yes, great question. There are a few frameworks for MLOps. Last year, I worked
    with Marvelous MLOps on a blog and content platform, and they created a framework
    called the MLOps Toolbelt. This is the one I like best. It’s based on existing
    literature and includes a set of components. If you have all of these, you’ve
    got a good MLOps setup.
  sec: 3108
  time: '51:48'
  who: Raphaël
- line: These components include experiment tracking, model registry, and monitoring?
  sec: 3153
  time: '52:33'
  who: Alexey
- line: Yes, exactly. Version control, CI/CD, containerization, model registry, experiment
    tracking, container registry, monitoring, compute, and serving. Another important
    component is a package registry. So, you should have a private package registry
    as well.
  sec: 3159
  time: '52:39'
  who: Raphaël
- header: 'Dependency Management: package registries for reproducible deployments'
- line: Why do we need a package registry?
  sec: 3188
  time: '53:08'
  who: Alexey
- line: You can certainly build a Docker image without it, but what I like about packaging
    is that you can bundle a lot of things together. For example, with Python packaging,
    you can manage dependencies and configurations. Python packaging lets you configure
    important settings in the package itself, which helps ensure compatibility with
    other software. You can define version ranges for your dependencies, which helps
    maintain compatibility with other packages.
  sec: 3207
  time: '53:27'
  who: Raphaël
- line: Why not just use Docker?
  sec: 3252
  time: '54:12'
  who: Alexey
- line: You can build a Docker image, but you’re often not working in isolation. Your
    package may interact with other pieces of software. With a package, you can configure
    its dependencies, so it will work with other software components. Without that,
    you risk creating conflicts between versions. You could also set up each component
    as its own container, but that could add a lot of overhead.
  sec: 3256
  time: '54:16'
  who: Raphaël
- line: I imagine that if everything isn't in its own container — each with its own
    dependencies like Pandas or NumPy — then when we deploy multiple models, each
    one will have its own set of dependencies, which can add up quickly. However,
    if we use packages, maybe all models could share the same version of a dependency
    like Pandas.
  sec: 3354
  time: '55:54'
  who: Alexey
- line: Hopefully, this can be resolved within the version ranges. That’s what we
    aim for.
  sec: 3396
  time: '56:36'
  who: Raphaël
- line: It sounds like there are use cases where it's easier to use a package rather
    than build a full container.
  sec: 3401
  time: '56:41'
  who: Alexey
- header: 'Container Strategy: Docker, Kubernetes, Databricks trade-offs'
- line: We usually do both. We're using Databricks, but we also have a setup with
    Kubernetes. We build our Docker images using our packages. It really depends on
    the complexity of your setup. When I first saw it in 2019, someone advocated for
    separate Docker images for each component of the ML pipeline — one for data handling,
    one for preprocessing, one for modeling, etc. It seemed amazing because you could
    easily swap them out when something changes. But the question is, does it make
    it easier or more complex for teams to be autonomous, and for data scientists
    to use this setup?
  sec: 3410
  time: '56:50'
  who: Raphaël
- line: I noticed we have some questions in the live chat. Sorry, I was focused on
    Slido. Do you have time for one more question?
  sec: 3459
  time: '57:39'
  who: Alexey
- line: I actually have time. We originally planned to finish early, but I have another
    half hour.
  sec: 3468
  time: '57:48'
  who: Raphaël
- header: 'Success & Failure Stories: deployment wins and integration freezes'
- line: Great! Let’s go for it. Here's a question from Zanna. She's starting in data
    management and likes your point about addressing pain points to get people to
    use your solution. Do you have examples where this approach worked well, and where
    it failed?
  sec: 3476
  time: '57:56'
  who: Alexey
- line: A successful example is something you mentioned earlier — showing the frequency
    of deployments. We showed the business that they were deploying just a few times
    a year, and then we showed them that with the new setup, they could deploy every
    day.
  sec: 3500
  time: '58:20'
  who: Raphaël
- line: So deployment was a pain point for them?
  sec: 3517
  time: '58:37'
  who: Alexey
- line: Yes, deployment was taking a long time. Training and compute were also slow,
    and testing took forever. We convinced them that with MLOps, we could do all of
    this in just a few hours, and that worked. As for a failure, we built a successful
    data science solution in a proof of concept for a client, and then sold them a
    data platform. In 2019, we recommended setting up a data lake, using Databricks
    to run models. We built and sold all these projects, but where we failed was when
    they had an integration freeze. The board decided no integrations were allowed.
    We had a complete data platform, but it couldn’t connect to any data sources.
    We worked there for two years, but in the end, we couldn't convince them to proceed.
  sec: 3520
  time: '58:40'
  who: Raphaël
- line: That must have been frustrating.
  sec: 3606
  time: '1:00:06'
  who: Alexey
- line: Yes, I had two difficult projects in a row, both just under a year. This was
    the second one. After that, I took some time off. I also faced some personal challenges,
    but I essentially burned out on that project.
  sec: 3609
  time: '1:00:09'
  who: Raphaël
- line: I can imagine. When there’s a decision you can’t influence and they just say,
    "We're not integrating it," it’s hard to understand why you spent all that time
    building it.
  sec: 3629
  time: '1:00:29'
  who: Alexey
- line: Exactly. They made us build something that would run on uploaded CSV files,
    so that’s what we did, but it wasn’t the right approach.
  sec: 3642
  time: '1:00:42'
  who: Raphaël
- header: 'MLOps Defined: operationalizing machine learning in business'
- line: 'Here’s an interesting question we might have started with earlier: What is
    MLOps in simple terms?'
  sec: 3654
  time: '1:00:54'
  who: Alexey
- line: MLOps is the operational side of building machine learning solutions. You
    build a machine learning model, and then it starts running in your business. It's
    similar to other business operations, like HR or logistics — this is the machine
    learning equivalent.
  sec: 3665
  time: '1:01:05'
  who: Raphaël
- line: So it’s a set of tools and best practices to ensure machine learning models
    run smoothly?
  sec: 3685
  time: '1:01:25'
  who: Alexey
- line: Yes, exactly.
  sec: 3691
  time: '1:01:31'
  who: Raphaël
- line: And to make sure they keep running properly?
  sec: 3692
  time: '1:01:32'
  who: Alexey
- line: Yes, it’s not just about getting them running once — it’s about keeping them
    running properly. The challenge is that data changes all the time.
  sec: 3696
  time: '1:01:36'
  who: Raphaël
- line: And by adding "the right way," you’ve made it so much more complex!
  sec: 3706
  time: '1:01:46'
  who: Alexey
- line: Yeah, it’s definitely more complex than other software operations, especially
    because of the data aspect.
  sec: 3710
  time: '1:01:50'
  who: Raphaël
- header: 'Core Challenge: keeping models deployed, monitored, and maintained'
- line: I think you mentioned earlier that the focus of an MLOps team is on easy deployment,
    easy monitoring, and easy maintenance. But achieving all three is quite difficult,
    isn’t it?
  sec: 3718
  time: '1:01:58'
  who: Alexey
- line: Yes, it's a challenging field. You need to know a little bit about a lot of
    things, and then a lot about a few specific areas.
  sec: 3735
  time: '1:02:15'
  who: Raphaël
- line: I’ve got your GitHub profile here.
  sec: 3756
  time: '1:02:36'
  who: Alexey
- line: Yes, definitely.
  sec: 3759
  time: '1:02:39'
  who: Raphaël
- header: Closing Remarks and next steps
- line: OK, I think that’s all for today. You’ve shared a lot of valuable insights.
    Thank you, Raphaël. I’ve taken tons of notes! I’m really happy we finally managed
    to record this. It’s been great, and we’ve had a great turnout. Thanks to everyone
    for joining and for your active participation. And once again, thanks to Raphaël
    for sharing your experience with us. I hope you won’t face another integration
    freeze in your career — it must have been devastating. Thanks for sharing that
    with us as well.
  sec: 3762
  time: '1:02:42'
  who: Alexey
- line: Thank you so much for having me, and thanks to everyone who joined. I’d also
    like to give a big compliment to you and Johanna for how well you prepare and
    the questions you put together. I really enjoy this format. It’s been a pleasure
    to be here today. Big thanks to both of you!
  sec: 3809
  time: '1:03:29'
  who: Raphaël
- line: The credit goes to Johanna.
  sec: 3827
  time: '1:03:47'
  who: Alexey
- line: Yes, for sure. She did a great job!
  sec: 3829
  time: '1:03:49'
  who: Raphaël
- line: OK, thanks everyone, and thanks again, Raphaël. Just a reminder, tomorrow
    we have another podcast episode, so I hope to see you all then.
  sec: 3832
  time: '1:03:52'
  who: Alexey
- line: Hopefully, I’ll see you again soon. Have a good day!
  sec: 3845
  time: '1:04:05'
  who: Raphaël
- line: Yes, goodbye!
  sec: 3847
  time: '1:04:07'
  who: Alexey
- line: Bye-bye!
  sec: 3847
  time: '1:04:07'
  who: Raphaël
context: 'Context: Raphaël Hoogvliets (Eneco) walks through his journey from agriculture
  to data science and MLOps, illustrating real-world tradeoffs in design, team structure,
  tooling, and delivery while sharing concrete practices, stories, and metrics for
  operationalizing ML.

  Core narrative: MLOps is fundamentally about operationalizing machine learning as
  sustainable product engineering—building an enabling, platform-led way of working
  that brings cross-functional teams, pragmatic engineering practices (CI/CD, reproducibility,
  testing, dependency management), and iterative adoption together so organizations
  can balance speed versus robustness, build trust with quick wins and measured KPIs,
  and keep models reliably deployed and delivering business impact.'
---
