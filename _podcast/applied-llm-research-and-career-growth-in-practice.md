---
title: 'Applied LLM Research & Career Growth: Long-Context Evaluation, Prototyping
  & Industry Publishing'
short: Build a Strong Career in Data
season: 20
episode: 7
guests:
- lavanyagupta
image: images/podcast/s20e07-build-strong-career-in-data.jpg
ids:
  anchor: datatalksclub/episodes/Build-a-Strong-Career-in-Data---Lavanya-Gupta-e32k61phttps://creators.spotify.com/pod/show/datatalksclub/episodes/Build-a-Strong-Career-in-Data---Lavanya-Gupta-e32k61p
  youtube: ekG5zJioyFs
links:
  anchor: https://creators.spotify.com/pod/show/datatalksclub/episodes/Build-a-Strong-Career-in-Data---Lavanya-Gupta-e32k61phttps://creators.spotify.com/pod/show/datatalksclub/episodes/Build-a-Strong-Career-in-Data---Lavanya-Gupta-e32k61p
  apple: https://podcasts.apple.com/us/podcast/build-a-strong-career-in-data-lavanya-gupta/id1541710331?i=1000706988972
  spotify: https://open.spotify.com/episode/2mJXd0lSZFPKJA0ZrG9iS2
  youtube: https://www.youtube.com/watch?v=ekG5zJioyFs
description: Learn LLM research tactics, long-context evaluation approaches and prototyping
  tips to boost your career, publish industry work, and ship impactful models.
intro: How do you evaluate and prototype long-context LLMs in a real-world setting
  while advancing a career as an applied researcher? In this episode Lavanya Gupta
  — a Carnegie Mellon Language Technologies Institute alum and Sr. AI/ML Applied Scientist
  at JPMorgan Chase’s Machine Learning Center of Excellence — walks through practical
  strategies for applied LLM research and career growth. With 5+ years of industrial
  research experience, public talks at WiDS, PyData, TensorFlow User Group and reviewer
  roles for NeurIPS 2024, ICLR 2025 and NAACL 2025, Lavanya connects technical practice
  with professional development. <br><br> We cover core topics including long-context
  evaluation methodologies for transformer models, rapid prototyping workflows for
  LLM systems, and best practices for industry publishing and technical communication.
  Listeners will get actionable guidance on setting up reproducible experiments, balancing
  research rigor with product timelines, and positioning industry work for peer-reviewed
  venues. This episode is for machine learning engineers, NLP researchers, and applied
  scientists seeking concrete tactics for prototyping LLMs, conducting robust long-context
  evaluations, and growing a research-oriented career in industry.
dateadded: 2025-05-12
duration: PT00H58M10S
quotableClips:
- name: Episode Introduction & Topic Overview
  startOffset: 0
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=0
  endOffset: 122
- name: 'Career Overview: From Software Engineering to ML & Master''s'
  startOffset: 122
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=122
  endOffset: 205
- name: 'Origin of ML Interest: Hackathons and Computer Vision'
  startOffset: 205
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=205
  endOffset: 295
- name: 'Early Project Case Study: OCR for Organization Charts'
  startOffset: 295
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=295
  endOffset: 523
- name: 'Role Snapshot: LLM Benchmarking at a Financial Institution'
  startOffset: 523
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=523
  endOffset: 615
- name: 'Research Focus: Evaluating Long-Context LLMs'
  startOffset: 615
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=615
  endOffset: 756
- name: 'Empirical Findings: Context Window Performance Droparound 32k–64k'
  startOffset: 756
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=756
  endOffset: 894
- name: 'Practical Approach: Chunking, Retrieval, and Summarization for Large Docs'
  startOffset: 894
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=894
  endOffset: 928
- name: 'Published Work: "Long Context LLMs on Financial Concepts" (EMNLP)'
  startOffset: 928
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=928
  endOffset: 1048
- name: 'Industry Research Practices: Publishing from Corporate Teams'
  startOffset: 1048
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=1048
  endOffset: 1185
- name: 'Motivation for Publications: Manager Support and Community Sharing'
  startOffset: 1185
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=1185
  endOffset: 1330
- name: 'Dissemination Paths: arXiv, Endorsement, and Early Publications'
  startOffset: 1330
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=1330
  endOffset: 1501
- name: 'Self-Learning & MLOps: Zoom Camps, Tutorials, and Mentoring'
  startOffset: 1501
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=1501
  endOffset: 1814
- name: 'Rapid Prototyping Tools: Streamlit for Demos and Feedback'
  startOffset: 1814
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=1814
  endOffset: 2004
- name: 'Kaggle Success Story: Building and Licensing a High-Impact Dataset'
  startOffset: 2004
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=2004
  endOffset: 2252
- name: 'Community Contribution: Women in Data Science and Open Mentoring'
  startOffset: 2252
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=2252
  endOffset: 2473
- name: 'Opportunity & Persistence: Timing, Luck, and "Shooting Arrows"'
  startOffset: 2473
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=2473
  endOffset: 2724
- name: 'Career Pivot Guidance: Non-CS Backgrounds Entering Data Roles'
  startOffset: 2724
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=2724
  endOffset: 2908
- name: 'Networking & Mentorship: Cold Outreach and Building Rapport'
  startOffset: 2908
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=2908
  endOffset: 3088
- name: 'Portfolio Strategy: Community Visibility vs. Job-Targeted Projects'
  startOffset: 3088
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=3088
  endOffset: 3273
- name: 'Interview Preparation: LeetCode, Conceptual Mastery, and Mock Interviews'
  startOffset: 3273
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=3273
  endOffset: 3416
- name: 'Project Selection: Industry-Backed Work for Real-World Impact'
  startOffset: 3416
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=3416
  endOffset: 3466
- name: Episode Wrap-Up & Final Career Advice
  startOffset: 3466
  url: https://www.youtube.com/watch?v=ekG5zJioyFs&t=3466
  endOffset: 3490
transcript:
- header: Episode Introduction & Topic Overview
- line: This week we'll talk about building a strong career in data and we have a
    special guest today.
  sec: 0
  time: 0:00
  who: Alexey
- line: Yes.
  sec: 109
  time: '1:49'
  who: Lavanya
- line: Great! NLP - tell us more about that. Welcome to the show Lavanya.
  sec: 116
  time: '1:56'
  who: Alexey
- header: 'Career Overview: From Software Engineering to ML & Master''s'
- line: Thank you, great to meet all of you.
  sec: 122
  time: '2:02'
  who: Lavanya
- line: The questions for today's interview are prepared by Johana. Thanks Yana for
    your help as always. Before we go into our main topic of building a strong career
    in data, let's start with your background. Can you tell us about your career journey
    so far?
  sec: 122
  time: '2:02'
  who: Alexey
- line: Happy to share my journey. I'll keep it short for now - we can delve deeper
    into the interesting parts later. After my undergrad in India in 2016, I worked
    for a couple of years as a software engineer until I got interested in AI/ML -
    back then it was more of data science stuff.
  sec: 142
  time: '2:22'
  who: Lavanya
- line: In 2018 I decided to switch from my software engineering profile to an ML
    profile, worked for a year in India, then decided to pursue my masters. I came
    to the US, did my masters from Carnegie Mellon, and now I'm working with JP Morgan
    in their ML vertical. That's quickly a short overview about my story.
  sec: 142
  time: '2:22'
  who: Lavanya
- header: 'Origin of ML Interest: Hackathons and Computer Vision'
- line: This is a typical question we ask, but since today's interview is mostly about
    your career, this question is kind of what the entire interview will be about.
    How did you actually become interested in machine learning and AI? Was there a
    specific moment or project that sparked your curiosity?
  sec: 205
  time: '3:25'
  who: Alexey
- line: Definitely. The interesting part is that while I was doing my undergrad, I
    had zero courses in ML or data science - this was back in 2016 when very few specialized
    people were doing ML. I joined my regular software engineering role with HSBC
    until I participated in a hackathon.
  sec: 232
  time: '3:52'
  who: Lavanya
- line: My interest peaked through computer vision - although now I'm deep into NLP,
    I started my journey in ML with vision models. ImageNet and those kind of things
    were really hot back in 2016-2017. I participated in a lot of hackathons within
    my firm where we were given liberty to experiment with new things coming up apart
    from my regular role. That's definitely a clear moment for me where my interests
    got piqued over 2-3 years of participating in hackathons.
  sec: 232
  time: '3:52'
  who: Lavanya
- line: Do you remember any of the projects you did there?
  sec: 287
  time: '4:47'
  who: Alexey
- header: 'Early Project Case Study: OCR for Organization Charts'
- line: One project that stood out and actually got converted into a product in the
    bank was an OCR model. When onboarding customers, the bank received organization
    charts - complicated flowchart diagrams. Back in 2016 with no fancy LLMs, we built
    specific models that could parse this structure to find relationships between
    entities and auto-populate forms showing the organization hierarchy - who reports
    to who.
  sec: 295
  time: '4:55'
  who: Lavanya
- line: We used vision models to extract boxes, arrow connections, then BFS/DFS algorithms
    to find connections, and finally OCR through Google Cloud to extract text within
    boxes and put it all together. It looks pretty naive today, but was very interesting
    stuff back then.
  sec: 295
  time: '4:55'
  who: Lavanya
- line: If you ask me now how to solve this problem, I wouldn't know - you could just
    send it to ChatGPT and it would work. But apart from that I have zero idea how
    to actually approach that. You were a developer - you probably had even less idea
    than I do now.
  sec: 378
  time: '6:18'
  who: Alexey
- line: It was a software engineering approach - just hacking parts of it together.
    None of us had an ML background, so we did the most basic things possible. It
    was a rewarding experience but definitely something that piqued my interests in
    this field.
  sec: 413
  time: '6:53'
  who: Lavanya
- line: I can imagine - if you have zero knowledge but manage to hack something together
    in a couple days just by Googling and trying tutorials, and it works in the end,
    that must be quite rewarding.
  sec: 438
  time: '7:18'
  who: Alexey
- line: Exactly. Also back then information was limited so it was easier to evaluate
    all options and pick one, instead of having 100 possible solutions now. That's
    another downside today.
  sec: 450
  time: '7:30'
  who: Lavanya
- line: So you're saying before it was easier because there were only handful of articles,
    and anything you did would come out as innovative because not many people were
    in this field?
  sec: 474
  time: '7:54'
  who: Alexey
- line: Yes.
  sec: 485
  time: '8:05'
  who: Lavanya
- line: These days people wouldn't worry about these things - they would just...
  sec: 490
  time: '8:10'
  who: Alexey
- line: I think the first thing people would do now is just ask any of the vision
    LLMs to do this for you.
  sec: 497
  time: '8:17'
  who: Lavanya
- line: Is it something you do now? You're doing something with LLMs right now, right?
  sec: 503
  time: '8:23'
  who: Alexey
- line: As I said, I started with vision but now I'm deeply into NLP, so not doing
    much on vision side now.
  sec: 509
  time: '8:29'
  who: Lavanya
- header: 'Role Snapshot: LLM Benchmarking at a Financial Institution'
- line: We benchmark on quality as well as deployment aspects like latency and throughput,
    then publish developer guidance, blogs, do webinars to share experiences with
    models and best practices. These models are sensitive to prompting and keep changing
    with new releases every week. We also negotiate with providers for best rates
    given the bank's high usage. So I'm heavily into benchmarking LLMs.
  sec: 523
  time: '8:43'
  who: Currently I'm more into... My team at JP Morgan is specifically focused on
    benchmarking LLMs. We talk to model providers like OpenAI, Anthropic, Meta - take
    their models and benchmark them on our internal datasets. Our team is the first
    entry barrier for any model the bank wants to ingest.
- line: I didn't ask you before - can you tell us more about these projects or is
    this something you can't discuss because banks take these things very seriously?
    I know some banks don't allow discussing work details - is that the case?
  sec: 595
  time: '9:55'
  who: Alexey
- header: 'Research Focus: Evaluating Long-Context LLMs'
- line: 'We can talk about the published work that we have. That''s also a similar
    domain. Benchmarking can be done on different aspects: traditional NLU, long context,
    code generation capabilities, math capabilities, and multimodal stuff.'
  sec: 615
  time: '10:15'
  who: Lavanya
- line: My focus for the published work was on the long context capabilities. These
    models claim they have this huge context window of 128k, but can they really read
    a 200- or 500-page book and give you the correct answer as they claim? We delved
    deep into that, so we can definitely talk about it since it's published.
  sec: 615
  time: '10:15'
  who: Lavanya
- line: 'So, short answer: can they read a 500-page book and give an answer?'
  sec: 650
  time: '10:50'
  who: Alexey
- line: I'll say yes and no. Yes on easier datasets, and no on real datasets.
  sec: 658
  time: '10:58'
  who: Lavanya
- line: If you check the published work or public benchmarks, they are really positive
    and say the models are getting better with time. But there is this concept of
    artificially simplified tasks, which makes it easier for the model. When you use
    the same model in the real world, especially in specialized domains like healthcare
    or finance, you start to see the pitfalls of these models in longer contexts.
  sec: 658
  time: '10:58'
  who: Lavanya
- line: Let's say after we finish recording this podcast, I get the transcript and
    put it into ChatGPT. If I ask questions based on the transcript, I'm pretty sure
    it will work fine because it's only one hour. I don't know how many pages that
    translates to.
  sec: 700
  time: '11:40'
  who: Alexey
- line: But if I take an Andrew Huberman podcast, like one of the episodes I listened
    to the other day, it's four hours long-probably the length of a book. The topics
    go deep and sometimes they talk about the latest research. I'm wondering if the
    model will be able to actually answer questions correctly there. What do you think?
  sec: 700
  time: '11:40'
  who: Alexey
- header: 'Empirical Findings: Context Window Performance Droparound 32k–64k'
- line: I have two opinions on that. In our studies, it's hard to say at what range
    in the context window the models fall, so we split it into less than 32k tokens
    and greater than 32k tokens. There's a clear dip around that.
  sec: 756
  time: '12:36'
  who: Lavanya
- line: We decided on 32k tokens because, at least in the bank, all our use cases
    usually fall within 32k tokens. As you said, maybe this transcript would be way
    less than that, but maybe the Huberman podcast would be much greater. If it's
    a shorter input, it's better, and as you push the boundary up to 128k, that's
    where you start to see these falls in the model capabilities.
  sec: 756
  time: '12:36'
  who: Lavanya
- line: We can talk more about the task, but that's another aspect. For natural language,
    the model will definitely give you something and you might think, "Oh, looks nice."
  sec: 810
  time: '13:30'
  who: The second thing is the natural language aspect. If you give these models a
    task to summarize or answer questions, it's easy for them to make up stuff, and
    it's hard for you to objectively verify whether it's correct or not. The tasks
    we evaluate the models on are very objective, like auto evals, so there's no subjectivity.
    For this, you would do something like a ROUGE score, but what we do is very specific,
    like precision and recall on specific data.
- line: Because it will start reading.
  sec: 855
  time: '14:15'
  who: Alexey
- line: It will still be grounded somewhere in the context but might not be exactly
    correct.
  sec: 861
  time: '14:21'
  who: Lavanya
- line: Should we, before long context models were available, chunk every page as
    a document, index it, ask a question, retrieve relevant pages, and then let the
    LLM summarize these pages to get the answer?
  sec: 869
  time: '14:29'
  who: Alexey
- header: 'Practical Approach: Chunking, Retrieval, and Summarization for Large Docs'
- line: 'Even when we try to use it in our bank, we know these models fail at around
    the 64k context, even though we are using these fancy 128k models. We do the same
    thing you said: we chunk it because we know up to this point the models don''t
    fail usually. So we chunk it and then do whatever is the downstream processing.'
  sec: 894
  time: '14:54'
  who: Lavanya
- line: And this is related to the paper you published, right?
  sec: 913
  time: '15:13'
  who: Alexey
- line: Yes.
  sec: 921
  time: '15:21'
  who: Lavanya
- header: 'Published Work: "Long Context LLMs on Financial Concepts" (EMNLP)'
- line: This was the paper we talked about at EMNLP. The name of the paper is "Systematic
    Evaluation of Long Context LLMs on Financial Concepts." That's exactly the paper,
    and this is what we're talking about right now, right?
  sec: 928
  time: '15:28'
  who: Alexey
- line: Yes, yes, this is exactly it.
  sec: 934
  time: '15:34'
  who: Lavanya
- line: Did you like writing the paper?
  sec: 942
  time: '15:42'
  who: Alexey
- line: Yes. In fact, it was my fourth publication. I had some publications back when
    I was studying, but they were not core NLP-they were all over the place.
  sec: 942
  time: '15:42'
  who: Lavanya
- line: As I said, I started out with data science and spent some time in visualization,
    so all my papers were on different topics. This was really rewarding, and it's
    an ACL conference, which is really reputed in the community. That was really nice.
  sec: 942
  time: '15:42'
  who: Lavanya
- line: ACL is Association for Computational Linguistics?
  sec: 974
  time: '16:14'
  who: Alexey
- line: Yes, Association for Computational Linguistics.
  sec: 980
  time: '16:20'
  who: Lavanya
- line: I did my master's also, it was related to NLP. I was working with mathematical
    formulas in Wikipedia and I remember this abbreviation from those days. It's like
    one of the top conferences, right?
  sec: 986
  time: '16:26'
  who: Alexey
- line: It's really top tier, highly filtered, and attended by really smart and brilliant
    people, like Andrew Ng and all of these people. You can imagine.
  sec: 997
  time: '16:37'
  who: Lavanya
- line: This is pretty rare, from what I see, to work on a paper while working at
    a company. Typically, when you work at a company, you are concerned about other
    things. At most, maybe you write a blog post or give a talk at a conference, but
    here you wrote a scientific paper for a scientific conference. Is it common for
    people at JPMorgan Chase to do this?
  sec: 1019
  time: '16:59'
  who: Alexey
- header: 'Industry Research Practices: Publishing from Corporate Teams'
- line: I think so. The vertical I work with is MLCO. We are a group of about 150
    people, so we are not tied to a single use case or product. We do work on products,
    but we also have some creative liberty because we do a lot of new stuff.
  sec: 1048
  time: '17:28'
  who: Lavanya
- line: If anything new comes up, we do publish, although it's in the industry track
    and we are not allowed to release the data. A lot of us keep coming across new
    findings. Our teams are divided into NLP, graphs, multimodal, vision, and speech.
    We have specialized teams for all of these, so there are very active and smart
    people in each group. If anything new comes up, there's always encouragement to
    go ahead and publish your work, but it comes in addition to your regular work-it's
    extra effort.
  sec: 1048
  time: '17:28'
  who: Lavanya
- line: It's always encouraged.
  sec: 1048
  time: '17:28'
  who: Lavanya
- line: Extra effort, because I know that writing a paper, especially for a top-tier
    conference, is not easy. If it comes as extra effort to what you already do-let's
    say you work 40 hours per week and then you probably need another 40 hours per
    week for a paper-how did you manage that?
  sec: 1124
  time: '18:44'
  who: Alexey
- line: This one was definitely something that my manager and I were excited about.
    When we tried to look up the existing research, we found limited resources, so
    we knew this would be a unique contribution.
  sec: 1144
  time: '19:04'
  who: Lavanya
- line: It was a lot motivated by knowing the scope, that this is an underexplored
    area. Maybe for things that are well developed, it needs a lot more thinking through,
    but for us, as soon as we found something in our experiments, we thought, "Okay,
    this needs to go out in the public, people are not aware of this."
  sec: 1144
  time: '19:04'
  who: Lavanya
- header: 'Motivation for Publications: Manager Support and Community Sharing'
- line: Since our conversation is about building a strong career in data, even though
    we talk specifically about your career, I think it's really good for a career
    to have publications. Most of us just go to work, do some stuff related to work,
    but at the end, we don't share the results. Even if we do, it's typically not
    top-tier scientific conferences.
  sec: 1185
  time: '19:45'
  who: Alexey
- line: How can we force ourselves to go this extra mile when it's extra work and
    it's not simple work, it's complicated work? How do you motivate yourself? You
    said that there is a unique contribution, but still, I imagine you could be pretty
    drained after work. Writing papers is super difficult, at least for me. I remember
    being a master's student and I really hated working on my thesis. That was terrible.
  sec: 1185
  time: '19:45'
  who: Alexey
- line: When I chose my master's, I had the option of doing a thesis. In undergrad,
    it was much easier, but I was so bad at it that I didn't want to do a thesis option.
    It takes so much out of you.
  sec: 1248
  time: '20:48'
  who: Lavanya
- line: But as I said, for me, my manager was a really good motivator. He had more
    experience in this field and was certain we should definitely put this out. I
    had moments of doubt, wondering if it was worth it with all the effort, but having
    guidance or a motivator besides yourself is really helpful.
  sec: 1248
  time: '20:48'
  who: Lavanya
- line: Published work is always valuable, even if it's not accepted to any conference.
    We were okay with just putting it out on arXiv. Unique contributions always shine,
    if nothing else. We were certain about at least putting a good quality paper on
    arXiv. If it gets accepted, great; if not, that's fine.
  sec: 1248
  time: '20:48'
  who: Lavanya
- line: Having the motivation to share it with the community is nice. It's hard, but
    it's nice.
  sec: 1248
  time: '20:48'
  who: Lavanya
- header: 'Dissemination Paths: arXiv, Endorsement, and Early Publications'
- line: I remember I took part in a competition and after the competition, I wrote
    a summary and, without thinking too much, just uploaded it to arXiv. People keep
    citing it even now.
  sec: 1330
  time: '22:10'
  who: Alexey
- line: The competition was something like fake news detection. Even now, I recently
    discovered there's one more citation. It's not overly cited-maybe ten or so-but
    it was something I didn't really think about. I just had this piece of writing,
    quickly put it together, generated a PDF, and uploaded it to arXiv.
  sec: 1330
  time: '22:10'
  who: Alexey
- line: You just upload the LaTeX file. I didn't really think much about it, just
    uploaded it and forgot. Later, I discovered people cited it and was like, wow.
  sec: 1330
  time: '22:10'
  who: Alexey
- line: That's what I did in my undergrad when I was first getting into this field.
    People said, "Let's just upload it there," and I thought, who is even going to
    look at it? There are hundreds and hundreds of publications out there. People
    want to trust credible sources, so I understand.
  sec: 1384
  time: '23:04'
  who: Lavanya
- line: Even arXiv has really high standards. You can't just upload anything; they
    check that your paper is well formatted, and you need to be endorsed by existing
    members. It's still a regulated community. So, even if you upload to arXiv, it's
    nice.
  sec: 1384
  time: '23:04'
  who: Lavanya
- line: Now I remember that I needed to ask a friend to endorse me. There were some
    categories where you didn't need endorsement. For machine learning, you needed
    endorsement, but for information retrieval back then, you did not-maybe now things
    have changed.
  sec: 1423
  time: '23:43'
  who: Alexey
- line: My earlier work, like my thesis, I just uploaded to arXiv because it was kind
    of related to NLP to some extent.
  sec: 1423
  time: '23:43'
  who: Alexey
- line: Correct, yes. I uploaded it to computer science, and that required endorsement.
  sec: 1447
  time: '24:07'
  who: Lavanya
- line: From what I understand by talking to you now, you were always looking to go
    the extra mile. When you were working as a software developer, you took part in
    hackathons.
  sec: 1455
  time: '24:15'
  who: Alexey
- line: Now you write research papers even though it's extra work on top of what you
    do. You engage in all these extra activities in the space. How do you find motivation
    to do that, and how has it been beneficial for you?
  sec: 1455
  time: '24:15'
  who: Alexey
- header: 'Self-Learning & MLOps: Zoom Camps, Tutorials, and Mentoring'
- line: In the hackathon space, going back to that era ten years ago, there were very
    limited resources. I was pretty active on the web at that time to self-learn.
    Self-learning was crucial for me because I had just finished my undergrad.
  sec: 1501
  time: '25:01'
  who: Lavanya
- line: I'm not sure if you remember, but we had a conversation back in 2020. I reached
    out to you randomly on LinkedIn.
  sec: 1501
  time: '25:01'
  who: Lavanya
- line: It was on LinkedIn, yes. We talked for a while.
  sec: 1501
  time: '25:01'
  who: Lavanya
- line: I went through a tutorial you made, found it interesting, and maybe got stuck
    at some step because I wanted something different from what you showed. So I reached
    out to you and asked for help.
  sec: 1539
  time: '25:39'
  who: I think I was doing some pet project and wanted some help around Docker or
    AWS Lambda-something in MLOps. You were doing these Zoom Camps back then. There
    were very limited resources, and I wanted quick, easy, trusted, credible help.
- line: That's what I used to do then-quickly seek help and self-learn. It's interesting
    because I completely forgot about it after we talked. You gave me plenty of resources,
    I finished the project, thanked you, and forgot about it until this session came
    up.
  sec: 1539
  time: '25:39'
  who: I think I was doing some pet project and wanted some help around Docker or
    AWS Lambda-something in MLOps. You were doing these Zoom Camps back then. There
    were very limited resources, and I wanted quick, easy, trusted, credible help.
- line: I realized, oh my god, I've spoken to Alexey before. I remember talking in
    detail about MLOps. At that time, we also discussed my roles-I was doing some
    mentoring, instructing, and developing a course with DataCamp. We talked a lot,
    all over the place.
  sec: 1539
  time: '25:39'
  who: I think I was doing some pet project and wanted some help around Docker or
    AWS Lambda-something in MLOps. You were doing these Zoom Camps back then. There
    were very limited resources, and I wanted quick, easy, trusted, credible help.
- line: That also keeps me motivated, seeing people doing so many new things. You
    were running Zoom Camp, I was doing my own pet projects. It was great.
  sec: 1539
  time: '25:39'
  who: I think I was doing some pet project and wanted some help around Docker or
    AWS Lambda-something in MLOps. You were doing these Zoom Camps back then. There
    were very limited resources, and I wanted quick, easy, trusted, credible help.
- line: It's cool. I went through our conversation and felt nostalgic about all the
    things we were doing. We were talking about deploying with Lambda, and back then
    there was a new feature where you could put Lambda inside a Docker container and
    serve it.
  sec: 1630
  time: '27:10'
  who: Alexey
- line: I remember making a post about this on LinkedIn and getting a lot of likes-maybe
    500 in a minute, though I'm exaggerating. People really liked this stuff back
    then.
  sec: 1630
  time: '27:10'
  who: Alexey
- line: So, you try to be active, self-learn, and when you're stuck, you reach out
    to people, make connections, and all this motivates you to do more and give back
    to the community.
  sec: 1630
  time: '27:10'
  who: Alexey
- line: Sometimes I feel I'm doing too much-I get too much information from the web.
    I see something and think, "I want to do this," or "This person is so cool, I
    want to contribute here."
  sec: 1681
  time: '28:01'
  who: Lavanya
- line: At that time, as I mentioned, I was in a software development profile. My
    aim was to become a full stack ML engineer, which was a hot term then-to know
    everything from data processing to data modeling and deployment.
  sec: 1681
  time: '28:01'
  who: Lavanya
- line: I was really interested in that. You were publishing a lot about MLOps, so
    you were my go-to resource for that. For data processing, I would speak to other
    people to get pipelines built.
  sec: 1681
  time: '28:01'
  who: Lavanya
- line: I was really inspired by this full stack ML role.
  sec: 1681
  time: '28:01'
  who: Lavanya
- line: And today, are you still inspired by this role, or maybe we don't need this
    role anymore?
  sec: 1740
  time: '29:00'
  who: Alexey
- line: I don't know if we don't need this role anymore, but I think now I've found
    my place more in NLP. The roots of software development are long forgotten.
  sec: 1747
  time: '29:07'
  who: Lavanya
- line: I still do some software development, but not so much. I think I've found
    my calling more in NLP.
  sec: 1747
  time: '29:07'
  who: Lavanya
- line: For me, the main idea behind this so-called full stack engineer or data scientist
    is not being afraid of things you don't know. There are things you need to do,
    and if not you, then who?
  sec: 1770
  time: '29:30'
  who: Alexey
- line: You can just take responsibility and say, "Okay, I'll try my best to do them."
    If you're not afraid of doing everything-from backend to frontend-even if you
    have no clue how it works, but you're willing to figure it out, then you are full
    stack.
  sec: 1770
  time: '29:30'
  who: Alexey
- line: I think people like that are still needed, especially today, because you have
    AI tools that can help you.
  sec: 1770
  time: '29:30'
  who: Alexey
- header: 'Rapid Prototyping Tools: Streamlit for Demos and Feedback'
- line: I think early in your career, it's hard but it's nice to get your hands dirty
    with everything. Then you don't feel like, "Oh, this is something I'm unaware
    of," or "This doesn't fall into my purview."
  sec: 1814
  time: '30:14'
  who: Lavanya
- line: Now, at least in my role, we use Streamlit a lot. As soon as you develop something,
    you don't want to wait for engineering or figure out how to pass it on to leadership
    for feedback. You don't want to be dependent on the engineering team. Streamlit
    has been a really valuable, quick spin-up tool to share what you've built and
    gather feedback.
  sec: 1814
  time: '30:14'
  who: Lavanya
- line: I remember creating React applications, which with Streamlit would have been
    much easier.
  sec: 1856
  time: '30:56'
  who: Alexey
- line: Instead of spending an hour figuring out React, you could just use Streamlit.
    But these days, with all this AI, it doesn't really matter, I guess.
  sec: 1856
  time: '30:56'
  who: Alexey
- line: From what I hear, you were interested in the full stack MLE role, which let
    you play with different areas. You were able to do things everywhere and then
    eventually realized the thing you like most is NLP, right?
  sec: 1856
  time: '30:56'
  who: Alexey
- line: Yes, NLP. I think it's also a consequence of my master's. When I went into
    my master's, I didn't have the idea that I would go deeper into NLP.
  sec: 1907
  time: '31:47'
  who: Lavanya
- line: But the way my course was structured and the people I interacted with really
    influenced me. My program was part of LTI at CMU-the Language Technologies Institute-which
    is highly focused on language and speech technologies.
  sec: 1907
  time: '31:47'
  who: Lavanya
- line: Because of the people I was surrounded with, that influenced me. Going into
    my master's, I didn't have this thought, but at the end of two years, that's what
    I realized.
  sec: 1907
  time: '31:47'
  who: Lavanya
- line: When we started this conversation, we talked about the hackathon, and then
    I asked you about what you do. Previously it was vision, now it's NLP. I asked
    you what you do, and we kind of jumped ten years or so.
  sec: 1958
  time: '32:38'
  who: Alexey
- line: But as we continued talking, you mentioned that you also did mentoring and
    were an instructor at DataCamp. Can you tell us more about these extra activities
    you did in addition to your main work or studies?
  sec: 1958
  time: '32:38'
  who: Alexey
- header: 'Kaggle Success Story: Building and Licensing a High-Impact Dataset'
- line: Sure. All of this work was definitely on the side. I think it's just like
    connecting the dots. Sometimes when I was free, I would attend Zoom camps like
    yours and others.
  sec: 2004
  time: '33:24'
  who: Lavanya
- line: One thing I attended was a web scraping tutorial. It was always fascinating
    back then to do web scraping, even with restrictions on websites that would block
    you. For some reason, I was looking up datasets on Kaggle and realized there were
    a lot of datasets on the Apple App Store, but not comprehensive ones on Google
    Play Store.
  sec: 2004
  time: '33:24'
  who: Lavanya
- line: I tried scraping and put the dataset on Kaggle. At one point, it was the highest
    voted dataset on Kaggle after the COVID dataset. In 2019, it had around 10-15k
    upvotes. I just checked before this webinar-currently, it's the tenth highest,
    but at one point, it was trending as the second highest.
  sec: 2045
  time: '34:05'
  who: At that time, the App Store's UI looked much simpler than Google Play, which
    had dynamic loading as you scrolled, while Apple was more static. Maybe that's
    why web scraping was more challenging for Google Play. This was back in 2019,
    so I'm speaking from memory.
- line: They wanted to onboard it as a paid project for learners. During COVID, self-learning
    and online learning were at their peak, so I got a lot of attention on that project.
    I made a guided version with guidelines for learners, then an unguided version.
    Things just connected one after the other.
  sec: 2098
  time: '34:58'
  who: Stuff like that, maybe it's luck, but also some motivation to do something
    unique. I published it on Kaggle, wrote a simple notebook with basic EDA and modeling,
    and then someone from DataCamp reached out to me because it was trending and getting
    a lot of interest.
- line: It's really amazing. We were talking about luck, and maybe we can attribute
    some of this to luck. I remember talking to Eugene Yan, who said something about
    luck that stuck with me.
  sec: 2186
  time: '36:26'
  who: Alexey
- line: He said, imagine you have a bow and want to shoot an arrow at a target. If
    you don't shoot any arrows, you won't hit the target. But if you shoot a hundred,
    maybe at least one will reach the target.
  sec: 2186
  time: '36:26'
  who: Alexey
- line: You need to be shooting, right?
  sec: 2186
  time: '36:26'
  who: Alexey
- line: I talk about opportunity-there are opportunities standing at your door. You
    just need to open it and peek out.
  sec: 2231
  time: '37:11'
  who: Lavanya
- line: For you, that was attending a tutorial, scraping the Google Play Store, and
    then it had a snowball effect, bringing more and more opportunities.
  sec: 2239
  time: '37:19'
  who: Alexey
- header: 'Community Contribution: Women in Data Science and Open Mentoring'
- line: Yes, definitely. DataCamp really highlighted my contribution, and I got more
    confidence to reach out to more people.
  sec: 2252
  time: '37:32'
  who: Lavanya
- line: I started getting involved in the community aspect, starting with Women in
    Data Science and small communities with local or regional chapters. I also reached
    out to more industry players to get more mentorship experience.
  sec: 2252
  time: '37:32'
  who: Lavanya
- line: That's really cool. What did you do as a mentor? What kind of things did you
    help people with?
  sec: 2293
  time: '38:13'
  who: Alexey
- line: All sorts of things. In one organization, I was an instructor and created
    my own modules to teach basic data science, EDA, all the way up to modeling.
  sec: 2304
  time: '38:24'
  who: Lavanya
- line: We went from regression to classification, and the last thing we covered was
    decision trees, random forest, boosting, that kind of stuff. All of this was targeted
    at introducing candidates to the module, but also with a focus on interviews.
  sec: 2304
  time: '38:24'
  who: Lavanya
- line: With organizations like Women in Data Science, it was more open mentoring.
    You could come and talk about whatever stage you were at in your life.
  sec: 2334
  time: '38:54'
  who: These organizations offer packages where you collaborate with them and they
    help with interview prep. So, a lot of mock interviews, teaching, resume refinement,
    LinkedIn reviews.
- line: So, just open mentoring sessions with these communities. It's a two-way exchange.
  sec: 2372
  time: '39:32'
  who: One time I had a really nice conversation with a college student who was brilliant
    and smart. There was a lot for me to learn from her too-she had already interned
    at Google. I was curious about the interview process.
- line: Maybe you have an opinion about this. Some might say you got lucky-your dataset
    trended and all these opportunities happened to you.
  sec: 2403
  time: '40:03'
  who: Alexey
- line: How would you encourage people to still try? It's unlikely if I just do a
    dataset, it will become trending. I've uploaded quite a few datasets on Kaggle,
    and none of them trend. Even if I had 10 or 20 upvotes, it didn't make much difference.
  sec: 2403
  time: '40:03'
  who: Alexey
- header: 'Opportunity & Persistence: Timing, Luck, and "Shooting Arrows"'
- line: Yeah, definitely. I mentioned luck because, as you said, at that time-during
    COVID-people were really active on Kaggle. Maybe the timing was luck, but it wasn't
    a random decision to just get up one day and scrape Google Play Store.
  sec: 2473
  time: '41:13'
  who: Lavanya
- line: I found my way out. I spoke to a lot of legal people. One of my friends told
    me I needed to license it properly, otherwise I could get in trouble.
  sec: 2518
  time: '41:58'
  who: I also compared it with the Apple App Store datasets and saw there were datasets
    for that, but not for Google Play. So there was definitely something missing.
    It wasn't just a matter of writing some code and it worked like magic. There were
    bots blocking me, and I got a lot of emails and notifications saying this wasn't
    allowed.
- line: It was a lot of work.
  sec: 2564
  time: '42:44'
  who: Alexey
- line: Yeah, it was not just scraping and uploading it. There's luck, but there's
    also analytical thought that went into why I wanted to do this.
  sec: 2564
  time: '42:44'
  who: Lavanya
- line: Yeah, I think this is even more important than the result. I mean, yes, it
    got trending and more people found you, but it's because of the effort you put
    in. You would probably put a similar amount of effort into a new project, and
    then one way or another-
  sec: 2576
  time: '42:56'
  who: Alexey
- line: Yeah, and you can't predict these things. You just can't predict them. You
    just have to do it, and if it happens, it happens.
  sec: 2599
  time: '43:19'
  who: Lavanya
- line: Yeah, I just checked my Kaggle datasets. The largest one has 171 upvotes,
    which I think is decent. It was a dataset with images of different clothing-pants,
    shorts, t-shirts-doing some image classification back then.
  sec: 2612
  time: '43:32'
  who: Alexey
- line: Yeah, I actually might have-I don't know, because at some point I was also
    building a fashion recommender system just as a pet project. So I did look into
    a lot of these clothing datasets on Kaggle. I might be one of the upvoters because
    I downloaded a ton of datasets from Kaggle on clothes.
  sec: 2631
  time: '43:51'
  who: Lavanya
- line: The reason I had this dataset-it's not like I just woke up one day and thought,
    "Let me do a dataset with clothing items." There was a reason. I was writing a
    book and needed to include pictures of datasets in the book, and all the other
    clothing datasets were scraped.
  sec: 2657
  time: '44:17'
  who: Alexey
- line: If I used a dataset from Amazon or Zalando or some other company, then the
    images belonged to them. If I printed them in a book, I might get into trouble.
    So I thought I needed to create a dataset with a good license that allowed me
    to do this kind of thing.
  sec: 2657
  time: '44:17'
  who: Alexey
- line: Yeah, one of my friends was into a lot of open-source contributions at that
    time, and he pointed out that there are these five types of licenses. He told
    me to quickly put a license on my dataset, otherwise I could get into trouble.
    I remember discussing it with him in detail.
  sec: 2706
  time: '45:06'
  who: Lavanya
- header: 'Career Pivot Guidance: Non-CS Backgrounds Entering Data Roles'
- line: Yeah, so we have a few questions. How can a career pivoter-somebody who's
    changing careers, a career changer without a computer science degree or main background-break
    into data?
  sec: 2724
  time: '45:24'
  who: Alexey
- line: Yeah, I think there are a lot of data roles, at least in the industry that
    I know. I'm in Seattle, which is a hub for Microsoft and Amazon. A lot of people
    here are from non-CS backgrounds, including a couple of my friends, all working
    for these big tech giants in data roles.
  sec: 2742
  time: '45:42'
  who: Lavanya
- line: My friend did civil engineering, did his master's in civil engineering, and
    then became a senior tech product manager at Amazon. Product manager is one thing.
    If you're early in your career, BI roles are also quick to get into.
  sec: 2766
  time: '46:06'
  who: If you're completely non-technical, a technical product manager is something
    people find easier to get into. You have business acumen, and all you need is
    to know a little about how software engineering works-not actual coding, but just
    how SQL works or things like that. That's a very good entry point.
- line: You can learn some skills, like Tableau or basic SQL, and just get going.
    Self-learning is crucial, but those roles still require effort because you need
    to spend time learning those technologies. But a product manager is easier to
    get through.
  sec: 2766
  time: '46:06'
  who: If you're completely non-technical, a technical product manager is something
    people find easier to get into. You have business acumen, and all you need is
    to know a little about how software engineering works-not actual coding, but just
    how SQL works or things like that. That's a very good entry point.
- line: Yeah, and we talked about Eugene. Eugene also lives in Seattle. I didn't realize
    you were from the same city. But it's a big city, right? It's not like you can
    randomly run into people on the street.
  sec: 2847
  time: '47:27'
  who: Alexey
- line: Yeah, I mean, I'm very close to the area where all these Amazon offices are.
    Every day, I come across someone familiar, either from my undergrad, master's,
    or just from LinkedIn. Very active people here.
  sec: 2862
  time: '47:42'
  who: Lavanya
- line: 'So another question: can you please highlight mentorship or networking opportunities
    which were helpful?'
  sec: 2880
  time: '48:00'
  who: Alexey
- line: Is this about getting those opportunities to be a mentor?
  sec: 2887
  time: '48:07'
  who: Lavanya
- line: The way I interpret this question is, you had some mentorship opportunities,
    but then some of them ended up being helpful for you in some way.
  sec: 2893
  time: '48:13'
  who: Alexey
- header: 'Networking & Mentorship: Cold Outreach and Building Rapport'
- line: Oh, I see. Yeah, I think getting those mentor roles is typically just through
    reaching out-cold emailing, LinkedIn messages, things like that.
  sec: 2908
  time: '48:28'
  who: Lavanya
- line: Once you build a certain rapport with people in these huge communities, it
    becomes word of mouth. If you've done good networking and put a good name to your
    work, it becomes word of mouth, and that's how you excel in these community-driven
    programs.
  sec: 2908
  time: '48:28'
  who: Lavanya
- line: Actually, speaking of our earlier conversation, I scrolled up and saw that
    we talked about mentorship. The reason I asked you about that is because I was
    doing some mentorship myself back then, so I was interested in your perspective.
  sec: 2956
  time: '49:16'
  who: Alexey
- line: And when people, one year after that, say, "Hey, thanks-because of the interview
    or the session we did together, now I work at this company. Thanks a lot, you
    changed my life." That's so cool.
  sec: 2975
  time: '49:35'
  who: For me, what was helpful is just structuring my knowledge around things. People
    come to you with requests and you try to help, and when you say things out loud,
    it helps structure these things in your head. Then it becomes easier to use this
    information in other settings. That was the main highlight for me.
- line: Yeah, I think that kind of exposure I only got with my open-ended mentorship
    experiences. Since you work independently, you had a lot of this, but I was tied
    to more formal, organized structures with these different groups I was working
    with.
  sec: 3019
  time: '50:19'
  who: Lavanya
- line: But when you hear back from people some years down the line that it helped,
    that's a really nice feeling that mentorship rewards you with.
  sec: 3031
  time: '50:31'
  who: There were set expectations and goals, less open-ended stuff. But there's more
    fun in the open-ended stuff because, as you say, it's just a normal conversation.
    It's not like I'm going to give you some magic tricks that will change your life.
- line: 'Thank you. So, another question in relation to successful datasets. I wanted
    to ask, Lavanya, what you consider to be another important element of a portfolio
    for data science. Basically, the question is: a dataset is one option, but what
    else, or what instead of a dataset, can we include in our portfolio?'
  sec: 3067
  time: '51:07'
  who: Alexey
- header: 'Portfolio Strategy: Community Visibility vs. Job-Targeted Projects'
- line: Oh yes. I think the dataset itself-I don't think it's even mentioned on my
    resume. Once you go out for these roles, these are like your pet projects. It's
    nice to show your curiosity or talk about it at the end of the interview, but
    it's hardly going to get you the job per se, because those requirements are different.
  sec: 3088
  time: '51:28'
  who: Lavanya
- line: 'Maybe this requires a follow-up question or clarification on what portfolio
    you''re talking about: is it just standing out or building networking opportunities,
    or is it more targeted at job applications? Maybe let''s cover both.'
  sec: 3113
  time: '51:53'
  who: To build your portfolio, one thing is you can stand out in the community through
    these pet projects and extra stuff. But if you're talking about a portfolio in
    terms of targeting jobs, that's a different kind of effort.
- line: If we want both-if we want to target networking, how should the portfolio
    look, and if we want to target job opportunities, how should it look?
  sec: 3148
  time: '52:28'
  who: Alexey
- line: Yeah, okay, sure. Let's talk about just the community-building, networking
    aspect first. Honestly, none of the things that I did gave me any jobs or roles.
    I've obviously just worked with two companies, but none of them got me anything
    from my extra stuff.
  sec: 3162
  time: '52:42'
  who: Lavanya
- header: 'Interview Preparation: LeetCode, Conceptual Mastery, and Mock Interviews'
- line: Profile building for job search is going to be a lot of LeetCode and a lot
    of conceptual drilling, going deep into the concepts so you can answer all interview
    questions precisely-no beating around the bush. And then mock interviews.
  sec: 3273
  time: '54:33'
  who: In terms of job building, I think that's very different. In my experience,
    at least in the US economy, it's highly competitive.
- line: But this is more for preparation to pass the interview. But I guess when it
    comes to portfolio, there are some projects that could be…
  sec: 3314
  time: '55:14'
  who: Alexey
- line: So, I don't know, let's say if I want to work at Amazon, then I can think,
    "What are the problems that Amazon solves?" and then do a project. Maybe a recommender
    system, or I know search.
  sec: 3314
  time: '55:14'
  who: Alexey
- line: Maybe there's a smaller e-commerce company, and you can get some e-commerce
    dataset and do a project on search. Especially if you want to work in this area,
    when you have an interview with them, you can say, "I maybe don't have professional
    experience, but I did this project on search, which we can talk about," and then
    all of a sudden you have things to discuss in the interview.
  sec: 3332
  time: '55:32'
  who: Maybe Amazon is a bad example because it's so huge and there is everything.
- line: For sure. I would just add to that, sometimes when I also interview people
    for roles, we do value some industrial experience more than pet projects, only
    because of scale.
  sec: 3369
  time: '56:09'
  who: Lavanya
- line: When you are doing something on your own, there is no real testing or feedback
    from actual users. You can just come and tell me, "I got 90% accuracy," but that's
    not verified.
  sec: 3369
  time: '56:09'
  who: Lavanya
- line: What I'm trying to say is, you can still do pet projects, but be associated
    with some organization. At my time, I was looking into this organization called
    OMD.
  sec: 3388
  time: '56:28'
  who: I understand if you're starting out new, it's a vicious circle-you want to
    get into the industry, and then I'm asking you to get industry experience to be
    able to get into the industry.
- header: 'Project Selection: Industry-Backed Work for Real-World Impact'
- line: That's a good way to showcase that you are still building your portfolio,
    but it's not completely your own project. There are others looking into it, equally
    invested, and it has some actual impact in the real world.
  sec: 3416
  time: '56:56'
  who: They do small projects, but they have industry experts looking over your projects.
    They organize students and people into data scientists, project managers, industry
    experts, and all of them work on a single project apart from their regular roles.
- line: There are many organizations like that who do these things. So for applications
    targeted at job building, I think that's a nice way to build your portfolio.
  sec: 3416
  time: '56:56'
  who: They do small projects, but they have industry experts looking over your projects.
    They organize students and people into data scientists, project managers, industry
    experts, and all of them work on a single project apart from their regular roles.
- header: Episode Wrap-Up & Final Career Advice
- line: Yeah, amazing. And on this note, I realize that I'm late for another meeting,
    so I need to run.
  sec: 3466
  time: '57:46'
  who: Alexey
- line: But that was an amazing conversation, Lavanya, so thanks a lot for agreeing
    to this interview. I had a lot of fun talking to you and also some memories from
    a couple of years ago. Thanks a lot for finding time to do this, and I wish you
    success.
  sec: 3466
  time: '57:46'
  who: Alexey
- line: Yeah, thank you so much. I'm sure we're going to stay in touch-no more nostalgia
    chats! And to everyone, thanks for joining us today, for asking questions, for
    being active. Enjoy the rest of your week.
  sec: 3490
  time: '58:10'
  who: Alexey
context: 'A practical, curiosity-driven bridge between research and engineering: relentlessly
  iterate with hands‑on prototyping, rigorous evaluation, and open dissemination to
  solve real-world ML problems (ex: long‑context LLMs), while leveraging community,
  mentorship, and strategic projects to accelerate career growth and drive measurable
  impact.'
---
Links:

* [Linkedin](https://www.linkedin.com/in/lgupta18/){:target="_blank"}