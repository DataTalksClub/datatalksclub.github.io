---
episode: 2
guests:
- katarzynaforemniak
ids:
  anchor: atatalksclub/episodes/Human-Centered-AI-for-Disordered-Speech-Recognition---Katarzyna-Foremniak-e2p8360
  youtube: yTZ4cddD7DU
image: images/podcast/s19e02-human-centered-ai-for-disordered-speech-recognition.jpg
links:
  anchor: https://podcasters.spotify.com/pod/show/datatalksclub/episodes/Human-Centered-AI-for-Disordered-Speech-Recognition---Katarzyna-Foremniak-e2p8360
  apple: https://podcasts.apple.com/us/podcast/human-centered-ai-for-disordered-speech-recognition/id1541710331?i=1000671805368
  spotify: https://open.spotify.com/show/0pck8zuiXdI0OrCg86DAPy?si=ac857db69d484277
  youtube: https://www.youtube.com/watch?v=yTZ4cddD7DU
season: 19
short: Human-Centered AI for Disordered Speech Recognition
title: 'Human-Centered ASR for Disordered Speech: Data, Multimodal Cues & Personalization'
transcript:
- header: Background and career journey of Katarzyna
- line: This week, we'll talk about human-centered AI for disordered speech recognition.
    We have a special guest today—Katarzyna Foremniak is a computational linguist
    with over ten years of experience in NLP and speech recognition. She has developed
    language models for automotive brands like Audi and Porsche and specializes in
    phonetics, morpho-syntax, and sentiment analysis. Katarzyna also teaches at the
    University of Warsaw and is passionate about human-centered AI and multilingual
    NLP. Welcome to the show!
  sec: 486
  time: '8:06'
  who: Alexey
- line: Thank you. I'm very happy and honored to be here with you today.
  sec: 521
  time: '8:41'
  who: Katarzyna
- line: How accurate was the bio? I asked GPT to summarize my longer bio, so I hope
    it was accurate.
  sec: 527
  time: '8:47'
  who: Alexey
- line: Yes, it was accurate. It was quite rich for a summary.
  sec: 536
  time: '8:56'
  who: Katarzyna
- header: Transition from linguistics to computational linguistics
- line: Before we dive into our main topic of human-centered AI and speech recognition,
    let’s start with your background. I think GPT already provided a lot of good insights,
    but could you tell us more about your career journey?
  sec: 546
  time: '9:06'
  who: Alexey
- line: Sure. I’m a computational linguist, which is a short answer. On one hand,
    I’m a researcher and a teacher at the Department of Italian Studies in the Faculty
    of Modern Languages at the University of Warsaw. On the other hand, I work on
    NLP projects in the automotive industry and collaborate with companies in data
    handling. My background is mainly in linguistics, which was my starting point.
    I studied Italian and Polish in parallel and added some technical skills along
    the way, leading me to my journey as a computational linguist. My main field of
    interest is phonetics, which is why this topic is relevant for today's meeting.
    Sorry for being a bit lengthy!
  sec: 561
  time: '9:21'
  who: Katarzyna
- line: It wasn’t long at all. I’m curious, to be a linguist in Italian, do you have
    to speak Italian perfectly?
  sec: 642
  time: '10:42'
  who: Alexey
- line: It helps!
  sec: 655
  time: '10:55'
  who: Katarzyna
- line: Do you speak it well?
  sec: 658
  time: '10:58'
  who: Alexey
- line: Yes, I do.
  sec: 658
  time: '10:58'
  who: Katarzyna
- line: I usually go to Italy as a tourist. The funny thing is that I live in Germany,
    and when I go to places like Garda, which is a popular vacation destination for
    Germans, people just look at me and start speaking German.
  sec: 660
  time: '11:00'
  who: Alexey
- line: Maybe that’s why! Italians are usually very open and appreciate it when you
    speak their language.
  sec: 681
  time: '11:21'
  who: Katarzyna
- header: Merging linguistics and computer science
- line: I’m curious about how difficult it was for you to transition from linguist
    to computational linguist. Linguistics, as I understand it, is less mathematical
    than other disciplines.
  sec: 698
  time: '11:38'
  who: Alexey
- line: There was an audio problem; I only caught the first part of your question.
  sec: 707
  time: '11:47'
  who: Katarzyna
- line: How difficult was it for you to become a computational linguist? It seems
    like linguistics isn’t as math-heavy, so how was the transition for you?
  sec: 713
  time: '11:53'
  who: Alexey
- line: You raised two interesting points. First, linguistics can indeed seem distant
    from mathematics. However, it depends on the approach; literature might seem far
    removed, but if you focus on data and relationships between datasets, it’s a different
    story.
  sec: 745
  time: '12:25'
  who: Katarzyna
- line: You’ve touched on how important the use of data is. Is it safe to say that
    computational linguistics merges linguistics and computer science?
  sec: 802
  time: '13:22'
  who: Alexey
- line: Yes, that’s correct. When I began, there were not many studies in computational
    linguistics in Poland. At that time, it was necessary to acquire programming skills
    along with linguistic knowledge. I had to catch up and was fortunate enough to
    meet several people who were willing to teach me.
  sec: 814
  time: '13:34'
  who: Katarzyna
- line: When you say that linguistics might not be too far from mathematics, I remember
    reading about syntax trees. They represent language in a more concrete way, almost
    like algebra, allowing us to work with mathematical abstractions instead of just
    letters and characters.
  sec: 827
  time: '13:47'
  who: Alexey
- line: 'Indeed, that’s one of the main perspectives in linguistics: viewing language
    as a structured system. This aligns closely with your description.'
  sec: 872
  time: '14:32'
  who: Katarzyna
- line: When you were studying linguistics in Italian, did you learn Italian first
    and then focus on linguistics, or did you approach it from a linguistic perspective
    before using the language more practically?
  sec: 887
  time: '14:47'
  who: Alexey
- line: I started learning Italian first. I managed to communicate in the language
    before diving deeper into its linguistic aspects. I also specialize in Polish
    linguistics, which felt much more natural since it’s my first language.
  sec: 904
  time: '15:04'
  who: Katarzyna
- header: Understanding phonetics and morpho-syntax
- line: In your biography, summarized by GPT, it mentions that you specialize in phonetics,
    morpho-syntax, and sentiment analysis. I’m familiar with sentiment analysis, but
    could you explain what phonetics and morpho-syntax are?
  sec: 925
  time: '15:25'
  who: Alexey
- line: Sure! Phonetics is the study of sounds in the language system, focusing on
    how we produce speech. It often intersects with phonology, which explores sounds
    on a mental level. Morphology, on the other hand, deals with how words are formed
    and how they interact with one another. In morphologically rich languages like
    Polish, this includes inflection and the use of prefixes and suffixes. It’s essential
    to consider both phonetics and morphology, as many aspects of our speech are interconnected.
  sec: 947
  time: '15:47'
  who: Katarzyna
- header: Exploring morpho-syntax and its relation to grammar
- line: So we have linguistics, and within linguistics, we have morphology and phonetics,
    right?
  sec: 1048
  time: '17:28'
  who: Alexey
- line: Yes, we can delve even deeper into other areas like semantics (the meaning
    of words) and pragmatics (the use of language). However, for today, I believe
    phonetics and speech are the most relevant.
  sec: 1054
  time: '17:34'
  who: Katarzyna
- line: I’m not sure if you answered this already, but what exactly is morpho-syntax?
  sec: 1078
  time: '17:58'
  who: Alexey
- line: Morpho-syntax combines morphology and syntax. It studies how words are constructed
    and how they fit into sentences, which are larger segments of text. There's a
    strong connection between how we use words and their arrangement in the overall
    structure.
  sec: 1083
  time: '18:03'
  who: Katarzyna
- line: How does morpho-syntax relate to grammar? Are they basically the same thing?
  sec: 1114
  time: '18:34'
  who: Alexey
- line: Grammar is linked to both morphology and syntax. When we discuss grammar,
    we often start with the word itself, looking at its structure and inflection,
    such as verb conjugation or tense.
  sec: 1119
  time: '18:39'
  who: Katarzyna
- line: So syntax is a higher-level concept, then? Each word is correctly formed and
    used, and syntax governs how we arrange them in sentences.
  sec: 1140
  time: '19:00'
  who: Alexey
- line: Exactly. Syntax is all about constructing sentences and how words relate to
    each other within those sentences.
  sec: 1158
  time: '19:18'
  who: Katarzyna
- line: Is syntax also related to word order? For example, in German, the verb must
    always be in the second position, right? That’s part of syntax, correct?
  sec: 1164
  time: '19:24'
  who: Alexey
- line: Yes, that’s a syntactic pattern.
  sec: 1174
  time: '19:34'
  who: Katarzyna
- line: For me, syntax was always associated with programming languages. For instance,
    in Java, you have to use curly braces. I didn’t realize that the concept originates
    from linguistics.
  sec: 1181
  time: '19:41'
  who: Alexey
- line: That’s a great example! It illustrates the same patterns we see in both natural
    languages and programming languages, such as what comes first and what should
    follow.
  sec: 1203
  time: '20:03'
  who: Katarzyna
- header: Connection between phonetics and speech disorders
- line: When it comes to phonetics, it’s about how we pronounce words, right?
  sec: 1233
  time: '20:33'
  who: Alexey
- line: Exactly, it’s about the production of sounds.
  sec: 1240
  time: '20:40'
  who: Katarzyna
- line: And we want to discuss speech disorders, so how are phonetics and speech disorders
    connected?
  sec: 1242
  time: '20:42'
  who: Alexey
- line: If we speak, it can be in a standard or non-standard manner. Speech disorders
    are examples of communication disorders. They can affect articulation, meaning
    the pronunciation of sounds and sound clusters, as well as fluency and voice quality.
    We might articulate a sound incorrectly, or have issues with fluency, such as
    interruptions or stuttering. These disorders can lead to difficulties in understanding
    speech.
  sec: 1251
  time: '20:51'
  who: Katarzyna
- line: Did I understand correctly that an accent can be seen as a speech disorder,
    or is it simply a normal variation?
  sec: 1399
  time: '23:19'
  who: Alexey
- line: When we talk about foreign accents, we don’t typically classify them as speech
    disorders. They may lead to comprehension difficulties, but they’re generally
    accepted as part of normal variation. Speech disorders, on the other hand, are
    often linked to biological or neurological causes that hinder normal speech production.
  sec: 1414
  time: '23:34'
  who: Katarzyna
- header: Improvement of voice recognition systems
- line: That’s interesting! I remember that even five years ago, many voice recognition
    systems struggled to transcribe what I said accurately. For instance, YouTube's
    transcription was quite poor in the beginning. But now, with systems like Whisper,
    they work much better with my accent. Perhaps my accent has improved too, but
    I think the systems have advanced more significantly.
  sec: 1481
  time: '24:41'
  who: Alexey
- line: That’s true! The models have improved considerably. Any non-standard speech
    can present challenges, but we generally don’t view foreign accents as a problem.
    We can choose to work on them if we want, or embrace our accents as part of our
    identity. However, discussing speech recognition for disordered speech is very
    similar to recognizing atypical speech, including child speech, foreign speech,
    and idiosyncratic pronunciations. Dialects and variations also fall under this
    category.
  sec: 1523
  time: '25:23'
  who: Katarzyna
- line: For example, the Scottish accent.
  sec: 1582
  time: '26:22'
  who: Alexey
- line: Some languages have rich regional varieties, and others do not. In the case
    of Scottish, a local variant can differ significantly from the so-called standard.
  sec: 1586
  time: '26:26'
  who: Katarzyna
- line: I remember watching a video from the British Parliament where a representative
    from Scotland spoke, and nobody could understand him. They asked him to repeat
    himself several times. It was quite amusing!
  sec: 1606
  time: '26:46'
  who: Alexey
- line: In such cases, it’s often best to use the standard version for effective communication,
    but that’s not always possible. Sometimes, not conveying everything can have its
    advantages.
  sec: 1626
  time: '27:06'
  who: Katarzyna
- header: Overview of speech recognition technology
- line: Can we talk about speech recognition? How does it work in general? Before
    diving into the differences between typical and atypical speech recognition, what’s
    the proper term to use for standard speech?
  sec: 1651
  time: '27:31'
  who: Alexey
- line: That’s a very timely topic, as there’s so much change happening in this area.
    We have large language models with generative AI. Traditionally, models were trained
    on precise datasets, typically collected according to a prepared scenario. A good
    example of standard speech is data collected from speakers like those from the
    BBC.
  sec: 1674
  time: '27:54'
  who: Katarzyna
- line: So if we consider British English, that would be a standard reference, right?
  sec: 1715
  time: '28:35'
  who: Alexey
- line: Yes, usually these datasets are designed to reflect standard speech. The systems
    are trained to map the output of spoken language to the model phrases they were
    trained on. However, with the advent of large language models, we can incorporate
    contextual training, which improves recognition. There’s hope for better recognition
    of atypical speech as well.
  sec: 1715
  time: '28:35'
  who: Katarzyna
- line: Right. It’s important to distinguish between standard and atypical speech.
  sec: 1734
  time: '28:54'
  who: Alexey
- line: Yes, and it’s essential to recognize that we all speak atypically at times.
    Factors like speaking fast or slow can affect clarity. Automatic speech recognition
    systems are typically trained on data from speakers without speech disorders.
    Therefore, they can struggle to recognize atypical patterns, which leads to poor
    accuracy.
  sec: 1742
  time: '29:02'
  who: Katarzyna
- line: So if I understand correctly, ASR (automatic speech recognition) systems struggle
    with speech that deviates from what they were trained on.
  sec: 1812
  time: '30:12'
  who: Alexey
- header: Challenges of ASR systems with atypical speech
- line: Absolutely. This misalignment between training and real-world usage can lead
    to significant recognition challenges.
  sec: 1824
  time: '30:24'
  who: Katarzyna
- line: So what can be done about this?
  sec: 1833
  time: '30:33'
  who: Alexey
- header: Strategies for improving recognition of disordered speech
- line: There are several strategies we can implement. One approach is to collect
    and curate specialized datasets that include speech from individuals with various
    disorders, using this as a subset in training. We can also employ transfer learning
    techniques to adapt models that were trained on standard data for individuals
    with speech disorders.
  sec: 1853
  time: '30:53'
  who: Katarzyna
- line: What if we don’t have such data available?
  sec: 2222
  time: '37:02'
  who: Alexey
- header: Data augmentation for training models
- line: If data collection is challenging, we can utilize data augmentation to expand
    the training dataset by artificially simulating disordered speech. For example,
    if we know specific sounds or consonant clusters are problematic, we can create
    artificial variations.
  sec: 2227
  time: '37:07'
  who: Katarzyna
- line: That’s interesting.
  sec: 2251
  time: '37:31'
  who: Alexey
- line: Another strategy is using multimodal outputs. While we learn from audio, adding
    visual data—such as lip reading or gesture recognition—
  sec: 2253
  time: '37:33'
  who: Katarzyna
- header: Transfer learning in speech recognition
- line: Yeah, not yet, of course. But I've worked with images, and in a typical situation,
    you have an ImageNet neural network trained on ImageNet. Then you have your own
    data, which could be tractors or anything else not included in ImageNet. You might
    find tractors there, but also something else. You take, say, 1,000 examples and
    fine-tune your network that was trained previously on ImageNet. The same process
    applies to speech. There is a model trained on standard data, and then you collect
    disordered speech. It doesn’t have to be a very large sample, like with images,
    and you apply transfer learning to fine-tune your model, right?
  sec: 2417
  time: '40:17'
  who: Alexey
- line: You mentioned data collection, and I said it’s not always easy because of
    the variety of speech disorders. For people with motor speech disorders, it can
    be challenging to organize the entire collection process. Also, considering we’re
    dealing with health issues, GDPR regulations come into play. It would be ideal
    for research purposes to have large corpora of disordered speech.
  sec: 2470
  time: '41:10'
  who: Katarzyna
- line: In terms of transfer learning, we don’t need a lot of data. A few hundred
    examples are usually sufficient to get started, at least with images.
  sec: 2515
  time: '41:55'
  who: Alexey
- line: Indeed. However, when we consider different disorders, languages, and non-English
    languages, the volume of data needed becomes substantial.
  sec: 2529
  time: '42:09'
  who: Katarzyna
- header: Challenges of collecting data for various speech disorders
- line: You need to account for each subset. For instance, stammering is a relatively
    common disorder. It might not be too difficult to gather data from American English
    speakers who stammer. But if we look at a particular dialect or a less common
    language—
  sec: 2538
  time: '42:18'
  who: Alexey
- line: For example, take a language that isn't widely spoken.
  sec: 2567
  time: '42:47'
  who: Katarzyna
- line: That's a good example. An interesting case is bilingual individuals, as studies
    show stammering can occur in one language and not in another. This is linked to
    specific semantic clusters or syllables that may be more common in one language
    than in another.
  sec: 2568
  time: '42:48'
  who: Alexey
- line: There are significant differences in these cases.
  sec: 2611
  time: '43:31'
  who: Katarzyna
- line: Exactly. I sometimes experience this myself. Maybe it’s because I'm thinking
    about what to say next, and then I start stammering. It doesn’t happen often,
    but I’ve noticed it happening to others, too.
  sec: 2633
  time: '43:53'
  who: Alexey
- header: Stammering and its connection to fluency issues
- line: Fluency issues are normal human behaviors, but stammering should be diagnosed
    as it differs from what most of us experience. Fluency is more common when using
    a foreign language; we often need time to find the right words.
  sec: 2671
  time: '44:31'
  who: Katarzyna
- line: With stammering, individuals know what they want to say, but they struggle
    with certain sequences of sounds.
  sec: 2683
  time: '44:43'
  who: Alexey
- line: Yes, it can block speech at the beginning of a sentence, while the rest flows
    fine. Sometimes, it’s linked to specific consonant clusters that are tough to
    pronounce. If a language has many of these, like English or Polish, it can be
    challenging.
  sec: 2688
  time: '44:48'
  who: Katarzyna
- header: Polish consonant combinations and pronunciation challenges
- line: Polish does have many challenging consonant combinations. I recall a funny
    story from Krakow about a central street called "Czarnowiejska." It's packed with
    consonants, and it’s always amusing to see a British person trying to pronounce
    it.
  sec: 2716
  time: '45:16'
  who: Alexey
- line: Exactly! There’s a trick with certain sounds, where two consonants together
    create one sound. This can make pronunciation tricky.
  sec: 2751
  time: '45:51'
  who: Katarzyna
- header: Use of Amazon Transcribe for generating podcast transcripts
- line: By the way, I use automatic speech recognition for podcast episodes after
    recording. I utilize Amazon Transcribe, which is supposed to recognize English.
  sec: 2777
  time: '46:17'
  who: Alexey
- line: So, you’re using that for generating transcripts?
  sec: 2792
  time: '46:32'
  who: Katarzyna
- line: Yes, but it expects English. Then, all of a sudden, I might throw in a Polish
    word, like "sensu." I think it’s interesting how LMs are being used more frequently
    in this area. I do this as well. After getting the output from Amazon Transcribe,
    I feed that into an LM or an LLM. Even if the transcriber misses something, the
    LLM, using the surrounding context, can often correct it.
  sec: 2794
  time: '46:34'
  who: Alexey
- header: Role of language models in speech recognition
- line: Absolutely. LMs can assist at various levels. Adding context is crucial for
    recognition and transcription. We should focus more on meaning preservation rather
    than traditional metrics like word error rate or accuracy.
  sec: 2848
  time: '47:28'
  who: Katarzyna
- header: Contextual understanding in speech recognition
- line: That makes sense. In my case, I had an interview about Scikit-learn. Many
    in the machine learning community know it, but the speech recognition system struggled
    to identify it correctly, especially since I was speaking with a French accent.
    However, because we provided context in our discussion, the LLM was able to infer
    the correct term.
  sec: 2959
  time: '49:19'
  who: Alexey
- line: That's a fantastic example! This two-step process—first speech recognition
    and then using an LM—works well. There are also models that can integrate both
    steps into one.
  sec: 3029
  time: '50:29'
  who: Katarzyna
- header: How voice recognition systems analyze utterances
- line: I suppose it’s about understanding how voice recognition systems operate.
    They analyze utterances, right?
  sec: 3087
  time: '51:27'
  who: Alexey
- line: It can be anything; it can be a word, it can be a phrase. A phoneme is what...
  sec: 3104
  time: '51:44'
  who: Katarzyna
- line: One sound, but like a word that I say, an utterance, right? For example, I
    have a disorder; I say "whiskey" instead of "whiskey." If I have a language model
    in my voice recognition system, it can understand from the context that I'm not
    talking about the alcoholic beverage, but rather about something that carries
    a lot of risk. Because it has access to the other words, the goal of a language
    model is to predict the next word based on the context. We can utilize language
    models, and we probably do, in voice recognition systems, right?
  sec: 3110
  time: '51:50'
  who: Alexey
- line: It can be trained to predict it or, in post-processing, it can say, "Okay,
    it’s strange that it’s in this context; it should be something else." All predictions
    we make are what happens next, as LLMs do. It can indicate that "risky" should
    come instead of the beverage from your example. So, it can be done before and
    also at the second step. One thing that came to my mind now, if we still have
    time, is that ASR models can also be personalized for specific individuals with
    specific disorders. In this case, this first stage, the training, and preparing
    the model to expect atypical productions or articulations can be done. It’s a
    bit different if we want to have a model that recognizes both standard speech
    and atypical productions.
  sec: 3151
  time: '52:31'
  who: Alexey
- header: Personalization of ASR models for individuals
- line: I guess with personalization, the way it works is I first need to train it
    as a user. It asks me, "Hey, can you pronounce this sentence?" I record myself
    saying the sentence, and then it asks me to pronounce something else. I do this
    for about ten sentences, and then it builds a model tailored specifically to the
    way I speak.
  sec: 3245
  time: '54:05'
  who: Katarzyna
- line: Individual speaking features, yes, tailored to your style of speaking.
  sec: 3275
  time: '54:35'
  who: Alexey
- line: There was a system like that before Whisper existed. I think I even tried
    it. But now with Whisper, it’s doing such a good job. I use it in ChatGPT; when
    I dictate whatever I want, it recognizes it, and even if it misunderstands something,
    ChatGPT figures out what I want. So it's quite convenient.
  sec: 3282
  time: '54:42'
  who: Katarzyna
- line: Amazing! But, I know, it’s a bit terrifying. It knows me so well—maybe better
    than I know myself!
  sec: 3313
  time: '55:13'
  who: Alexey
- line: I assume that when we talk about speech disorders—not accents or similar issues—having
    a personalized model is quite useful, right?
  sec: 3323
  time: '55:23'
  who: Katarzyna
- line: Of course. Especially when it’s difficult for you to use standard models.
    The personalized one can be a tool for communication. Today, we also discussed
    speech disorders, but there are also language disorders, especially as a result
    of neurological diseases, where individuals struggle to find the right words or
    use words in context. These issues are more connected to the content itself than
    to articulation.
  sec: 3339
  time: '55:39'
  who: Alexey
- header: Language disorders and their impact on communication
- line: How does it work? For example, if I have a disorder, I cannot pronounce a
    specific word?
  sec: 3385
  time: '56:25'
  who: Katarzyna
- line: You might not remember the right word in context, for example.
  sec: 3393
  time: '56:33'
  who: Alexey
- line: That happens to me all the time.
  sec: 3399
  time: '56:39'
  who: Katarzyna
- line: Yes, it occurs to all of us from time to time.
  sec: 3402
  time: '56:42'
  who: Katarzyna
- line: Especially in foreign languages.
  sec: 3406
  time: '56:46'
  who: Alexey
- line: If it happens too frequently, it can become problematic.
  sec: 3407
  time: '56:47'
  who: Katarzyna
- line: When it happens in your native language and interferes with your daily life,
    then it’s a problem.
  sec: 3412
  time: '56:52'
  who: Alexey
- line: Absolutely. Then it’s a real issue because stuttering and other speech problems
    can occur to anyone, and that’s completely natural. However, if it becomes excessive,
    it leads to communication problems.
  sec: 3419
  time: '56:59'
  who: Katarzyna
- line: 'Maybe this is something I should have asked you at the beginning: how do
    we actually use this? What are the applications? I might guess, given your work
    with the automotive industry, that what we discussed could be used in cars. Can
    you provide some examples?'
  sec: 3444
  time: '57:24'
  who: Alexey
- line: When you ask me about where speech recognition models can be used...
  sec: 3470
  time: '57:50'
  who: Katarzyna
- line: Especially regarding disorders.
  sec: 3478
  time: '57:58'
  who: Alexey
- header: Applications of speech recognition technology
- line: Of course, in cars. But that wouldn’t be my first answer for people with speech
    disorders. It can be used as a communication tool because sometimes it’s hard
    for humans to understand atypical speech. A pre-trained, personalized model can
    do this more easily. So that would be my primary answer, as I think that’s the
    most important application today.
  sec: 3480
  time: '58:00'
  who: Katarzyna
- line: How does it look in practice? Like, let's say I have an app on my phone, and
    I speak to the app, and then I show the result to someone I want to communicate
    with.
  sec: 3527
  time: '58:47'
  who: Alexey
- line: For example, it's not common yet—common enough, but we're talking today about...
  sec: 3540
  time: '59:00'
  who: Katarzyna
- line: I do this all the time with German.
  sec: 3544
  time: '59:04'
  who: Alexey
- line: But we are not— I think you see any serious pronunciation problems in your
    case. We are discussing human-centered AI today, and yeah, AI shouldn't be the
    obvious one. The number of centers that dedicate their work to human-centered
    AI proves that this is an important topic and not obvious for everyone. Human
    needs should come first. It's not yet standard to give each person with serious
    disorders this kind of tool. But I think there is hope.
  sec: 3549
  time: '59:09'
  who: Katarzyna
- line: What you're working on right now to make it possible.
  sec: 3595
  time: '59:55'
  who: Alexey
- line: Yes, that's one small step. But...
  sec: 3598
  time: '59:58'
  who: Katarzyna
- line: Are these models heavy? Because especially when it comes to personalized models
    and fine-tuning—if we talk about mobile devices, then these apps need to run on
    devices, and not everyone has the latest iPhone Pro. They have to be conservative
    regarding resource consumption. I guess that's quite challenging.
  sec: 3602
  time: '1:00:02'
  who: Alexey
- header: Challenges of personalized and universal models
- line: It's a challenge. One is creating a personalized model for a person—that's
    quite doable. But creating one that can be universal for many speech disorders
    makes things much more complicated. We’ll probably include some readings in the
    episode description. There are some studies from Asia for Korean and Chinese that
    managed to create such tools. We often talk about European languages, but let’s
    not forget about what’s happening a bit farther away from us.
  sec: 3634
  time: '1:00:34'
  who: Katarzyna
- header: Voice recognition in automotive applications
- line: And when it comes to cars, how is it used? How is voice recognition used in
    cars? Like, play Spotify with Alexa or...?
  sec: 3683
  time: '1:01:23'
  who: Alexey
- line: Yeah, you can ask it to behave in a certain manner. For example, if it...
  sec: 3696
  time: '1:01:36'
  who: Katarzyna
- line: If I cannot pronounce "R," then please park, and the car has no idea what
    I'm talking about.
  sec: 3708
  time: '1:01:48'
  who: Alexey
- line: And it's parking, and it's parking! Everything you need and what is planned
    by the producers and car designers includes opening the windows, air conditioning,
    seat heating, steering wheel heating, radio, calling, etc. That’s also an interesting
    example—not for disordered speech but for recognition in general. For years, it
    was part of IR systems that developed a bit slower because the purpose was to
    make it work all by itself, even when not connected to the internet. So these
    built-in systems were more traditional, based on n-grams. Now things are changing,
    and LLMs are being introduced, so recognition is improving. Still, there are many
    videos on YouTube with Porsche drivers and others trying to convince their cars
    in Polish, Italian, Spanish, etc., to do something, and the cars are doing completely
    different things. So there is so much work to be done.
  sec: 3713
  time: '1:01:53'
  who: Katarzyna
- header: Humorous voice recognition failures in cars
- line: There’s this hilarious video with two Scottish guys trying to go to the 11th
    floor in an elevator using voice recognition. Have you seen that?
  sec: 3807
  time: '1:03:27'
  who: Alexey
- line: No, you have to send it to me!
  sec: 3817
  time: '1:03:37'
  who: Katarzyna
- line: I’ll send it if you just Google...
  sec: 3820
  time: '1:03:40'
  who: Alexey
- line: Alex, we’ll put it in the post like the icing on the cake.
  sec: 3822
  time: '1:03:42'
  who: Katarzyna
- line: If I just Google "11 elevator," then there’s a video. I’ll include this in
    the description; it's amazing. So, I think we’ll run out of time.
  sec: 3832
  time: '1:03:52'
  who: Alexey
- line: Sorry, but it was fantastic!
  sec: 3850
  time: '1:04:10'
  who: Katarzyna
- header: Closing remarks and reflections on the discussion
- line: I think we covered only three questions out of—I don’t know how many we prepared,
    but it was...
  sec: 3853
  time: '1:04:13'
  who: Alexey
- line: I think that's good! There’s still something to think about, something to
    read, to dive deeper into.
  sec: 3858
  time: '1:04:18'
  who: Katarzyna
- line: I’m going to send the video to you right now. Then for the rest, I'll send
    it to Zoom, and I will also put it on YouTube. That should be a good way to end
    this interview today. There’s still so much to discuss. I think this video is
    like ten years old, if not more. You probably know the systems are better now.
  sec: 3867
  time: '1:04:27'
  who: Alexey
- line: Ten years old and it’s still valid. We still have the same problems. I think
    the situation is a bit better, but probably such things still happen.
  sec: 3898
  time: '1:04:58'
  who: Katarzyna
- line: Thanks, Kasha, for joining us today, for answering questions, and for sharing
    your experience and what you work on. That was amazing! I really enjoyed this,
    and I'm sure everyone else did too.
  sec: 3913
  time: '1:05:13'
  who: Alexey
- line: Thank you. Thank you for the invitation, and really congratulations on the
    great series of podcasts, but also for the fantastic platform that you created.
    I feel really impressed, and as I said at the beginning, I feel honored to be
    here.
  sec: 3925
  time: '1:05:25'
  who: Katarzyna
description: 'Learn ASR strategies for disordered speech: data, multimodal cues and
  personalization to build robust assistive voice systems and on-device speech tools.'
intro: 'How can automatic speech recognition (ASR) systems reliably understand disordered
  and atypical speech without compromising user identity or privacy? In this episode
  Katarzyna Foremniak, a computational linguist with 10+ years in NLP who developed
  language models for Audi and Porsche and teaches at the University of Warsaw, tackles
  that question through a human‑centered lens. <br><br> We explore core phonetics
  and morpho‑syntax concepts that matter for disordered speech, distinctions between
  accents and disorders, and practical limits of modern models (e.g., Whisper) when
  faced with atypical articulation, stammering, and voice quality variation. Katarzyna
  walks through data‑driven strategies: specialized datasets, data augmentation, transfer
  learning and fine‑tuning with limited data, plus multimodal ASR approaches that
  integrate lip‑reading and visual cues. The conversation also covers data collection
  challenges (GDPR, clinical data, language and dialect coverage), personalization
  and on‑device adaptation, and assistive and automotive use cases with deployment
  constraints. <br><br> If you work on speech recognition, accessibility, or multilingual
  NLP, this episode offers concrete technical strategies and ethical considerations
  for building personalized, multimodal ASR systems that better serve people with
  speech disorders.'
---

Links:

* [Eleven elevator](https://www.youtube.com/live/NMS2VnDveP8){:target="_blank"}