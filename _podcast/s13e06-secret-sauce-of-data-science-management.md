---
episode: 6
guests:
- shirmeirlador
ids:
  anchor: ow/datatalksclub/episodes/The-Secret-Sauce-of-Data-Science-Management---Shir-Meir-Lador-e21cu92
  youtube: gcxP0qRO-MY
image: images/podcast/s13e06-secret-sauce-of-data-science-management.jpg
links:
  anchor: https://podcasters.spotify.com/pod/pod/show/datatalksclub/episodes/The-Secret-Sauce-of-Data-Science-Management---Shir-Meir-Lador-e21cu92
  apple: https://podcasts.apple.com/us/podcast/the-secret-sauce-of-data-science-management-shir-meir-lador/id1541710331?i=1000606790142
  spotify: https://open.spotify.com/episode/4kzcUCVPVN1Opq7XI1Dibd?si=f7GlEOs-TFiC9dxTJlXVyw
  youtube: https://www.youtube.com/watch?v=gcxP0qRO-MY
season: 13
short: The Secret Sauce of Data Science Management
title: 'Master Data Science Management: Agile ML, Debrief Culture, Metrics & Scale
  to Production'
transcript:
- header: 'Episode Introduction: The Secret Sauce of Data Science Management'
- line: This week, we'll talk about the secret sauce of data science management. We
    have a special guest today, Shir. Shir is a data science group manager at Intuit
    in the United States. She's working on developing machine learning and deep learning
    models for document intelligence for products like TurboTax and QuickBooks. Welcome,
    Shir.
  sec: 100
  time: '1:40'
  who: Alexey
- line: Thank you, Alex. It's great to be here. Should I call you Alex or Alexey?
  sec: 123
  time: '2:03'
  who: Shir
- line: Alexey. Well, some people do call me Alex and I'm used to that. But my mom
    would never call me Alex.
  sec: 127
  time: '2:07'
  who: Alexey
- line: Okay. [chuckles] So thank you, Alexey. I'm glad to be here with you guys.
    Sorry about the technical issues.
  sec: 136
  time: '2:16'
  who: Shir
- line: Yeah, we always have these technical issues. Anyways. I think we should start.
    But before we go into our main topic of the secret sauce of data science management,
    let's start with your background. Can you tell us about your career journey so
    far?
  sec: 142
  time: '2:22'
  who: Alexey
- header: 'Career Background: Electrical Engineering to Document Intelligence at Intuit'
- line: Yeah, sure. I studied electrical engineering, I did my Bachelor’s and Master's
    in that topic at Ben-Gurion University in Negev, Israel. Since then, I have worked
    in two startups as a data scientist – an algorithm developer. Five and a half
    years ago, I moved to Intuit and I took a leadership role. I've been at Intuit
    for five years, leading and growing the team, developing AI solutions for security
    risk and fraud, and smart products.
  sec: 160
  time: '2:40'
  who: Shir
- line: About seven months ago, I relocated to the Bay Area (in which we don't have
    electricity in a large part of it at the moment) to take on a new leadership position,
    leading a team that is focused around document intelligence. Basically, the mission
    of my current team is to use computer vision and NLP methods to extract and classify
    information from any financial document that is uploaded to our system in order
    to simplify tax submission and small business management for our customers. It's
    gonna save them time, basically, to take the heavy lifting out of all of their
    financial hard work that they have in their life.
  sec: 160
  time: '2:40'
  who: Shir
- line: Just curious, did you happen to serve in the army in Israel? We had another
    guest from Israel more than a year ago, and I think she was some sort of pilot.
    That was pretty interesting. Did you need to do anything like that? Did you need
    to pilot anything?
  sec: 252
  time: '4:12'
  who: Alexey
- header: 'Military Leadership Lessons: Pilot Training & Debrief Culture Origins'
- line: Yeah, I actually served for three years in the army. I was a pilot instructor.
    It's interesting that you said that. I was a pilot instructor in a flight simulator.
    Basically, I trained helicopter pilots for Blackhawks and Super Stallion.
  sec: 271
  time: '4:31'
  who: Shir
- line: Was it useful for your career afterwards?
  sec: 294
  time: '4:54'
  who: Alexey
- line: Well, definitely. I've learned so much from the Israeli Air Force. The debrief
    culture really helps me in really developing a growth mindset. Also, being a pilot
    trainer really helps you to be a good leader and a good mentor for your team.
  sec: 298
  time: '4:58'
  who: Shir
- header: 'Debriefing Practice: Pre/post Focus Areas for Continuous Improvement'
- line: Which, I guess, is quite related to the topic today. So what is this debrief
    culture? What does “debrief” mean?
  sec: 324
  time: '5:24'
  who: Alexey
- line: Basically how it works is, every time a pilot goes onto a flight in the sky,
    or in the flight simulator, they look at their lessons learned from previous flights
    in the simulator or in the air on the helicopter. They take three things they
    want to focus on in this flight. After they get off the flight, they debrief –
    retrospect on what happened in that flight and what are the three things that
    they want to work on for the next time. So that's something I’ve really taken
    into my life, because this is really the best way to get better. Just keep on
    learning from experience, keep on marking the areas where you want to focus.
  sec: 335
  time: '5:35'
  who: Shir
- line: I just want to make sure I understand it correctly. Before the flight (or
    before doing something) you think about “What are the three things I want to focus
    on in this flight/project/whatever.” Right? You think about these things in advance,
    and you try to focus on them. After the flight/project/thing, you do a retrospective
    and you think about, “Okay, what are the three things I want to improve based
    on the experience I just had?” Is this a correct summary?
  sec: 386
  time: '6:26'
  who: Alexey
- line: Yes, it's the correct summary. And the three things that you focus on are
    usually things that you noticed in the past where you should do better.
  sec: 416
  time: '6:56'
  who: Shir
- line: So it could be like “better navigating the helicopter,” right? Or something
    like that?
  sec: 427
  time: '7:07'
  who: Alexey
- line: Yeah, it could be better navigating. If it’s specifically in a simulator,
    we work on how they handle emergencies in the air. For example, if the tail rotor
    is now malfunctioning and the helicopter starts spinning around, how fast did
    they reduce the collective – the power? And how fast did they react with the pedals?
    Also things about teamwork – how much did they listen to their co-pilot? Many
    things like that.
  sec: 434
  time: '7:14'
  who: Shir
- line: Sorry about the technical difficulties. You would think that they would have
    better infrastructure in Silicon Valley. [chuckles] Right?
  sec: 525
  time: '8:45'
  who: Shir
- line: Than in Israel? Why?
  sec: 529
  time: '8:49'
  who: Alexey
- line: Better than anywhere, than it’s not [inaudible]
  sec: 531
  time: '8:51'
  who: Shir
- line: Oh, because all the IT companies are there and they need electricity, right?
  sec: 535
  time: '8:55'
  who: Alexey
- line: Yeah.
  sec: 539
  time: '8:59'
  who: Shir
- line: Okay. [chuckles] Well, maybe they have backup generators or whatever. I think
    YouTube still works, although the servers are probably not in the Bay Area. It’s
    probably too expensive.
  sec: 541
  time: '9:01'
  who: Alexey
- header: 'Group Manager Role: Strategy, Mentoring, Standards and Roadmaps'
- line: Anyways, we stopped at the debrief summary – debrief culture. What I wanted
    to ask you next is – what do you actually do as a group manager? What are your
    responsibilities?
  sec: 558
  time: '9:18'
  who: Alexey
- line: I am a manager of three teams. Basically, I have team leads that lead those
    teams. I also manage a couple of ICs. The group manager’s role is basically to
    define the strategy for the whole group to plan ahead – look one year, three years
    ahead and see what the vision is. Where do we want to get to? Find the right things
    we want to focus on as a group. What is the right direction from research? What
    are the metrics we would want to look at? What does success look like for our
    team?
  sec: 574
  time: '9:34'
  who: Shir
- line: Of course, we also make sure we're building a high-performing group by mentoring
    the managers, as well as reviewing what the team is doing – what we are delivering
    – setting the standards, and working with the leadership team to make sure we
    are delivering the highest standards?
  sec: 574
  time: '9:34'
  who: Shir
- line: Okay, that's quite a few things. So you're managing the managers and you also
    mentioned ICs, but I guess these are people on the staff or principal level, right?
    Then you also take care of the strategy for the entire group when it comes to
    data, I guess, in terms of what exactly you want to achieve in the next couple
    of years. You need to know a lot about management to manage managers, right? So
    what does it mean to be a successful data science manager?
  sec: 655
  time: '10:55'
  who: Alexey
- line: As you know, I gave a talk about that about [cross-talk]
  sec: 687
  time: '11:27'
  who: Shir
- line: Exactly. The story – it was PyData Berlin, right? I saw your talk there and
    I reached out. It's been one year since then and we finally managed to be on the
    same call and discuss that. Sorry, for interrupting you.
  sec: 692
  time: '11:32'
  who: Alexey
- header: 'Measuring Success: Business Impact and Team Engagement Metrics'
- line: Yeah. I think that when you say “What is success,” you always need to have
    a measurement for that. I think the two things I focus on, is how much we're able
    to deliver impact for the business. This is one aspect of success. And the other
    aspect is the growth and engagement of the team members. These are the two things
    I measure myself by.
  sec: 713
  time: '11:53'
  who: Shir
- line: The growth of the business and growth of the people. [Shir agrees] These two
    aspects, business and people. What kind of metrics are there? I guess for business,
    it could be usual business metrics that we look at for the product, because we
    use data science to improve the product. But what about the second aspect – the
    people? What kind of metrics do you look at there?
  sec: 751
  time: '12:31'
  who: Alexey
- header: 'People Metrics: Pulse Surveys, Manager Score and Skip-level Feedback'
- line: Luckily, because we are at a big company, we have what's called a “pulse survey”.
    Basically, that's a survey that's done every six months. That's not everything,
    but that actually measures at least one aspect of that. It measures, for example,
    the engagement of the team, by asking different questions for all the team members.
  sec: 776
  time: '12:56'
  who: Shir
- line: That's one very easy measurement you can look at. It also has our questions.
    There's also something called “manager score,” which also measures different aspects
    of management, like giving recognition, giving guidance for the team, providing
    feedback – all of those things are measured and combined. This is what we call
    the manager score.
  sec: 776
  time: '12:56'
  who: Shir
- line: You ask the people, whom these managers manage, what they think about their
    managers, right? Then they give some feedback like whether they feel that they
    receive enough guidance, they receive enough feedback, that they have all the
    resources they need to be successful at their job. I’m guessing this is anonymized,
    right?
  sec: 838
  time: '13:58'
  who: Alexey
- line: Yeah, it's just a survey through this app – this company called Glint does
    it – and it's all anonymized. We just get the aggregated scores of our teams.
    The team can also provide comments, but I don't have access to the comments. You
    need to have, I think, at least 50 people to see all the comments. So the VP or
    director would have that access to all the comments and then they can direct me
    to comments that they think may be relevant to my team.
  sec: 860
  time: '14:20'
  who: Shir
- line: In this case, to call a data science manager successful, we look at two things
    – business metrics, to see if the team is delivering on their OKRs or whatever
    framework they use for setting goals, if they’re delivering on the goals. They
    show impact on business metrics and secondaries – the manager score? If both metrics
    are good, then we can call the manager successful.
  sec: 898
  time: '14:58'
  who: Alexey
- line: Well, it's not just that. And it's not just the manager score, it's also the
    engagement score. One of the roles of the manager is to make sure the team is
    engaged, so it's not just directly through the manager's score. But, this is not
    the only way I would measure my managers. I would also look at the feedback I
    get from their team. I'm doing skip levels with their team. There are multiple
    aspects. This is just a part of it.
  sec: 930
  time: '15:30'
  who: Shir
- line: Okay. On that talk that you mentioned – the PyData talk – you talked about
    the three pillars of data science management. Do you remember what these pillars
    are? Or is this what we already talked about just now?
  sec: 965
  time: '16:05'
  who: Alexey
- header: 'Leadership Pillars: Vision, Driving Results, Building High-performance
    Culture'
- line: Yeah. We have managing up, managing across, and managing below. Actually,
    I want to add something more – we have the Intuit leadership playbook, which I
    think is also really helpful. They also kind of define three pillars. It's not
    exactly parallel, but it's also an interesting way to look at it. One is, “lead
    with a clear vision”. The second is “drive winning results”. And the third one
    is “build a high-performance culture – a high-performance team, high-performance
    culture”.
  sec: 979
  time: '16:19'
  who: Shir
- line: I think these are all kind of related to the things we talked about, just
    a different way to frame it. But when you talk about managing up, across, and
    below, I think this is more about “How do you achieve all of those things we just
    mentioned?” Leading with a clear vision, driving winning results. So this is more
    of the practice of that.
  sec: 979
  time: '16:19'
  who: Shir
- line: So what is “managing up”? What does it mean?
  sec: 1040
  time: '17:20'
  who: Alexey
- header: 'Managing Leadership Relationships: Communicating Vision and Securing Resources'
- line: “Managing up” is how you work with your leadership team – with your manager,
    with a manager of your manager – to give them an understanding of what your vision
    of your team is. Share with them the success of your team, the requirements for
    your team to be successful. Maybe you need their help, maybe you need more resources.
    It's a lot about educating them about the accomplishments of your team. Because
    if you don't share the successes of your team with them, how would they know?
    Right? This is the role of the manager. And if they don’t know, what would be
    their motivation to invest further in this team? So this is “managing up”.
  sec: 1043
  time: '17:23'
  who: Shir
- line: What happens if we don't manage up? What happens if we don't talk about how
    good our team is? What can happen? What can go wrong?
  sec: 1095
  time: '18:15'
  who: Alexey
- line: Yeah. It's like the saying, “If a tree falls in the forest, and no one hears
    it, did it even fall?” I think it's kind of similar to that. Because if you don't
    get the backup from your leadership, how would you be able to grow and get more
    impact and improve? This is also true for one's career. I also encourage my employees
    to do that for themselves and I try to do that for myself. Try to list your own
    accomplishments and share that with your manager. Because your managers don't
    see everything you do. So to be recognized, you need to help your manager recognize
    you.
  sec: 1108
  time: '18:28'
  who: Shir
- line: I think I heard about a concept called the “brag document” where you “brag”
    about all your accomplishments. You say “Okay, in this quarter, I did this, this
    and this,” and then you have this document. Then you can share this document with
    your manager and the manager will realize how valuable you are. Maybe they don't
    know all the details about all the work you do, but with this brag document, you
    help them know about that.
  sec: 1155
  time: '19:15'
  who: Alexey
- line: In this instance, we’re talking about managing up from me, as an individual
    contributor, to my manager. But then it also can work up even further – from the
    manager level to the group manager level or head of data science or whatever –
    to the manager of managers. This is what you're talking about when you mean “managing
    up”?
  sec: 1155
  time: '19:15'
  who: Alexey
- line: Yeah. Also, find opportunities for your team to share their work with leadership.
    That's also very important. This way, leadership would understand the impact and
    also the complexities of the things we deliver.
  sec: 1207
  time: '20:07'
  who: Shir
- line: What kind of tools can we use for that? What can we do for that?
  sec: 1227
  time: '20:27'
  who: Alexey
- line: We have regular – we call them “side visits” or “tech reviews” – with the
    upper leadership. We need to make sure that our team gets a slot in this. And
    myself, as a group manager, I'm not a frontline manager, so I also do those tech
    reviews with my team members to get to know what they're working on and to see
    if I have some directions for them. It gives an opportunity to ask them some questions.
  sec: 1233
  time: '20:33'
  who: Shir
- line: So it's a meeting where we have the management and the actual people who do
    the work, together in the same room (or virtual room) where the employees talk
    about their accomplishments, right? What they did so far, how successful their
    projects are – and then the management learns about all the complexities and all
    the things.
  sec: 1271
  time: '21:11'
  who: Alexey
- line: Yeah. It's mainly focused around projects. When we get a project to a certain
    milestone, we ask the managers “Should we do a tech review on this? I would like
    to learn about this, this and this,” and then we can set the tech review. They
    prepare something and present it and then we have a discussion.
  sec: 1298
  time: '21:38'
  who: Shir
- line: In practice, it could be just a Zoom meeting, depending on the setup of the
    team – a Zoom meeting, or just a meeting, right?
  sec: 1319
  time: '21:59'
  who: Alexey
- line: Yeah. We have a regular schedule for that. Once a week, we have a tech review
    or design review.
  sec: 1328
  time: '22:08'
  who: Shir
- line: Do you do anything else apart from these meetings? Maybe some sort of email
    with a summary of all the accomplishments? Or not really?
  sec: 1336
  time: '22:16'
  who: Alexey
- line: No. That's what I do with my team members, but we have similar forms with
    the upper management that we do every few months. We present them with something.
    In the past, I actually tried to do a kind of ‘accomplishments’ email – I did
    a newsletter of what the team does. I think at small companies, it may be a good
    idea to do that.
  sec: 1347
  time: '22:27'
  who: Shir
- line: In a startup, it's really helpful to give the rest of the departments an understanding
    of what the data science or AI team is doing. I think that in a big company like
    Intuit, we have a lot of data science teams so it's a little bit challenging to
    actually read through those things.
  sec: 1347
  time: '22:27'
  who: Shir
- line: How many teams are there? Do you even know?
  sec: 1390
  time: '23:10'
  who: Alexey
- line: 'Wow. When I joined, we were like a few dozen – some data scientists and MLEs.
    That was five and a half years ago. We were just building up the organization.
    Right now, I think we have a few hundreds of data scientists and MLEs. This organization
    has also been unified with our organization of the data analysts and data engineers.
    So the entire organization has like 1500 people: data analysts, machine learning
    engineers, data engineers, and data analysts. Many of them do work around AI.'
  sec: 1394
  time: '23:14'
  who: Shir
- line: So it's very hard to keep up with everything that is happening.
  sec: 1428
  time: '23:48'
  who: Alexey
- line: Exactly. [chuckles]
  sec: 1432
  time: '23:52'
  who: Shir
- line: Okay, so we've talked about “managing up,” which is letting your manager,
    or the manager of your manager, (when you are manager, the upper management) know
    what exactly your team is doing and what their accomplishments are.
  sec: 1434
  time: '23:54'
  who: Alexey
- line: What about the other two main things you mentioned – “managing down” and “managing
    across”? What are these things? Let's maybe start with “managing down” or “managing
    below” as I think you called it. What is that?
  sec: 1434
  time: '23:54'
  who: Alexey
- header: 'Team Development: Goal-setting, One-on-ones, Feedback and Recognition'
- line: Yeah. “Managing down” or “managing below” basically means similar things to
    building a high performing culture. It is doing everything that consists of the
    manager score, if you are a manager. This is defining the expectations for your
    team, providing them the training, the resources, giving them the feedback, the
    recognition, making sure they get the guidance they need from you, making sure
    people are getting growth opportunities in their role – all that aspects of building
    a team and making it successful.
  sec: 1464
  time: '24:24'
  who: Shir
- line: I guess the tools you use here are performance reviews, setting goals, and
    all these things, right?
  sec: 1513
  time: '25:13'
  who: Alexey
- line: Yeah, definitely. We're setting goals. We have weekly one-on-ones. We have
    monthly check-ins, during which we review the progress of the goals they set for
    themselves. As I mentioned, we provide feedback. That's really important. We provide
    recognition of things.
  sec: 1523
  time: '25:23'
  who: Shir
- line: How does it work? These goals that you set and this monthly check-in meeting
    – how do you set these goals? What kind of framework do you use for setting these
    goals? And how often do you do this?
  sec: 1546
  time: '25:46'
  who: Alexey
- line: We have year-level goals, which we update sometimes. If there's a shift in
    priorities, they might be changed a little bit. But we do a goal-setting session
    at the beginning of the year. [cross-talk]
  sec: 1561
  time: '26:01'
  who: Shir
- line: With each person individually or with the team?
  sec: 1582
  time: '26:22'
  who: Alexey
- header: 'Goal Alignment: Cascading Roadmap Goals to Individual Development'
- line: No, I did something with the entire group. But in the company, because it's
    a big company, you have a lot of levels of hierarchy. It starts with the most
    upper levels – the directors, the VP – they set their goals, and according to
    that, the managers set their goals in alignment of the roadmap. Then according
    to that, the team members define their own goals in collaboration with their managers.
  sec: 1585
  time: '26:25'
  who: Shir
- line: Basically, we share our goals, which are more high-level, and then in the
    people according to what they think they should be doing for the business and
    also where they think they want to focus on from their personal development, they
    set their goals and then they get feedback from the manager. We always try to
    set goals that have a timeline and also some metrics that we can measure them
    by.
  sec: 1585
  time: '26:25'
  who: Shir
- line: So your manager has goals, then you look at these goals, and based on (it's
    probably some sort of VP) – based on their goals, you need to define goals for
    your group. You have the goals for your group, and perhaps you have some personal
    development goals, too. Then the managers who report to you (your team) look at
    your goals and think, “Okay, which of these goals make sense for me?” Then they
    break them down? For each team, each manager does the same work that you did –
    they come up with the goals for their own teams. Eventually, each person in their
    team does the same thing. This is how they end up with individual goals. Is this
    correct?
  sec: 1657
  time: '27:37'
  who: Alexey
- line: Exactly, you got it just right. We even have a system in which we are able
    to align all of those goals to each other. So if people in the manager’s team
    are making progress with their goals, and they align it to his goal, then the
    manager would get progress in their goal.
  sec: 1708
  time: '28:28'
  who: Shir
- line: Okay. So then at the very bottom, where individuals work, they have their
    own goals – business goals, I guess – and then they also have personal goals.
    For example, let's say I want to learn more about neural networks or whatever.
    This could also be a personal development goal, which should be related to one
    of the business goals, but can be a separate goal. Right?
  sec: 1727
  time: '28:47'
  who: Alexey
- line: Exactly.
  sec: 1755
  time: '29:15'
  who: Shir
- line: What does this look like in practice? What's the fraction between these business
    goals and individual self-development goals?
  sec: 1756
  time: '29:16'
  who: Alexey
- line: Yeah. If you're lucky, you're able to achieve the development goals within
    the scope of your business work. Sometimes, it doesn’t specifically have a complete
    overlap. Then you'll try to focus 10% of your time in building those skills you're
    not able to build during your regular work day-to-day.
  sec: 1766
  time: '29:26'
  who: Shir
- line: But I guess the business goals are more like, “Improve this KPI by X percent,”
    or “deliver this thing,” or “open a new flow,” or “index this amount of documents”
    or whatever. They are business-related goals with business metrics attached to
    them. But what I was asking more about was “Okay, there are business goals. But
    now I opened Twitter and I see all these things about large language models and
    all that and I want to kind of learn all that because I feel like I'm missing
    out if I don't.” Which can still be connected to these goals, but I can probably
    have some individual goals that are something like “Okay, I want to learn that
    thing in order to see how I can contribute the skills I build to one of the business
    goals.” Right?
  sec: 1790
  time: '29:50'
  who: Alexey
- line: Yes. You definitely got it. I want to say that, in my team, the work we are
    doing is definitely relevant to large language models. So even if it's your development
    goals, it might be something that you will actually work on.
  sec: 1842
  time: '30:42'
  who: Shir
- line: That's cool when you work on NLP stuff, right? This is exactly what your group
    is doing? You work on NLP. [Shir agrees] That's cool. Then you have these goals
    and every month, you have some sort of checkup where you check “Okay, am I progressing
    towards achieving those goals? Does each of these goals even make sense or should
    we rethink them?” Right? [Shir agrees] So that's “managing down”. I guess when
    you look at how these goals cascade down, it also makes sense. They go all the
    way down to individuals. Did we forget about anything? You talked about expectations
    for the team, feedback, recognition, growth opportunities. We kind of covered
    all that when it comes to managing down.
  sec: 1860
  time: '31:00'
  who: Alexey
- header: 'Fostering Innovation: Hackathons, Paper Clubs and Learning Forums'
- line: Yeah. This is a lot about the individuals in the team. There are also some
    aspects of team building, where you want to make sure that you have good team
    spirit and team atmosphere. You want to have a group meeting where there is a
    safe place to share things. You want to have those forums when an open discussion
    can be had. Some people run a book club or a paper club where they review papers
    together. So you want to open up the space for learning, you want to make sure
    you have a place like that.
  sec: 1920
  time: '32:00'
  who: Shir
- line: You make a place, as a manager, for innovation in your team. Indeed, it's
    really nice, because we have a hackathon for the entire company twice a year –
    it's a five-day hackathon, basically. It's called Global Engineering Days. I really
    like it because it's the five days in a year that I get to go back to actually
    being a hands-on engineer myself. Basically, it's just a few days where everyone
    plays with whatever technology they like and solves whatever problems they want.
    It's one way to encourage innovation, but I also try to do some work in encouraging
    innovation all the time.
  sec: 1920
  time: '32:00'
  who: Shir
- line: It’s about finding the balance between working on deliveries, as well as doing
    some long-term research within the team. That's also a lot of the work of the
    manager. Of course, it’s also what we talked about – we need to set the vision
    and the roadmap for the team. These are things that go in all directions – both
    above, sideways, and below. There’s planning for the team – there are so many
    aspects of management.
  sec: 1920
  time: '32:00'
  who: Shir
- line: Okay, interesting. The last pillar is “managing across”. I understand “up”.
    You have this hierarchy – you manage your managers, and the hierarchy goes down,
    where you manage your team. But what is “across”? Does it mean that, in your case,
    for example, your group manager – you collaborate with other group managers? This
    is the “across” part? Or what does it mean?
  sec: 2042
  time: '34:02'
  who: Alexey
- header: 'Cross-Functional Integration: Product Partnerships and Expectation Management'
- line: It will change from organization to organization. One thing would be the collaboration
    with other AI managers in the organization. This would be one aspect of that.
    But a lot of that is just working with product teams, because we are not a self-sufficient
    product team. We are working cross-functionally with different product teams across
    the company. I have my own product managers, which handle all of their collaborations
    with the product team. My team builds reusable services, which are used across
    many offerings at Intuit – different offerings of TurboTax, different offerings
    of QuickBooks, which use our services.
  sec: 2071
  time: '34:31'
  who: Shir
- line: We maintain relationships with a lot of teams. This is also managing across
    –  getting product requirements, translating those to the capabilities that the
    team is building, collaborating with the product teams. When our services are
    going to production, communicating the expected performance. When the services
    are in production, maintenance of those services. What is the support model that
    we are building for their services? What is the maintenance model? All of those
    things.
  sec: 2071
  time: '34:31'
  who: Shir
- line: So this means we need to talk with the product team, by which I guess you
    mean engineers and product managers and everyone who are not data scientists –
    not AI folks. [Shir agrees] Okay. What kind of managing across do you need in
    this case? How do you actually do this with them?
  sec: 2163
  time: '36:03'
  who: Alexey
- line: It's a lot of expectation management, sharing our roadmap, communicating.
    What are the capabilities that we are building? How are they solving the problems
    of the customers? How are they achieving the business metrics? What should they
    expect from them? Even just explaining things like, “These are machine learning
    models. They are not perfect. They have some predicted amount of error.
  sec: 2188
  time: '36:28'
  who: Shir
- line: We cannot review every error that they have in production. We want to focus
    on patterns.” For example, things that would require a retrain of the model or
    any further, more deeper change in the model. All of that work – educating product
    teams on what AI looks like, what we can achieve, and what we cannot.
  sec: 2188
  time: '36:28'
  who: Shir
- line: I guess the main goal for managing across is making sure that the product
    teams know how important the stuff that you’re working on is. They want to help
    you, they want to work with you, they want to integrate the models you build,
    because they know that these models contribute to the goals of the company, and
    they want to be a part of that. They also want to contribute to the goals of the
    company. Did I get this right?
  sec: 2254
  time: '37:34'
  who: Alexey
- line: Yeah. It's even deeper than that. They come to us with their requests so we
    have to explain to them – how do we fulfill those requests with the solutions
    we build?
  sec: 2280
  time: '38:00'
  who: Shir
- line: Okay. So the ask comes from the product team. They come to you and you discuss
    how exactly you can help. You need to make sure that you set expectations right
    and this is the expectation management aspect you mentioned. Then you need to
    make sure that there are no unreasonable requests, so you need to talk about education
    for them to know what is possible to achieve. I guess this is also kind of related
    to expectation management.
  sec: 2296
  time: '38:16'
  who: Alexey
- line: Then, based on that, you come up with a roadmap, you share this roadmap with
    them, and you start working. You then probably also need to somehow let them know
    how exactly you're progressing and what state you're in so they know when exactly
    they can start integrating your solution. Right?
  sec: 2296
  time: '38:16'
  who: Alexey
- line: Right. Yeah, definitely. We try to meet with them regularly and discuss the
    progress and raise questions. We also have joint Slack channels in which we can
    raise questions and get their questions about things. Yeah. I think that one more
    important thing that we kind of talked about, but didn't – as AI leaders, we need
    to give the product team the perspective of what kind of problem is suitable for
    an AI solution and what is not. This is another aspect of our mutual work with
    the product team.
  sec: 2343
  time: '39:03'
  who: Shir
- line: Also, I wanted to mention that it's not always requests coming from the product
    team. In many cases, because we are the ones who play with the data and have a
    better understanding of the data – of AI technologies – in many cases we will
    push in the other direction and tell them “You have to adopt this. This will help
    the customers.” So we’re also making that case and making sure they integrate
    those solutions and build the right product experience to handle them.
  sec: 2343
  time: '39:03'
  who: Shir
- line: So it's not only coming from the product teams. They don’t only come to you
    and say, “Hey, implement this,” but you also know that if they add this particular
    feature or capability, it will be a better user experience. You come to them with
    suggestions like, “Hey, this is what we can do in our team. What do you think
    about this? Does it sound interesting? Do you want to integrate it in the future?
    This is how this integration will look like.” [Shir agrees] I see. This is managing
    across. So “managing up” is managing your management, “managing down” is managing
    the team, and “managing across” is managing the people that you need to work with
    when you actually deliver your solution – the product teams. Anything else we
    forgot?
  sec: 2415
  time: '40:15'
  who: Alexey
- line: Um. No? [chuckles]
  sec: 2462
  time: '41:02'
  who: Shir
- header: 'AI Project Uncertainty: Data Risks, Unknowns and Rapid Experimentation'
- line: Okay. [chuckles] We have a few questions so maybe I'll go and check out these
    questions. A question from Patrick is, “Is there any major difference that you
    noticed regarding the management of data science teams versus usual business teams?
    Maybe you talked to your peers in management and discussed that there are some
    differences in that?”
  sec: 2466
  time: '41:06'
  who: Alexey
- line: Yeah. I think that everyone who has worked in an AI team knows that there
    is a lot more risk and a lot more unknowns when you're working on a project that
    is reliant on data. Because until you get into the data, explore it, play with
    it, you don't really know how relevant it is to actually solving the problem.
    In some cases, the results just won't be good enough. There are a lot more unknowns.
    Also, in many cases, things will pop up and will take more time than expected
    for various reasons – the technology is not suitable and there are certain aspects
    of data sensitivity or data security that you need to address in your work.
  sec: 2496
  time: '41:36'
  who: Shir
- line: Things that come from production, (that's also true maybe for engineering
    teams) but sometimes we have things we need to maintain from production that interrupt
    the plans. So there are a lot of unknowns, I think and it's hard to communicate
    that to the product team. That's why it's important, as you said, that we need
    to build a plan with the milestones and communicate that in advance. I know that
    maybe when this role of a data scientist has started being a thing, they would
    go and research for months, and then they will come up with something. We really
    try to avoid that and try to have rapid experimentation, rapid communication,
    showing the progress, getting feedback, and trying to deliver small parts as possible.
  sec: 2496
  time: '41:36'
  who: Shir
- line: Yesterday we had another interview about software engineering practices for
    machine learning. The guest, Nadia, mentioned that it's quite difficult right
    now to merge the data science process (how we deliver projects with data science)
    with traditional software engineering processes like Scrum, Kanban, and all that.
    I'm wondering, what does the process look like in your case for your teams? How
    do they work on things?
  sec: 2610
  time: '43:30'
  who: Alexey
- line: Let's say you have an idea – a business or a product team comes to you with
    an idea – what does it typically look like from the beginning (when you just start
    discussing this idea) to actually delivering this? I know this can be its own
    talk, but maybe you can briefly walk us through it?
  sec: 2610
  time: '43:30'
  who: Alexey
- header: 'Agile for ML: Two-week Sprints, Exploration Tasks and Grooming Practices'
- line: Yeah. [chuckles] Actually, my current team does work in Scrum.
  sec: 2658
  time: '44:18'
  who: Shir
- line: The data science team?
  sec: 2663
  time: '44:23'
  who: Alexey
- line: Yeah. Actually, the team that I lead right now is a mix of data scientists
    and machine learning engineers, because we have the model part, but we also have
    services that we build on top of those models. We have a lot of engineering work
    in our team because it may be that one service will include multiple models that
    we merge together. These are complex services.
  sec: 2664
  time: '44:24'
  who: Shir
- line: There are our MLEs and data scientists in all three Scrum teams – they are
    all working together. They work in two-week sprints, like the rest of the company.
    They do sprint planning. We do retrospectives as a group. They try to size all
    of those tasks according to the amount of unknowns they have in each of the tasks.
  sec: 2664
  time: '44:24'
  who: Shir
- line: I was going to ask about that. Because with the amount of unknowns in AI teams
    and data science teams, it's very different from software engineering teams, where
    it's a lot easier to scope the problem to understand, “Okay, this thing will take
    five story points (or whatever).” With data science, it’s more difficult. So how
    do you handle that?
  sec: 2717
  time: '45:17'
  who: Alexey
- header: 'Scoping ML Work: Exploration Sprints, Design Stories and Iterative Milestones'
- line: I think they're doing a really great job at that. But it's really hard. They
    try to break things into smaller points. I think that in the data science part,
    sometimes it will take longer than we expected. But they try to break it down
    as much as possible. I don't have any magic here. It's mainly just thinking about
    how you break things down, what kind of milestones you can define for each of
    those stories. When we start a project, or when we start looking at a problem,
    we try to think about what the design would be first.
  sec: 2736
  time: '45:36'
  who: Shir
- line: We kind of brainstorm during that. I would usually ask the data scientist
    or the machine learning engineer to do some exploration for a couple of weeks
    and then present some kind of an idea of what they think they're going to build
    – what they think they're going to do – after they played a little bit with the
    data. Then there's an opportunity there to show the overall direction, talk about
    and discuss the plan, about the anticipated impact, and what the architecture
    we think we're going to build around this thing is, and have a discussion about
    those things. So I think that trying to plan ahead a little bit helps you break
    things down later.
  sec: 2736
  time: '45:36'
  who: Shir
- line: So if there is an unknown – you don't know how exactly this data looks like,
    if it's good or not – so what you do is, one ML engineer or one data scientist
    just takes the entire sprint only to explore the data. Because you can say, “We
    don't know yet. We need time to figure this out.” Then they take the entire sprint
    to figure this out and at the end they say, “Okay, this looks promising. These
    are the tasks we need to do for the next steps.”
  sec: 2828
  time: '47:08'
  who: Alexey
- line: Yeah. Well, it doesn't really translate directly to tasks, but more I would
    expect, “This is an overall type of solution I would look at. This is the type
    of architecture I would anticipate.” Maybe it's not a task level, but more of
    a story level thing. Then the tasks would be broken down later, sprint by sprint.
  sec: 2859
  time: '47:39'
  who: Shir
- line: Do you do some sort of grooming sessions when you actually break down things
    as a team together?
  sec: 2884
  time: '48:04'
  who: Alexey
- line: Yeah. The Scrum team does that. I'm not very much involved in that. But they
    do it.
  sec: 2890
  time: '48:10'
  who: Shir
- line: Because you're a group manager. But the people who you manage, the frontline
    managers, they're probably involved in these things, right? [Shir agrees] So you
    have these grooming sessions, then you have estimation sessions or maybe it's
    the same session – I don't know how it works for you. Then, for two weeks, they
    work on a sprint and at the end they have a retrospective, right? That’s the process?
  sec: 2896
  time: '48:16'
  who: Alexey
- line: Yeah. After you do these kinds of grooming or sizing sessions, I ask them
    to share those results. I review it and ask questions and see if there are things
    that I think they missed. Also we have principal engineers that  also look at
    that and review that feedback.
  sec: 2927
  time: '48:47'
  who: Shir
- line: Do you know any resources that talk about Scrum for data science? I'm really
    curious to see how this could be implemented.
  sec: 2949
  time: '49:09'
  who: Alexey
- line: Well, I'm not sure I do unfortunately. I did the regular Scrum training for
    years. We just tried to adjust it to what works.
  sec: 2960
  time: '49:20'
  who: Shir
- line: So it's internal expertise that you have.
  sec: 2975
  time: '49:35'
  who: Alexey
- line: Yeah. I think everyone should just experiment. There might be some good resources
    out there because other people have experimented and wrote things. But there’s
    not something specific that I can recommend.
  sec: 2979
  time: '49:39'
  who: Shir
- header: 'Core Manager Skills: Communication, Strategic Clarity and Growth Mindset'
- line: Okay, another question from Patrick. “What is, in your view, the most important
    skill for a manager in data science and machine learning?”
  sec: 2994
  time: '49:54'
  who: Alexey
- line: Oh, wow. [chuckles] There are so many skills.
  sec: 3003
  time: '50:03'
  who: Shir
- line: Well, maybe let's take the top three. “The most important one” is too difficult
    to come up with.
  sec: 3008
  time: '50:08'
  who: Alexey
- line: I don't know if these are skills, but the three pillars that I mentioned are
    the three things you need to focus on – lead with a clear vision, drive winning
    results, and build a high performing team. You need to try to do all of those
    things. This is what leads me when I try to do my work.
  sec: 3014
  time: '50:14'
  who: Shir
- line: It’s more like strategy – how exactly you manage, how exactly you lead – you
    follow this principle. What does it actually mean to lead with a clean vision?
    This is about clarity? You know what exactly the team will do in the next quarter,
    in the next year?
  sec: 3037
  time: '50:37'
  who: Alexey
- line: 'It’s about having a vision of where you want the team to be in a year. What
    is the roadmap for this year? But it''s also about communicating, as we mentioned,
    both to leadership and to the team: What is expected? What is going to happen?
    What is going to be the impact? All of those things. So that people will not have
    ambiguities.'
  sec: 3056
  time: '50:56'
  who: Shir
- line: Okay, “lead with clear vision” is the first one and the other two – can you
    remind us?
  sec: 3082
  time: '51:22'
  who: Alexey
- line: Drive winning results.
  sec: 3088
  time: '51:28'
  who: Shir
- line: Drive winning results. This is about, again, setting expectations and then
    setting goals? What is it about?
  sec: 3091
  time: '51:31'
  who: Alexey
- line: This is about making sure we deliver with the highest quality and actually
    make an impact for the business. That's more of the tactics, from my perspective.
    “Leading with a clear vision” is more of a strategy and “drive winning results”
    is more of a tactic. It’s making sure that we execute with efficiency.
  sec: 3100
  time: '51:40'
  who: Shir
- line: If we talk about these large language models, they are fun to play with. But
    if we play with them, we also need to make sure that we’re actually contributing
    to the business goals. It's not like we just have fun with whatever models – we're
    doing this because we want to have results. We want to have an impact. That's
    what this thing is about. Right?
  sec: 3123
  time: '52:03'
  who: Alexey
- line: Exactly.
  sec: 3148
  time: '52:28'
  who: Shir
- line: Then the last one is what?
  sec: 3149
  time: '52:29'
  who: Alexey
- line: Build a high-performing team.
  sec: 3152
  time: '52:32'
  who: Shir
- line: I think this is what we talked about, like setting goals and all that, so
    that each team member knows what exactly they should do.
  sec: 3156
  time: '52:36'
  who: Alexey
- line: Yeah. And, as we said, mentoring them, coaching them, giving them the growth
    opportunities they need. If you want to focus on one skill that I think is most
    important for me, I think this is the growth mindset, which I also mentioned in
    my talk back then. There are so many skills, but the most important thing for
    me, at least, is to be able to… we started talking in the beginning of this conversation
    about me being in the Air Force, and this is what I learned from that – the debrief
    culture.
  sec: 3165
  time: '52:45'
  who: Shir
- line: This is exactly what it is – always being in growth mode. When you get feedback,
    don't be sad that you're not good at something like “Why did you say this thing
    about me? I'm really trying.” Well, this is all great, but really understanding
    how feedback is a gift and everything that you didn't do that well in the past
    is an opportunity to get better in the future –nailing down those areas and those
    things you need to do to grow and to be better.
  sec: 3165
  time: '52:45'
  who: Shir
- line: I realized that our talk is called “the secret sauce for data science management”.
    So what is the secret sauce? Is this the growth mindset?
  sec: 3235
  time: '53:55'
  who: Alexey
- line: Maybe? [chuckles] I think that it’s all of the things we talked about. It's
    a lot about the growth mindset. It's a lot about internalizing those pillars of
    management that we talked about and understanding “What is the strategy of you
    as a manager, to handle those things in the organization that you're in?”
  sec: 3245
  time: '54:05'
  who: Shir
- line: What does “internalizing” mean?
  sec: 3271
  time: '54:31'
  who: Alexey
- line: I’m having a hard time thinking about other words. It’s just taking it in,
    really believing it, and living by it.
  sec: 3280
  time: '54:40'
  who: Shir
- line: So doing it, basically. Implementing it.
  sec: 3286
  time: '54:46'
  who: Alexey
- line: Yeah and believing it. Taking a belief and value out of it – something you
    go by.
  sec: 3295
  time: '54:55'
  who: Shir
- header: 'POC to Production: Customer-focused Metrics, A/B Testing and Incremental
    Rollout'
- line: There is another question. I know we don't have a lot of time – only two minutes.
    I don't know if you can answer this in two minutes. Maybe you can give us a very
    brief overview. The question is, “A lot of data science proof of concepts do not
    get productionized and efforts are ‘wasted’. How do you increase the chances of
    a POC being successful?”
  sec: 3299
  time: '54:59'
  who: Alexey
- line: It's a lot about really deeply understanding the customer problem that you're
    trying to solve, working with the product partner to understand how the customer’s
    story looks like, and working with the customer, if you can. We call it “follow
    me home,” which is actually looking at what the customer is doing and what you
    can improve.
  sec: 3323
  time: '55:23'
  who: Shir
- line: It’s also about trying to choose those problems to solve in which you have
    an opportunity to make an impact for the customer and an opportunity to show that
    impact – a measurable impact. Then, when you go and do that POC, try to define
    “Okay, I want to get this metric from X to Y,” and then try to show that you have
    achieved that. So I think it's much easier to make the case to get a POC to production
    if you've shown that some business metric has moved due to that. If you have an
    opportunity to do A/B testing – experimentation to actually show it and combine
    it in the project for some fraction of the users – that's really a great way to
    kind of push things to production in a gradual manner, through rapid experimentation.
  sec: 3323
  time: '55:23'
  who: Shir
- line: So it’s about focusing on solving the problem. If you do this, and you show
    the impact on business metrics, the POC will be successful.
  sec: 3410
  time: '56:50'
  who: Alexey
- line: Yeah, hopefully. Of course, it takes a lot of communication with the product
    side. Listen and try to see how you can really help.
  sec: 3421
  time: '57:01'
  who: Shir
- line: Maybe that's also the answer to the question earlier from Patrick, “What is
    the most important skill?” Maybe communication is the answer there, because I
    see that it's a lot about talking. You need to talk with the team, you need to
    talk with management, you need to talk with product teams, you need to talk about
    understanding the customer problem. All of this involves talking, right? So you
    need to be quite good at this.
  sec: 3432
  time: '57:12'
  who: Alexey
- line: You definitely need to be a good communicator. It's important.
  sec: 3457
  time: '57:37'
  who: Shir
- line: Okay, I think that's all we have time for today. So thanks a lot for joining
    us today, for sharing your experience, we will definitely link the talk. Because
    I don't think we managed to cover most of this. I think we just talked about the
    first couple of slides for most of the time, but I think that went really well.
    If there is anything else you want us to add, maybe anything useful that you think
    that our listeners will enjoy on that topic, please send us links. And that's
    it. Thanks, everyone, for joining us today.
  sec: 3463
  time: '57:43'
  who: Alexey
- header: 'Resources & Further Reading: Shir’s Talks and Blog Posts'
- line: Yeah. Thank you so much, Alexey. I do have a few blog posts I've written in
    the past about leading a data science team, about how to streamline the development
    process of AI solutions. So I'm happy to share those resources.
  sec: 3498
  time: '58:18'
  who: Shir
- header: Episode Wrap-up and Closing Remarks
- line: Okay, well, then, have a great weekend, everyone. Goodbye.
  sec: 3517
  time: '58:37'
  who: Alexey
description: 'Master data science management: learn Agile ML, debrief culture, metrics
  and POC-to-production strategies to scale teams, boost impact and ship reliable
  models.'
intro: How do you run data science teams so experiments become reliable, measurable
  products? In this episode, Shir Meir Lador, a data science group manager at Intuit
  who builds machine and deep learning models for document intelligence in TurboTax
  and QuickBooks, walks through practical approaches to data science management and
  agile ML. <br><br> We explore the origins of debrief culture from military pilot
  training and how pre/post debriefs drive continuous improvement; concrete practices
  for agile ML including two-week sprints, exploration sprints, design stories and
  grooming; and how to scope work, handle AI project uncertainty, and use rapid experimentation
  to mitigate data risks. Shir also digs into metrics for production ML—business impact,
  A/B testing, customer-focused KPIs—and people metrics like pulse surveys, manager
  score and skip-level feedback. You’ll hear about leadership pillars (vision, driving
  results, culture), team development, goal alignment, cross-functional product partnerships,
  and tactics for fostering innovation (hackathons, paper clubs). <br><br> Listen
  for actionable guidance on measuring success, scaling ML to production, and building
  the managerial skills to lead high-performance data science teams. This episode
  is for managers and technical leads focused on production ML, machine learning operations,
  and team-driven impact.
dateadded: '2023-04-01'
duration: PT00H56M57S
quotableClips:
- name: 'Episode Introduction: The Secret Sauce of Data Science Management'
  startOffset: 100
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=100
  endOffset: 160
- name: 'Career Background: Electrical Engineering to Document Intelligence at Intuit'
  startOffset: 160
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=160
  endOffset: 271
- name: 'Military Leadership Lessons: Pilot Training & Debrief Culture Origins'
  startOffset: 271
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=271
  endOffset: 324
- name: 'Debriefing Practice: Pre/post Focus Areas for Continuous Improvement'
  startOffset: 324
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=324
  endOffset: 558
- name: 'Group Manager Role: Strategy, Mentoring, Standards and Roadmaps'
  startOffset: 558
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=558
  endOffset: 713
- name: 'Measuring Success: Business Impact and Team Engagement Metrics'
  startOffset: 713
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=713
  endOffset: 776
- name: 'People Metrics: Pulse Surveys, Manager Score and Skip-level Feedback'
  startOffset: 776
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=776
  endOffset: 979
- name: 'Leadership Pillars: Vision, Driving Results, Building High-performance Culture'
  startOffset: 979
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=979
  endOffset: 1043
- name: 'Managing Leadership Relationships: Communicating Vision and Securing Resources'
  startOffset: 1043
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=1043
  endOffset: 1464
- name: 'Team Development: Goal-setting, One-on-ones, Feedback and Recognition'
  startOffset: 1464
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=1464
  endOffset: 1585
- name: 'Goal Alignment: Cascading Roadmap Goals to Individual Development'
  startOffset: 1585
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=1585
  endOffset: 1920
- name: 'Fostering Innovation: Hackathons, Paper Clubs and Learning Forums'
  startOffset: 1920
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=1920
  endOffset: 2071
- name: 'Cross-Functional Integration: Product Partnerships and Expectation Management'
  startOffset: 2071
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=2071
  endOffset: 2466
- name: 'AI Project Uncertainty: Data Risks, Unknowns and Rapid Experimentation'
  startOffset: 2466
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=2466
  endOffset: 2658
- name: 'Agile for ML: Two-week Sprints, Exploration Tasks and Grooming Practices'
  startOffset: 2658
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=2658
  endOffset: 2736
- name: 'Scoping ML Work: Exploration Sprints, Design Stories and Iterative Milestones'
  startOffset: 2736
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=2736
  endOffset: 2994
- name: 'Core Manager Skills: Communication, Strategic Clarity and Growth Mindset'
  startOffset: 2994
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=2994
  endOffset: 3299
- name: 'POC to Production: Customer-focused Metrics, A/B Testing and Incremental
    Rollout'
  startOffset: 3299
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=3299
  endOffset: 3498
- name: 'Resources & Further Reading: Shir’s Talks and Blog Posts'
  startOffset: 3498
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=3498
  endOffset: 3517
- name: Episode Wrap-up and Closing Remarks
  startOffset: 3517
  url: https://www.youtube.com/watch?v=gcxP0qRO-MY&t=3517
  endOffset: 3417
---

Links:

* [The secret sauce of data science management](https://www.youtube.com/watch?v=tbBfVHIh-38){:target="_blank"}
* [Lessons learned leading AI teams](https://blogs.intuit.com/2020/06/23/lessons-learned-leading-ai-teams/){:target="_blank"}
* [How to avoid conflicts and delays in the AI development process (Part I)](https://blogs.intuit.com/2020/12/08/how-to-avoid-conflicts-and-delays-in-the-ai-development-process-part-i/){:target="_blank"}
* [How to avoid conflicts and delays in the AI development process (Part II)](https://blogs.intuit.com/2021/01/06/how-to-avoid-conflicts-and-delays-in-the-ai-development-process-part-ii/){:target="_blank"}
* [Leading AI teams deck](https://drive.google.com/drive/folders/1_CnqjugtsEbkIyOUKFHe48BeRttX0uJG){:target="_blank"}
* [Leading AI teams video](https://www.youtube.com/watch?app=desktop&v=tbBfVHIh-38){:target="_blank"}